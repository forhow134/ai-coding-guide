{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 Speech & Audio\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/forhow134/ai-coding-guide/blob/main/demos/08-multimodal/speech_audio.ipynb)\n",
    "\n",
    "**预计 API 费用: ~$0.01**\n",
    "\n",
    "Whisper 语音转文字、TTS 文字转语音。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nfrom getpass import getpass\n\nif not os.environ.get('OPENAI_API_KEY'):\n    os.environ['OPENAI_API_KEY'] = getpass('OpenAI API Key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Whisper Speech转文字\n",
    "<!-- 实验 1: Whisper 语音转文字 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n\nclient = OpenAI()\n\n# 示例: 如果有音频文件\n# audio_file = open('meeting.mp3', 'rb')\n# transcript = client.audio.transcriptions.create(\n#     model='whisper-1',\n#     file=audio_file,\n#     response_format='text'\n# )\n# print(transcript)\n\nprint('提示: 准备一个音频文件 (mp3/wav/m4a) 后测试')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: 不同Output格式\n",
    "<!-- 实验 2: 不同输出格式 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n\nclient = OpenAI()\n\n# 假设有音频文件\n# audio_file = open('audio.mp3', 'rb')\n\n# 1. 纯文本\n# text = client.audio.transcriptions.create(model='whisper-1', file=audio_file, response_format='text')\n\n# 2. JSON\n# json_result = client.audio.transcriptions.create(model='whisper-1', file=audio_file, response_format='json')\n\n# 3. SRT 字幕\n# srt = client.audio.transcriptions.create(model='whisper-1', file=audio_file, response_format='srt')\n\n# 4. Verbose JSON (详细信息)\n# verbose = client.audio.transcriptions.create(\n#     model='whisper-1',\n#     file=audio_file,\n#     response_format='verbose_json',\n#     timestamp_granularities=['word', 'segment']\n# )\n\nprint('提示: 取消注释后测试不同输出格式')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: TTS 文字转Speech\n",
    "<!-- 实验 3: TTS 文字转语音 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\nfrom pathlib import Path\n\nclient = OpenAI()\n\nresponse = client.audio.speech.create(\n    model='tts-1',\n    voice='alloy',\n    input='欢迎使用 AI 语音助手。今天我将为您介绍如何使用 OpenAI 的 TTS 功能。'\n)\n\nspeech_file = Path('output.mp3')\nresponse.stream_to_file(speech_file)\n\nprint(f'语音已生成: {speech_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4: 不同音色Comparison\n",
    "<!-- 实验 4: 不同音色对比 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\nfrom pathlib import Path\n\nclient = OpenAI()\n\ntext = '这是一段测试语音'\nvoices = ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer']\n\nfor voice in voices:\n    response = client.audio.speech.create(\n        model='tts-1',\n        voice=voice,\n        input=text\n    )\n    response.stream_to_file(f'{voice}.mp3')\n    print(f'{voice}.mp3 已生成')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5: 完整SpeechConversation流程\n",
    "<!-- 实验 5: 完整语音对话流程 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n\nclient = OpenAI()\n\ndef voice_chat(audio_path: str):\n    # 1. 语音转文字\n    with open(audio_path, 'rb') as audio_file:\n        question = client.audio.transcriptions.create(\n            model='whisper-1',\n            file=audio_file\n        ).text\n    print(f'用户: {question}')\n    \n    # 2. LLM 回答\n    answer = client.chat.completions.create(\n        model='gpt-4o-mini',\n        messages=[{'role': 'user', 'content': question}]\n    ).choices[0].message.content\n    print(f'AI: {answer}')\n    \n    # 3. 文字转语音\n    response = client.audio.speech.create(\n        model='tts-1',\n        voice='nova',\n        input=answer\n    )\n    response.stream_to_file('reply.mp3')\n    print('语音回复已生成: reply.mp3')\n\n# voice_chat('user_question.mp3')\nprint('提示: 准备音频文件后测试完整流程')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关键要点\n\n1. **Whisper**: $0.006/分钟,支持 99 种语言\n2. **输出格式**: text | json | srt | vtt | verbose_json\n3. **TTS**: $15/1M 字符 (tts-1)\n4. **6 种音色**: alloy, echo, fable, onyx, nova, shimmer\n5. **语速可调**: 0.25-4.0 倍速\n6. **组合使用**: STT + LLM + TTS = 语音助手\n\n---\n\n**下一步**: 学习 [8.4 Video & Realtime](./realtime.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}