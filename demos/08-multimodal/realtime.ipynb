{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# 8.4 Video & Realtime\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-org/ai-first-app/blob/main/demos/08-multimodal/realtime.ipynb)\n\n**预计 API 费用: ~$0.10**\n\n实时语音对话、视频理解概览。\n\n**注意**: Realtime API 使用 WebSocket,代码较复杂,本 Notebook 提供概念演示。"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install -q openai websockets"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\nfrom getpass import getpass\n\nif not os.environ.get('OPENAI_API_KEY'):\n    os.environ['OPENAI_API_KEY'] = getpass('OpenAI API Key: ')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## Realtime API 概念\n\n**特点**:\n- 低延迟: 平均 320ms\n- 支持打断: 用户可随时打断 AI\n- WebSocket 连接: 持久化双向通信\n\n**工作流程**:\n1. 建立 WebSocket 连接\n2. 配置会话 (session.update)\n3. 发送音频输入 (input_audio_buffer.append)\n4. 接收实时响应 (response.audio.delta)\n5. 支持打断 (response.cancel)"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 实验 1: Realtime API 配置示例"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 这是概念代码,实际使用需要 WebSocket 环境\n\nsession_config = {\n    'type': 'session.update',\n    'session': {\n        'modalities': ['text', 'audio'],\n        'instructions': '你是一个友好的助手',\n        'voice': 'alloy',\n        'input_audio_format': 'pcm16',\n        'output_audio_format': 'pcm16',\n        'turn_detection': {\n            'type': 'server_vad'  # 服务端语音活动检测\n        }\n    }\n}\n\nprint('Realtime API 会话配置:')\nimport json\nprint(json.dumps(session_config, indent=2))"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 实验 2: 事件类型"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 客户端发送事件\nclient_events = [\n    'session.update',\n    'input_audio_buffer.append',\n    'input_audio_buffer.commit',\n    'response.create',\n    'response.cancel'\n]\n\n# 服务端返回事件\nserver_events = [\n    'session.created',\n    'input_audio_buffer.speech_started',\n    'input_audio_buffer.speech_stopped',\n    'response.audio.delta',\n    'response.audio.done',\n    'response.text.delta'\n]\n\nprint('客户端事件:', client_events)\nprint('\\n服务端事件:', server_events)"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 实验 3: 成本计算"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Realtime API 价格\naudio_input_price = 100 / 1_000_000  # $100 / 1M tokens\naudio_output_price = 200 / 1_000_000  # $200 / 1M tokens\n\n# 假设 1 分钟对话\n# 输入: 约 1000 tokens\n# 输出: 约 1000 tokens\ninput_tokens = 1000\noutput_tokens = 1000\n\ncost = input_tokens * audio_input_price + output_tokens * audio_output_price\n\nprint(f'1 分钟对话成本: ${cost:.4f}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 视频理解概览\n\n**Gemini 2.0 Flash** 支持视频理解:\n\n```python\n# 概念代码 (需要 Google AI SDK)\nimport google.generativeai as genai\n\nmodel = genai.GenerativeModel('gemini-2.0-flash-exp')\nvideo_file = genai.upload_file(path='video.mp4')\n\nresponse = model.generate_content([\n    '分析这段视频,提取关键信息',\n    video_file\n])\n\nprint(response.text)\n```\n\n**能力**:\n- 场景识别\n- 物体检测\n- 动作识别\n- 时序分析"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 视频生成概览\n\n**主流模型**:\n- **Sora** (OpenAI): 内测,最高质量\n- **Runway Gen-3**: 可用,$0.05/秒\n- **Kling** (快手): 中文支持,¥0.2/秒\n\n**挑战**:\n- 成本高\n- 生成时间长\n- 可控性有限"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 关键要点\n\n1. **Realtime API**: 低延迟 (320ms)、支持打断、WebSocket 连接\n2. **成本高**: 是普通 API 的 10-20 倍\n3. **视频理解**: Gemini 2.0 Flash 支持\n4. **视频生成**: 技术前沿,成本极高\n5. **适用场景**: 语音客服、实时翻译、视频分析\n\n---\n\n**多模态 AI 总结**:\n- Vision: 图像 → 文本\n- Image Gen: 文本 → 图像\n- STT: 语音 → 文本\n- TTS: 文本 → 语音\n- Realtime: 语音 ↔ 语音\n- Video: 视频 ↔ 文本\n\n---\n\n**下一步**: 学习第 9 章 AI Agent"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3"}},
 "nbformat": 4,
 "nbformat_minor": 2
}
