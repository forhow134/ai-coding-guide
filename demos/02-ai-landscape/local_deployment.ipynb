{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Local Model Deployment\n",
    "# 2.4 Local Model Deployment\n",
    "# 2.4 æœ¬åœ°æ¨¡å‹éƒ¨ç½²\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/forhow134/ai-coding-guide/blob/main/demos/02-ai-landscape/local_deployment.ipynb)\n",
    "\n",
    "**é¢„è®¡ API è´¹ç”¨ï¼š$0**ï¼ˆå®Œå…¨æœ¬åœ°è¿è¡Œï¼‰\n",
    "\n",
    "**âš ï¸ é‡è¦æç¤ºï¼š** æœ¬ Notebook å±•ç¤ºçš„æ˜¯ä»£ç æ¨¡å¼ï¼Œä½†éœ€è¦åœ¨æœ¬åœ°å®‰è£…å¹¶è¿è¡Œ Ollamaã€‚Colab ç¯å¢ƒä¸‹æ— æ³•ç›´æ¥è¿è¡Œ Ollama æœåŠ¡ã€‚\n",
    "\n",
    "æœ¬ Notebook æ¼”ç¤ºå¦‚ä½•ï¼š\n",
    "1. é€šè¿‡ OpenAI å…¼å®¹ API è°ƒç”¨æœ¬åœ° Ollama æ¨¡å‹\n",
    "2. åˆ—å‡ºæœ¬åœ°å¯ç”¨æ¨¡å‹\n",
    "3. å¯¹æ¯”æœ¬åœ°æ¨¡å‹ä¸äº‘ç«¯æ¨¡å‹çš„æ€§èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‰ç½®å‡†å¤‡ï¼šå®‰è£… Ollama\n",
    "\n",
    "åœ¨è¿è¡Œæœ¬ Notebook å‰ï¼Œè¯·å…ˆåœ¨æœ¬åœ°å®‰è£… Ollamaï¼š\n",
    "\n",
    "**macOS / Linuxï¼š**\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "**Windowsï¼š**\n",
    "å‰å¾€ [Ollama å®˜ç½‘](https://ollama.com/download) ä¸‹è½½å®‰è£…åŒ…ã€‚\n",
    "\n",
    "**ä¸‹è½½å¹¶è¿è¡Œæ¨¡å‹ï¼š**\n",
    "```bash\n",
    "# ä¸‹è½½ Qwen2.5-7Bï¼ˆçº¦ 4.7GBï¼‰\n",
    "ollama run qwen2.5:7b\n",
    "\n",
    "# æˆ–è€…ä¸‹è½½ Llama 3.2-3Bï¼ˆçº¦ 2GBï¼Œæ›´è½»é‡ï¼‰\n",
    "ollama run llama3.2:3b\n",
    "```\n",
    "\n",
    "**å¯åŠ¨ API æœåŠ¡ï¼š**\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "é»˜è®¤ä¼šåœ¨ `http://localhost:11434` å¯åŠ¨ HTTP APIã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "<!-- å®‰è£…ä¾èµ– -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€\n",
    "\n",
    "ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"âœ… Ollama æœåŠ¡è¿è¡Œæ­£å¸¸\")\n",
    "        \n",
    "        # åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹\n",
    "        models = response.json().get(\"models\", [])\n",
    "        if models:\n",
    "            print(f\"\\nå·²ä¸‹è½½çš„æ¨¡å‹ï¼ˆ{len(models)} ä¸ªï¼‰ï¼š\")\n",
    "            for model in models:\n",
    "                print(f\"  - {model['name']} (å¤§å°: {model['size'] / 1e9:.2f} GB)\")\n",
    "        else:\n",
    "            print(\"\\nâš ï¸  æœªæ‰¾åˆ°å·²ä¸‹è½½çš„æ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œï¼šollama run qwen2.5:7b\")\n",
    "    else:\n",
    "        print(f\"âŒ Ollama æœåŠ¡è¿”å›é”™è¯¯ï¼š{response.status_code}\")\nexcept requests.exceptions.ConnectionError:\n",
    "    print(\"âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡\")\n",
    "    print(\"è¯·ç¡®ä¿ï¼š\")\n",
    "    print(\"  1. å·²å®‰è£… Ollamaï¼ˆhttps://ollama.com/downloadï¼‰\")\n",
    "    print(\"  2. å·²è¿è¡Œ 'ollama serve'\")\n",
    "    print(\"  3. å·²ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚ 'ollama run qwen2.5:7b'ï¼‰\")\nexcept Exception as e:\n",
    "    print(f\"âŒ æ£€æŸ¥å¤±è´¥ï¼š{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basicsç”¨æ³•ï¼šç”¨ OpenAI SDK Callæœ¬åœ°Models\n",
    "<!-- åŸºç¡€ç”¨æ³•ï¼šç”¨ OpenAI SDK è°ƒç”¨æœ¬åœ°æ¨¡å‹ -->\n",
    "\n",
    "Ollama æä¾› OpenAI å…¼å®¹çš„ APIï¼Œåªéœ€ä¿®æ”¹ `base_url` å³å¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# è¿æ¥åˆ°æœ¬åœ° Ollama æœåŠ¡\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\",  # Ollama ä¸éªŒè¯ API keyï¼Œéšä¾¿å¡«\n",
    ")\n",
    "\n",
    "# è°ƒç”¨æœ¬åœ°æ¨¡å‹\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen2.5:7b\",  # æ¨¡å‹åè¦å’Œ ollama list ä¸­çš„ä¸€è‡´\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Python ç¼–ç¨‹åŠ©æ‰‹\"},\n",
    "        {\"role\": \"user\", \"content\": \"ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"ã€Qwen2.5-7B æœ¬åœ°æ¨¡å‹ã€‘\")\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nTokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StreamingOutputï¼šå®æ—¶æ˜¾ç¤ºGenerateå†…å®¹\n",
    "<!-- æµå¼è¾“å‡ºï¼šå®æ—¶æ˜¾ç¤ºç”Ÿæˆå†…å®¹ -->\n",
    "\n",
    "åƒ ChatGPT ä¸€æ ·é€å­—è¾“å‡ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\",\n",
    ")\n",
    "\n",
    "print(\"ã€æµå¼è¾“å‡ºã€‘\")\n",
    "print(\"AI: \", end=\"\", flush=True)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen2.5:7b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"ç”¨ 50 å­—ä»‹ç» Python çš„å†å²\"}],\n",
    "    stream=True,  # å¯ç”¨æµå¼è¾“å‡º\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ—å‡ºæœ¬åœ°å¯ç”¨Models\n",
    "<!-- åˆ—å‡ºæœ¬åœ°å¯ç”¨æ¨¡å‹ -->\n",
    "\n",
    "é€šè¿‡ Ollama API æŸ¥çœ‹å·²ä¸‹è½½çš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def list_local_models():\n",
    "    \"\"\"åˆ—å‡ºæœ¬åœ°æ‰€æœ‰å¯ç”¨çš„ Ollama æ¨¡å‹\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        models = response.json().get(\"models\", [])\n",
    "        \n",
    "        if not models:\n",
    "            print(\"âš ï¸  æœªæ‰¾åˆ°å·²ä¸‹è½½çš„æ¨¡å‹\")\n",
    "            print(\"è¯·è¿è¡Œï¼šollama run qwen2.5:7b\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"æ‰¾åˆ° {len(models)} ä¸ªæœ¬åœ°æ¨¡å‹ï¼š\\n\")\n",
    "        \n",
    "        for model in models:\n",
    "            name = model[\"name\"]\n",
    "            size_gb = model[\"size\"] / 1e9\n",
    "            modified = model.get(\"modified_at\", \"æœªçŸ¥\")\n",
    "            \n",
    "            print(f\"  ğŸ“¦ {name}\")\n",
    "            print(f\"     å¤§å°: {size_gb:.2f} GB\")\n",
    "            print(f\"     æ›´æ–°æ—¶é—´: {modified[:10]}\")\n",
    "            print()\n",
    "        \n",
    "        return [m[\"name\"] for m in models]\n",
    "    \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æŸ¥è¯¢å¤±è´¥ï¼š{e}\")\n",
    "        return []\n",
    "\n",
    "available_models = list_local_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€§èƒ½åŸºå‡†Test\n",
    "<!-- æ€§èƒ½åŸºå‡†æµ‹è¯• -->\n",
    "\n",
    "å¯¹æ¯”æœ¬åœ°æ¨¡å‹ä¸äº‘ç«¯æ¨¡å‹çš„é€Ÿåº¦å’Œè´¨é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "def benchmark_model(client, model, question):\n",
    "    \"\"\"æµ‹è¯•æ¨¡å‹çš„é€Ÿåº¦å’Œè´¨é‡\"\"\"\n",
    "    try:\n",
    "        start = time.time()\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start\n",
    "        answer = response.choices[0].message.content\n",
    "        tokens = response.usage.total_tokens\n",
    "        \n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"latency\": latency,\n",
    "            \"tokens\": tokens,\n",
    "            \"answer\": answer,\n",
    "            \"tokens_per_sec\": tokens / latency,\n",
    "            \"success\": True,\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ¬åœ°æ¨¡å‹å®¢æˆ·ç«¯\n",
    "local_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\",\n",
    ")\n",
    "\n",
    "# Testé—®é¢˜\n",
    "test_question = \"ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯é€’å½’ï¼Ÿ\"\n",
    "\n",
    "print(f\"æµ‹è¯•é—®é¢˜ï¼š{test_question}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Testæœ¬åœ°æ¨¡å‹\n",
    "if available_models:\n",
    "    local_model = available_models[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹\n",
    "    print(f\"\\nã€æœ¬åœ°æ¨¡å‹ã€‘{local_model}\")\n",
    "    \n",
    "    local_result = benchmark_model(local_client, local_model, test_question)\n",
    "    \n",
    "    if local_result[\"success\"]:\n",
    "        print(f\"å›ç­”ï¼š{local_result['answer']}\")\n",
    "        print(f\"\\nâ±ï¸  å»¶è¿Ÿï¼š{local_result['latency']:.2f}s\")\n",
    "        print(f\"ğŸ’° Tokens: {local_result['tokens']}\")\n",
    "        print(f\"âš¡ ååé‡ï¼š{local_result['tokens_per_sec']:.1f} tokens/s\")\n",
    "    else:\n",
    "        print(f\"âŒ æµ‹è¯•å¤±è´¥ï¼š{local_result['error']}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œè·³è¿‡æµ‹è¯•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisonäº‘ç«¯Modelsï¼ˆå¯é€‰ï¼‰\n",
    "<!-- å¯¹æ¯”äº‘ç«¯æ¨¡å‹ï¼ˆå¯é€‰ï¼‰ -->\n",
    "\n",
    "å¦‚æœæœ‰ OpenAI API Keyï¼Œå¯ä»¥å¯¹æ¯”æœ¬åœ°ä¸äº‘ç«¯æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# å¯é€‰ï¼šé…ç½® OpenAI API Key è¿›è¡Œå¯¹æ¯”\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"æç¤ºï¼šè¾“å…¥ OpenAI API Key å¯ä»¥å¯¹æ¯”äº‘ç«¯æ¨¡å‹ï¼ˆæŒ‰ Enter è·³è¿‡ï¼‰\")\n",
    "    key = getpass(\"OpenAI API Key (å¯é€‰): \")\n",
    "    if key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    # äº‘ç«¯æ¨¡å‹å®¢æˆ·ç«¯\n",
    "    cloud_client = OpenAI()\n",
    "    \n",
    "    print(\"\\nã€äº‘ç«¯æ¨¡å‹ã€‘GPT-4o-mini\")\n",
    "    \n",
    "    cloud_result = benchmark_model(cloud_client, \"gpt-4o-mini\", test_question)\n",
    "    \n",
    "    if cloud_result[\"success\"]:\n",
    "        print(f\"å›ç­”ï¼š{cloud_result['answer']}\")\n",
    "        print(f\"\\nâ±ï¸  å»¶è¿Ÿï¼š{cloud_result['latency']:.2f}s\")\n",
    "        print(f\"ğŸ’° Tokens: {cloud_result['tokens']}\")\n",
    "        print(f\"âš¡ ååé‡ï¼š{cloud_result['tokens_per_sec']:.1f} tokens/s\")\n",
    "        print(f\"ğŸ’µ ä¼°ç®—æˆæœ¬ï¼š${(cloud_result['tokens'] * 0.15) / 1_000_000:.6f}\")\n",
    "    \n",
    "    # å¯¹æ¯”æ€»ç»“\n",
    "    if available_models and local_result[\"success\"] and cloud_result[\"success\"]:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"å¯¹æ¯”æ€»ç»“ï¼š\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'æŒ‡æ ‡':<20s} {'æœ¬åœ°æ¨¡å‹':<25s} {'äº‘ç«¯æ¨¡å‹':<25s}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'æ¨¡å‹':<20s} {local_result['model']:<25s} {cloud_result['model']:<25s}\")\n",
    "        print(f\"{'å»¶è¿Ÿ':<20s} {local_result['latency']:.2f}s{'':<21s} {cloud_result['latency']:.2f}s\")\n",
    "        print(f\"{'Tokens':<20s} {local_result['tokens']:<25d} {cloud_result['tokens']:<25d}\")\n",
    "        print(f\"{'ååé‡':<20s} {local_result['tokens_per_sec']:.1f} tokens/s{'':<13s} {cloud_result['tokens_per_sec']:.1f} tokens/s\")\n",
    "        print(f\"{'æˆæœ¬':<20s} {'$0ï¼ˆæœ¬åœ°è¿è¡Œï¼‰':<25s} ${(cloud_result['tokens'] * 0.15) / 1_000_000:.6f}\")\nelse:\n",
    "    print(\"\\nè·³è¿‡äº‘ç«¯æ¨¡å‹å¯¹æ¯”ï¼ˆæœªé…ç½® OPENAI_API_KEYï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¸¸ç”¨Modelsæ¨è\n",
    "<!-- å¸¸ç”¨æ¨¡å‹æ¨è -->\n",
    "\n",
    "æ ¹æ®ä¸åŒåœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š\n",
    "\n",
    "| æ¨¡å‹ | å‚æ•°é‡ | å†…å­˜éœ€æ±‚ | æ“…é•¿é¢†åŸŸ | ä¸‹è½½å‘½ä»¤ |\n",
    "|-----|-------|---------|---------|----------|\n",
    "| **qwen2.5:7b** | 7B | ~8GB | ä¸­æ–‡ã€ä»£ç ã€é€šç”¨ | `ollama run qwen2.5:7b` |\n",
    "| **llama3.2:3b** | 3B | ~4GB | è‹±æ–‡ã€é€Ÿåº¦å¿«ã€èµ„æºå°‘ | `ollama run llama3.2:3b` |\n",
    "| **deepseek-r1:7b** | 7B | ~8GB | æ¨ç†ã€æ•°å­¦ã€é€»è¾‘ | `ollama run deepseek-r1:7b` |\n",
    "| **codellama:7b** | 7B | ~8GB | ä»£ç ç”Ÿæˆã€è¡¥å…¨ | `ollama run codellama:7b` |\n",
    "| **mistral:7b** | 7B | ~8GB | å¤šè¯­è¨€ã€é€šç”¨ | `ollama run mistral:7b` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Exercises\n",
    "<!-- åŠ¨æ‰‹ç»ƒä¹  -->\n",
    "\n",
    "1. ä¸‹è½½å¤šä¸ªä¸åŒå¤§å°çš„æ¨¡å‹ï¼ˆ3Bã€7Bã€14Bï¼‰ï¼Œå¯¹æ¯”å®ƒä»¬çš„é€Ÿåº¦å’Œè´¨é‡\n",
    "2. å°è¯•åˆ›å»º Modelfile è‡ªå®šä¹‰æ¨¡å‹ï¼ˆæ·»åŠ ç³»ç»Ÿæç¤ºè¯ã€è°ƒæ•´å‚æ•°ï¼‰\n",
    "3. å®ç°ä¸€ä¸ªæ™ºèƒ½è·¯ç”±ï¼šæ ¹æ®é—®é¢˜å¤æ‚åº¦è‡ªåŠ¨é€‰æ‹©æœ¬åœ°æ¨¡å‹æˆ–äº‘ç«¯æ¨¡å‹\n",
    "4. æµ‹è¯•æµå¼è¾“å‡ºçš„é€å­—æ˜¾ç¤ºæ•ˆæœ\n",
    "\n",
    "## Summary\n",
    "<!-- å°ç»“ -->\n",
    "\n",
    "- **æœ¬åœ°éƒ¨ç½²çš„ä¼˜åŠ¿**ï¼š\n",
    "  - âœ… é›¶ API æˆæœ¬ï¼Œé€‚åˆå¤§é‡è°ƒç”¨\n",
    "  - âœ… æ•°æ®éšç§ï¼Œä¸ä¸Šä¼ åˆ°äº‘ç«¯\n",
    "  - âœ… ç¦»çº¿å¯ç”¨ï¼Œä¸ä¾èµ–ç½‘ç»œ\n",
    "  - âœ… ä½å»¶è¿Ÿï¼Œé€‚åˆå®æ—¶äº¤äº’\n",
    "\n",
    "- **æœ¬åœ°éƒ¨ç½²çš„åŠ£åŠ¿**ï¼š\n",
    "  - âŒ ç¡¬ä»¶è¦æ±‚ï¼š7B æ¨¡å‹éœ€è¦ 8GB+ å†…å­˜\n",
    "  - âŒ èƒ½åŠ›è¾ƒå¼±ï¼š7B æ¨¡å‹å¼±äº GPT-4o / Claude Opus\n",
    "  - âŒ ç»´æŠ¤æˆæœ¬ï¼šéœ€è¦æ‰‹åŠ¨æ›´æ–°æ¨¡å‹\n",
    "\n",
    "**å…³é”®è¦ç‚¹ï¼š**\n",
    "- Ollama æä¾› OpenAI å…¼å®¹ APIï¼Œåªéœ€æ”¹ `base_url`\n",
    "- æ¨èæ¨¡å‹ï¼šQwen2.5-7Bï¼ˆä¸­æ–‡+ä»£ç ï¼‰ã€Llama3.2-3Bï¼ˆè½»é‡ï¼‰\n",
    "- é‡åŒ–æŠ€æœ¯å¯å‡å°‘ 70% å†…å­˜å ç”¨\n",
    "- é€‚åˆåœºæ™¯ï¼šæ•°æ®éšç§ã€æˆæœ¬æ§åˆ¶ã€ç¦»çº¿ç¯å¢ƒ\n",
    "\n",
    "ä¸‹ä¸€æ­¥ï¼šæ·±å…¥å­¦ä¹  LLM çš„å·¥ä½œåŸç† â†’ [ç¬¬ 3 ç« ï¼šLLM æ ¸å¿ƒåŸç†](../../docs/03-llm-fundamentals/index.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}