{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 ä¸»æµæ¨¡å‹æä¾›å•†å¯¹æ¯”\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-org/ai-first-app/blob/main/demos/02-ai-landscape/model_providers.ipynb)\n",
    "\n",
    "**é¢„è®¡ API è´¹ç”¨ï¼š~$0.01**\n",
    "\n",
    "æœ¬ Notebook æ¼”ç¤ºå¦‚ä½•è°ƒç”¨ OpenAI GPT-4o-miniã€Google Gemini 2.0 Flash å’Œ DeepSeek Chatï¼Œå¯¹æ¯”å®ƒä»¬çš„å›ç­”è´¨é‡ã€é€Ÿåº¦å’Œ token ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é…ç½® API Keys\n",
    "\n",
    "éœ€è¦å‡†å¤‡ä¸‰ä¸ª API Keyï¼š\n",
    "- **OpenAI API Key**ï¼šä» [OpenAI Platform](https://platform.openai.com/api-keys) è·å–\n",
    "- **Google API Key**ï¼šä» [Google AI Studio](https://aistudio.google.com/apikey) è·å–\n",
    "- **DeepSeek API Key**ï¼šä» [DeepSeek Platform](https://platform.deepseek.com/api-keys) è·å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"è¯·è¾“å…¥ OpenAI API Key: \")\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"è¯·è¾“å…¥ Google API Key: \")\n",
    "\n",
    "if not os.environ.get(\"DEEPSEEK_API_KEY\"):\n",
    "    os.environ[\"DEEPSEEK_API_KEY\"] = getpass(\"è¯·è¾“å…¥ DeepSeek API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯¹æ¯”ä¸‰ä¸ªæ¨¡å‹çš„å›ç­”\n",
    "\n",
    "æˆ‘ä»¬ç”¨åŒä¸€ä¸ªé—®é¢˜æµ‹è¯•ä¸‰ä¸ªæ¨¡å‹ï¼Œå¯¹æ¯”å®ƒä»¬çš„å›ç­”è´¨é‡ã€é€Ÿåº¦å’Œ token ç”¨é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "\n",
    "# æµ‹è¯•é—®é¢˜\n",
    "question = \"ç”¨ä¸€ä¸ªç”Ÿæ´»ä¸­çš„ç±»æ¯”ï¼Œè§£é‡Šä»€ä¹ˆæ˜¯ APIï¼Ÿè¦æ±‚ï¼šç®€æ´ã€æœ‰è¶£ã€æ˜“æ‡‚\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"é—®é¢˜ï¼š\", question)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. OpenAI GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nã€OpenAI GPT-4o-miniã€‘\")\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "start_time = time.time()\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    ")\n",
    "latency = time.time() - start_time\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nâ±ï¸  å»¶è¿Ÿï¼š{latency:.2f}s\")\n",
    "print(f\"ğŸ’° Token ç”¨é‡ï¼šè¾“å…¥ {response.usage.prompt_tokens}, è¾“å‡º {response.usage.completion_tokens}, æ€»è®¡ {response.usage.total_tokens}\")\n",
    "print(f\"ğŸ’µ ä¼°ç®—æˆæœ¬ï¼š${(response.usage.prompt_tokens * 0.15 + response.usage.completion_tokens * 0.60) / 1_000_000:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Google Gemini 2.0 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nã€Google Gemini 2.0 Flashã€‘\")\n",
    "\n",
    "google_client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "start_time = time.time()\n",
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=question,\n",
    ")\n",
    "latency = time.time() - start_time\n",
    "\n",
    "print(response.text)\n",
    "print(f\"\\nâ±ï¸  å»¶è¿Ÿï¼š{latency:.2f}s\")\n",
    "print(f\"ğŸ’° Token ç”¨é‡ï¼šè¾“å…¥ {response.usage_metadata.prompt_token_count}, è¾“å‡º {response.usage_metadata.candidates_token_count}, æ€»è®¡ {response.usage_metadata.total_token_count}\")\n",
    "print(f\"ğŸ’µ ä¼°ç®—æˆæœ¬ï¼š${(response.usage_metadata.prompt_token_count * 0.075 + response.usage_metadata.candidates_token_count * 0.30) / 1_000_000:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DeepSeek Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nã€DeepSeek Chatã€‘\")\n",
    "\n",
    "# DeepSeek å…¼å®¹ OpenAI API\n",
    "deepseek_client = OpenAI(\n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "response = deepseek_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    ")\n",
    "latency = time.time() - start_time\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(f\"\\nâ±ï¸  å»¶è¿Ÿï¼š{latency:.2f}s\")\n",
    "print(f\"ğŸ’° Token ç”¨é‡ï¼šè¾“å…¥ {response.usage.prompt_tokens}, è¾“å‡º {response.usage.completion_tokens}, æ€»è®¡ {response.usage.total_tokens}\")\n",
    "print(f\"ğŸ’µ ä¼°ç®—æˆæœ¬ï¼š${(response.usage.prompt_tokens * 0.27 + response.usage.completion_tokens * 1.10) / 1_000_000:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¿›é˜¶ï¼šæ‰¹é‡å¯¹æ¯”å¤šä¸ªæ¨¡å‹\n",
    "\n",
    "åˆ›å»ºä¸€ä¸ªé€šç”¨çš„å¯¹æ¯”å‡½æ•°ï¼Œæ–¹ä¾¿æµ‹è¯•ä¸åŒé—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(question, test_cases):\n",
    "    \"\"\"\n",
    "    å¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„å›ç­”\n",
    "    \n",
    "    test_cases æ ¼å¼ï¼š[\n",
    "        {\n",
    "            \"name\": \"æ¨¡å‹åç§°\",\n",
    "            \"call\": lambda: è°ƒç”¨æ¨¡å‹çš„å‡½æ•°,\n",
    "            \"extract\": lambda response: æå–å›ç­”å’Œ token çš„å‡½æ•°,\n",
    "        },\n",
    "    ]\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"é—®é¢˜ï¼š{question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for case in test_cases:\n",
    "        print(f\"\\nã€{case['name']}ã€‘\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = case[\"call\"]()\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            answer, tokens = case[\"extract\"](response)\n",
    "            \n",
    "            print(answer)\n",
    "            print(f\"\\nâ±ï¸  å»¶è¿Ÿï¼š{latency:.2f}s | ğŸ’° Tokens: {tokens}\")\n",
    "            \n",
    "            results.append({\n",
    "                \"model\": case[\"name\"],\n",
    "                \"latency\": latency,\n",
    "                \"tokens\": tokens,\n",
    "                \"answer_length\": len(answer),\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è°ƒç”¨å¤±è´¥ï¼š{e}\")\n",
    "    \n",
    "    # æ‰“å°å¯¹æ¯”æ€»ç»“\n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"å¯¹æ¯”æ€»ç»“ï¼š\")\n",
    "        print(\"=\" * 80)\n",
    "        for r in results:\n",
    "            print(f\"{r['model']:20s} | å»¶è¿Ÿ: {r['latency']:.2f}s | Tokens: {r['tokens']:4d} | å›ç­”é•¿åº¦: {r['answer_length']} å­—\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰æµ‹è¯•ç”¨ä¾‹\n",
    "test_question = \"ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯ Dockerï¼Ÿ\"\n",
    "\n",
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"OpenAI GPT-4o-mini\",\n",
    "        \"call\": lambda: openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": test_question}],\n",
    "        ),\n",
    "        \"extract\": lambda r: (r.choices[0].message.content, r.usage.total_tokens),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Google Gemini 2.0 Flash\",\n",
    "        \"call\": lambda: google_client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=test_question,\n",
    "        ),\n",
    "        \"extract\": lambda r: (r.text, r.usage_metadata.total_token_count),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DeepSeek Chat\",\n",
    "        \"call\": lambda: deepseek_client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[{\"role\": \"user\", \"content\": test_question}],\n",
    "        ),\n",
    "        \"extract\": lambda r: (r.choices[0].message.content, r.usage.total_tokens),\n",
    "    },\n",
    "]\n",
    "\n",
    "# è¿è¡Œå¯¹æ¯”\n",
    "results = compare_models(test_question, test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŠ¨æ‰‹ç»ƒä¹ \n",
    "\n",
    "1. ä¿®æ”¹ `question`ï¼Œæµ‹è¯•ä¸‰ä¸ªæ¨¡å‹åœ¨ä¸åŒä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼ˆå¦‚å†™ä»£ç ã€å†™æ–‡æ¡ˆã€æ•°å­¦æ¨ç†ï¼‰\n",
    "2. è§‚å¯Ÿå“ªä¸ªæ¨¡å‹é€Ÿåº¦æœ€å¿«ï¼Ÿå“ªä¸ªå›ç­”æœ€è¯¦ç»†ï¼Ÿå“ªä¸ªæœ€ä¾¿å®œï¼Ÿ\n",
    "3. å°è¯•æ·»åŠ æ›´å¤šæ¨¡å‹ï¼ˆå¦‚ Qwenã€Claudeï¼‰åˆ°å¯¹æ¯”å‡½æ•°ä¸­\n",
    "\n",
    "## å°ç»“\n",
    "\n",
    "- **OpenAI GPT-4o-mini**ï¼šè´¨é‡ç¨³å®šï¼Œç”Ÿæ€æœ€å¥½ï¼Œä»·æ ¼ä¸­ç­‰\n",
    "- **Google Gemini 2.0 Flash**ï¼šé€Ÿåº¦å¿«ï¼Œä»·æ ¼æœ€ä½ï¼Œå…è´¹é¢åº¦å¤§\n",
    "- **DeepSeek Chat**ï¼šæ€§ä»·æ¯”é«˜ï¼Œæ¨ç†èƒ½åŠ›å¼ºï¼Œä¸­æ–‡å‹å¥½\n",
    "\n",
    "**é€‰å‹å»ºè®®ï¼š**\n",
    "- é«˜è´¨é‡åœºæ™¯ï¼šOpenAI GPT-4o / Claude Opus\n",
    "- æ€§ä»·æ¯”åœºæ™¯ï¼šGemini Flash / DeepSeek / GPT-4o-mini\n",
    "- å¤šæ¨¡æ€åœºæ™¯ï¼šGemini 2.0 Flash / GPT-4o\n",
    "\n",
    "ä¸‹ä¸€æ­¥ï¼šå­¦ä¹ å¦‚ä½•ç”¨èšåˆå¹³å°ç»Ÿä¸€è°ƒç”¨è¿™äº›æ¨¡å‹ â†’ [2.3 èšåˆå¹³å°ç»Ÿä¸€è°ƒç”¨](./aggregators.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
