{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 聚合平台统一调用\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-org/ai-first-app/blob/main/demos/02-ai-landscape/aggregators.ipynb)\n",
    "\n",
    "**预计 API 费用：$0**（使用 OpenRouter 的免费模型测试）\n",
    "\n",
    "本 Notebook 演示如何使用 OpenRouter 聚合平台，用一套代码调用多个模型，并实现模型降级（fallback）和异步多模型对比。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置 OpenRouter API Key\n",
    "\n",
    "1. 前往 [OpenRouter](https://openrouter.ai/) 注册\n",
    "2. 在 [Keys](https://openrouter.ai/keys) 页面创建 API Key\n",
    "3. （可选）充值最低 $5，或使用免费模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"请输入 OpenRouter API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础用法：用 OpenAI SDK 调用 OpenRouter\n",
    "\n",
    "OpenRouter 完全兼容 OpenAI API 格式，只需修改 `base_url` 和 `model` 参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 创建 OpenRouter 客户端\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    ")\n",
    "\n",
    "# 调用 GPT-4o-mini\n",
    "response = client.chat.completions.create(\n",
    "    model=\"openai/gpt-4o-mini\",  # 格式：提供商/模型名\n",
    "    messages=[{\"role\": \"user\", \"content\": \"用一句话介绍 Python\"}],\n",
    ")\n",
    "\n",
    "print(\"【OpenAI GPT-4o-mini】\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一套代码调用多个模型\n",
    "\n",
    "通过改变 `model` 参数，可以无缝切换不同提供商的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"用一句话解释什么是 API？\"\n",
    "\n",
    "models = [\n",
    "    \"openai/gpt-4o-mini\",\n",
    "    \"google/gemini-2.0-flash-exp:free\",  # 免费模型\n",
    "    \"meta-llama/llama-3.2-3b-instruct:free\",  # 免费模型\n",
    "]\n",
    "\n",
    "print(f\"问题：{question}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model in models:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n【{model}】\")\n",
    "        print(response.choices[0].message.content)\n",
    "        print(f\"Tokens: {response.usage.total_tokens}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n【{model}】\")\n",
    "        print(f\"❌ 调用失败：{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型自动降级（Fallback Pattern）\n",
    "\n",
    "在生产环境中，主模型可能因为限流、服务中断等原因调用失败。我们可以实现自动降级到备选模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_fallback(user_message, models, timeout=10):\n",
    "    \"\"\"\n",
    "    优先使用高质量模型，失败时自动降级到备选模型\n",
    "    \n",
    "    Args:\n",
    "        user_message: 用户消息\n",
    "        models: 模型列表，按优先级排序\n",
    "        timeout: 单次调用超时时间（秒）\n",
    "    \n",
    "    Returns:\n",
    "        (answer, model_used) 或 (None, None)\n",
    "    \"\"\"\n",
    "    for model in models:\n",
    "        try:\n",
    "            print(f\"尝试调用：{model}\")\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "                timeout=timeout,\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            print(f\"✅ 成功！使用模型：{model}\\n\")\n",
    "            \n",
    "            return answer, model\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 失败：{str(e)[:100]}\\n\")\n",
    "            continue\n",
    "    \n",
    "    print(\"⚠️  所有模型均调用失败\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试自动降级\n",
    "fallback_models = [\n",
    "    \"openai/gpt-4o\",                        # 主力：质量最高但最贵\n",
    "    \"openai/gpt-4o-mini\",                   # 备选 1：性价比高\n",
    "    \"google/gemini-2.0-flash-exp:free\",     # 备选 2：免费\n",
    "    \"meta-llama/llama-3.2-3b-instruct:free\",  # 备选 3：免费备份\n",
    "]\n",
    "\n",
    "answer, model_used = chat_with_fallback(\n",
    "    \"用一句话解释什么是递归\",\n",
    "    fallback_models\n",
    ")\n",
    "\n",
    "if answer:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"最终使用模型：{model_used}\")\n",
    "    print(f\"回答：{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异步并发：同时对比多个模型\n",
    "\n",
    "用 `asyncio` 并发调用多个模型，快速对比回答质量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "async def compare_models_async(question, models):\n",
    "    \"\"\"\n",
    "    并发调用多个模型，对比回答\n",
    "    \"\"\"\n",
    "    client = AsyncOpenAI(\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "    )\n",
    "    \n",
    "    async def call_model(model):\n",
    "        try:\n",
    "            response = await client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": question}],\n",
    "                timeout=30,\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"model\": model,\n",
    "                \"answer\": response.choices[0].message.content,\n",
    "                \"tokens\": response.usage.total_tokens,\n",
    "                \"success\": True,\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"model\": model,\n",
    "                \"error\": str(e)[:100],\n",
    "                \"success\": False,\n",
    "            }\n",
    "    \n",
    "    # 并发调用所有模型\n",
    "    tasks = [call_model(model) for model in models]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试：同时对比 5 个模型\n",
    "test_question = \"用一个生活中的类比，解释什么是 Docker？\"\n",
    "\n",
    "comparison_models = [\n",
    "    \"openai/gpt-4o-mini\",\n",
    "    \"google/gemini-2.0-flash-exp:free\",\n",
    "    \"meta-llama/llama-3.2-3b-instruct:free\",\n",
    "    \"mistralai/mistral-7b-instruct:free\",\n",
    "    \"qwen/qwen-2.5-7b-instruct:free\",\n",
    "]\n",
    "\n",
    "print(f\"问题：{test_question}\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"正在并发调用 5 个模型...\\n\")\n",
    "\n",
    "results = await compare_models_async(test_question, comparison_models)\n",
    "\n",
    "# 显示结果\n",
    "for result in results:\n",
    "    print(f\"\\n【{result['model']}】\")\n",
    "    \n",
    "    if result['success']:\n",
    "        print(result['answer'])\n",
    "        print(f\"\\nTokens: {result['tokens']}\")\n",
    "    else:\n",
    "        print(f\"❌ 调用失败：{result['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"对比完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 智能路由：自动选择最优模型\n",
    "\n",
    "OpenRouter 支持自动路由策略，可以根据价格、速度、质量自动选择模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动选择最便宜的模型\n",
    "response = client.chat.completions.create(\n",
    "    model=\"openrouter/auto\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"你好\"}],\n",
    "    extra_body={\n",
    "        \"route\": \"cheapest\",  # 按价格选（cheapest / fastest / best）\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"【自动路由：最便宜】\")\n",
    "print(f\"使用的模型：{response.model}\")\n",
    "print(f\"回答：{response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动手练习\n",
    "\n",
    "1. 尝试添加更多模型到对比列表中（查看 [OpenRouter 模型列表](https://openrouter.ai/models)）\n",
    "2. 实现一个智能选择器：根据问题类型（编程、写作、数学）自动选择最合适的模型\n",
    "3. 测试降级策略的响应时间：主模型失败后多久能切换到备选模型？\n",
    "\n",
    "## 小结\n",
    "\n",
    "- **聚合平台解决了什么**：用一套代码（OpenAI SDK）调用 200+ 模型，降低维护成本\n",
    "- **Fallback Pattern**：实现高可用性，主模型失败时自动降级\n",
    "- **异步并发**：快速对比多个模型的回答质量\n",
    "- **智能路由**：根据价格、速度、质量自动选择最优模型\n",
    "\n",
    "**关键要点：**\n",
    "- 只需改 `base_url` 和 `model` 参数，无需修改其他代码\n",
    "- 模型名格式：`提供商/模型名`（如 `openai/gpt-4o-mini`）\n",
    "- OpenRouter 支持 200+ 模型，价格与官方相同\n",
    "\n",
    "下一步：学习如何在本地部署开源模型，实现零成本运行 → [2.4 本地模型部署](./local_deployment.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
