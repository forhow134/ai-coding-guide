{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 æ¨ç†å‚æ•°å®éªŒ\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬å°†é€šè¿‡å®é™…å®éªŒ,å¸®åŠ©ä½ ç†è§£å’ŒæŒæ¡ LLM çš„å…³é”®æ¨ç†å‚æ•°ã€‚\n",
    "\n",
    "**é¢„ä¼°æˆæœ¬**: ~$0.02 (ä½¿ç”¨ GPT-4o-mini)\n",
    "\n",
    "**å®‰è£…ä¾èµ–**:\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Initializeå®¢æˆ·ç«¯\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# æ£€æŸ¥ API key\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"âš ï¸  è­¦å‘Š: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡\")\n",
    "    print(\"è¯·è¿è¡Œ: export OPENAI_API_KEY='your-api-key'\")\n",
    "else:\n",
    "    print(\"âœ“ API key å·²è®¾ç½®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str, \n",
    "             model: str = \"gpt-4o-mini\",\n",
    "             temperature: float = 1.0,\n",
    "             top_p: float = 1.0,\n",
    "             frequency_penalty: float = 0.0,\n",
    "             presence_penalty: float = 0.0,\n",
    "             max_tokens: int = 500) -> Dict[str, Any]:\n",
    "    \"\"\"è°ƒç”¨ LLM API å¹¶è¿”å›ç»“æœ\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"content\": response.choices[0].message.content,\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens\n",
    "        },\n",
    "        \"model\": response.model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 1: Temperature ParametersExperiment\n",
    "<!-- Demo 1: Temperature å‚æ•°å®éªŒ -->\n",
    "\n",
    "**Temperature** æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§:\n",
    "- **0**: ç¡®å®šæ€§è¾“å‡º,æ¯æ¬¡ç»“æœç›¸åŒ\n",
    "- **0.5**: ä¸­ç­‰éšæœºæ€§\n",
    "- **1.0**: é»˜è®¤å€¼,å¹³è¡¡åˆ›é€ æ€§å’Œä¸€è‡´æ€§\n",
    "- **1.5+**: é«˜éšæœºæ€§,æ›´æœ‰åˆ›æ„ä½†å¯èƒ½ä¸è¿è´¯\n",
    "\n",
    "æˆ‘ä»¬å°†ç”¨åŒä¸€ä¸ªæç¤ºè¯æµ‹è¯•ä¸åŒæ¸©åº¦,æ¯ä¸ªæ¸©åº¦è¿è¡Œ 3 æ¬¡è§‚å¯Ÿå˜åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"ç”¨ä¸€å¥è¯æè¿°äººå·¥æ™ºèƒ½çš„æœªæ¥ã€‚\"\n",
    "temperatures = [0, 0.5, 1.0, 1.5]\n",
    "\n",
    "print(f\"æç¤ºè¯: {prompt}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nğŸŒ¡ï¸  Temperature = {temp}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    responses = []\n",
    "    for i in range(3):\n",
    "        result = call_llm(prompt, temperature=temp, max_tokens=100)\n",
    "        responses.append(result[\"content\"])\n",
    "        print(f\"\\nç¬¬ {i+1} æ¬¡: {result['content']}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å“åº”éƒ½ç›¸åŒ\n",
    "    if len(set(responses)) == 1:\n",
    "        print(\"\\nğŸ“Œ æ‰€æœ‰ 3 æ¬¡è¾“å‡ºå®Œå…¨ç›¸åŒ (ç¡®å®šæ€§)\")\n",
    "    else:\n",
    "        unique_count = len(set(responses))\n",
    "        print(f\"\\nğŸ“Œ 3 æ¬¡è¾“å‡ºä¸­æœ‰ {unique_count} ä¸ªä¸åŒç‰ˆæœ¬ (éšæœºæ€§)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nè§‚å¯Ÿ:\")\n",
    "print(\"- Temperature=0 æ—¶è¾“å‡ºå®Œå…¨ä¸€è‡´\")\n",
    "print(\"- Temperature è¶Šé«˜,è¾“å‡ºè¶Šå¤šæ ·åŒ–\")\n",
    "print(\"- Temperature=1.5 æ—¶å¯èƒ½å‡ºç°æ›´æœ‰åˆ›æ„ä½†é£é™©æ›´é«˜çš„è¾“å‡º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 2: Temperature vs Top-p Comparison\n",
    "<!-- Demo 2: Temperature vs Top-p å¯¹æ¯” -->\n",
    "\n",
    "**Top-p (nucleus sampling)** æ˜¯å¦ä¸€ç§æ§åˆ¶éšæœºæ€§çš„æ–¹æ³•:\n",
    "- åªä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„ token ä¸­é‡‡æ ·\n",
    "- top_p=0.1: éå¸¸ä¿å®ˆ,åªé€‰æ‹©æœ€å¯èƒ½çš„ tokens\n",
    "- top_p=0.9: å¹³è¡¡é€‰æ‹©\n",
    "- top_p=1.0: è€ƒè™‘æ‰€æœ‰å¯èƒ½çš„ tokens\n",
    "\n",
    "**æ³¨æ„**: OpenAI å»ºè®®åªè°ƒæ•´ temperature æˆ– top_p,ä¸è¦åŒæ—¶è°ƒæ•´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creative_prompt = \"åˆ›ä½œä¸€ä¸ªå…³äºæœºå™¨äººå­¦ä¼šåšæ¢¦çš„æ•…äº‹å¼€å¤´ã€‚\"\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"Temperature=0.7\", \"temperature\": 0.7, \"top_p\": 1.0},\n",
    "    {\"name\": \"Top-p=0.3 (ä¿å®ˆ)\", \"temperature\": 1.0, \"top_p\": 0.3},\n",
    "    {\"name\": \"Top-p=0.9 (å¹³è¡¡)\", \"temperature\": 1.0, \"top_p\": 0.9},\n",
    "]\n",
    "\n",
    "print(f\"æç¤ºè¯: {creative_prompt}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nâš™ï¸  {config['name']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = call_llm(\n",
    "        creative_prompt,\n",
    "        temperature=config[\"temperature\"],\n",
    "        top_p=config[\"top_p\"],\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{result['content']}\")\n",
    "    print(f\"\\nğŸ“Š Tokens: {result['usage']['completion_tokens']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nè§‚å¯Ÿ:\")\n",
    "print(\"- Top-p=0.3 å€¾å‘äºä½¿ç”¨æ›´å¸¸è§ã€å®‰å…¨çš„è¯æ±‡\")\n",
    "print(\"- Top-p=0.9 å…è®¸æ›´å¤šæ ·åŒ–çš„è¯æ±‡é€‰æ‹©\")\n",
    "print(\"- åˆ›æ„ä»»åŠ¡é€šå¸¸ä½¿ç”¨è¾ƒé«˜çš„ top_p å€¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3: Frequency Penalty å’Œ Presence Penalty\n",
    "\n",
    "è¿™ä¸¤ä¸ªå‚æ•°æ§åˆ¶é‡å¤æ€§:\n",
    "\n",
    "**Frequency Penalty** (é¢‘ç‡æƒ©ç½š, -2.0 åˆ° 2.0):\n",
    "- æ ¹æ® token å·²ç»å‡ºç°çš„æ¬¡æ•°è¿›è¡Œæƒ©ç½š\n",
    "- æ­£å€¼é™ä½é‡å¤,è´Ÿå€¼å¢åŠ é‡å¤\n",
    "\n",
    "**Presence Penalty** (å­˜åœ¨æƒ©ç½š, -2.0 åˆ° 2.0):\n",
    "- æ ¹æ® token æ˜¯å¦å·²ç»å‡ºç°è¿›è¡Œæƒ©ç½š(ä¸è€ƒè™‘æ¬¡æ•°)\n",
    "- æ­£å€¼é¼“åŠ±è°ˆè®ºæ–°è¯é¢˜,è´Ÿå€¼é¼“åŠ±ä¿æŒè¯é¢˜\n",
    "\n",
    "è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå®¹æ˜“äº§ç”Ÿé‡å¤çš„ä»»åŠ¡æ¥æµ‹è¯•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitive_prompt = \"åˆ—ä¸¾ 10 ä¸ªå­¦ä¹ ç¼–ç¨‹çš„å¥½å¤„,æ¯ä¸ªç”¨ä¸€å¥è¯è¯´æ˜ã€‚\"\n",
    "\n",
    "penalty_configs = [\n",
    "    {\"name\": \"æ— æƒ©ç½š (é»˜è®¤)\", \"freq\": 0.0, \"pres\": 0.0},\n",
    "    {\"name\": \"é«˜é¢‘ç‡æƒ©ç½š\", \"freq\": 1.5, \"pres\": 0.0},\n",
    "    {\"name\": \"é«˜å­˜åœ¨æƒ©ç½š\", \"freq\": 0.0, \"pres\": 1.5},\n",
    "    {\"name\": \"ä¸¤è€…ç»“åˆ\", \"freq\": 1.0, \"pres\": 1.0},\n",
    "]\n",
    "\n",
    "print(f\"æç¤ºè¯: {repetitive_prompt}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for config in penalty_configs:\n",
    "    print(f\"\\nâš™ï¸  {config['name']}\")\n",
    "    print(f\"   (frequency_penalty={config['freq']}, presence_penalty={config['pres']})\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = call_llm(\n",
    "        repetitive_prompt,\n",
    "        temperature=0.7,\n",
    "        frequency_penalty=config[\"freq\"],\n",
    "        presence_penalty=config[\"pres\"],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{result['content']}\")\n",
    "    \n",
    "    # ç®€å•çš„é‡å¤æ£€æµ‹\n",
    "    words = result['content'].lower().split()\n",
    "    unique_ratio = len(set(words)) / len(words) if words else 0\n",
    "    print(f\"\\nğŸ“Š è¯æ±‡å¤šæ ·æ€§: {unique_ratio:.2%} ({len(set(words))}/{len(words)} ä¸ªç‹¬ç‰¹è¯)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nè§‚å¯Ÿ:\")\n",
    "print(\"- Frequency penalty å‡å°‘å¸¸ç”¨è¯å’ŒçŸ­è¯­çš„é‡å¤\")\n",
    "print(\"- Presence penalty é¼“åŠ±å¼•å…¥æ–°è¯é¢˜å’Œæ¦‚å¿µ\")\n",
    "print(\"- ä¸¤è€…ç»“åˆå¯ä»¥äº§ç”Ÿæ›´å¤šæ ·åŒ–çš„è¾“å‡º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 4: ä¸åŒä»»åŠ¡çš„æœ€ä½³Parameters\n",
    "<!-- Demo 4: ä¸åŒä»»åŠ¡çš„æœ€ä½³å‚æ•° -->\n",
    "\n",
    "æ ¹æ®ä»»åŠ¡ç±»å‹,æ¨èä¸åŒçš„å‚æ•°é…ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ä¸åŒä»»åŠ¡åœºæ™¯å’Œæ¨èå‚æ•°\n",
    "scenarios = [\n",
    "    {\n",
    "        \"task\": \"æ•°æ®æå–\",\n",
    "        \"prompt\": 'ä»ä»¥ä¸‹æ–‡æœ¬æå–äººåã€å…¬å¸åå’Œæ—¥æœŸ:\\n\"å¼ ä¸‰äº2024å¹´1æœˆåŠ å…¥é˜¿é‡Œå·´å·´å…¬å¸,æ‹…ä»»é«˜çº§å·¥ç¨‹å¸ˆã€‚\"',\n",
    "        \"params\": {\"temperature\": 0, \"top_p\": 1.0, \"frequency_penalty\": 0, \"presence_penalty\": 0},\n",
    "        \"reason\": \"éœ€è¦ç¡®å®šæ€§è¾“å‡º,ä¸éœ€è¦åˆ›é€ æ€§\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"åˆ›æ„å†™ä½œ\",\n",
    "        \"prompt\": \"å†™ä¸€å¥èµ›åšæœ‹å…‹é£æ ¼çš„å¹¿å‘Šè¯­,æ¨å¹¿ä¸€æ¬¾ AI åŠ©æ‰‹ã€‚\",\n",
    "        \"params\": {\"temperature\": 0.9, \"top_p\": 1.0, \"frequency_penalty\": 0.5, \"presence_penalty\": 0.5},\n",
    "        \"reason\": \"éœ€è¦é«˜åˆ›é€ æ€§å’Œè¯æ±‡å¤šæ ·æ€§\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"ä»£ç ç”Ÿæˆ\",\n",
    "        \"prompt\": \"ç”¨ Python å†™ä¸€ä¸ªå‡½æ•°,è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬ n é¡¹ã€‚\",\n",
    "        \"params\": {\"temperature\": 0.2, \"top_p\": 1.0, \"frequency_penalty\": 0, \"presence_penalty\": 0},\n",
    "        \"reason\": \"éœ€è¦ç²¾ç¡®æ€§,ä½†å…è®¸å°‘é‡å˜åŒ–(å¦‚å˜é‡å‘½å)\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"å¤´è„‘é£æš´\",\n",
    "        \"prompt\": \"ä¸ºä¸€ä¸ªé¢å‘å¼€å‘è€…çš„æŠ€æœ¯åšå®¢æƒ³ 5 ä¸ªåˆ›æ–°æ ç›®åç§°ã€‚\",\n",
    "        \"params\": {\"temperature\": 0.8, \"top_p\": 0.95, \"frequency_penalty\": 1.0, \"presence_penalty\": 0.6},\n",
    "        \"reason\": \"éœ€è¦å¤šæ ·æ€§å’Œæ–°é¢–æ€§,é¿å…é‡å¤æƒ³æ³•\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"æ–‡æœ¬æ‘˜è¦\",\n",
    "        \"prompt\": \"ç”¨ä¸€å¥è¯æ€»ç»“: LLM é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šé¢„è®­ç»ƒå­¦ä¹ è¯­è¨€æ¨¡å¼,ç„¶åå¯ä»¥é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚å®ƒä»¬ä½¿ç”¨ Transformer æ¶æ„,èƒ½å¤Ÿå¤„ç†é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚\",\n",
    "        \"params\": {\"temperature\": 0.3, \"top_p\": 1.0, \"frequency_penalty\": 0, \"presence_penalty\": 0},\n",
    "        \"reason\": \"éœ€è¦å‡†ç¡®æ€§å’Œä¸€è‡´æ€§,ä½†å…è®¸ä¸€äº›æ”¹å†™\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"ä¸åŒä»»åŠ¡çš„æ¨èå‚æ•°é…ç½®\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\nğŸ“‹ ä»»åŠ¡ç±»å‹: {scenario['task']}\")\n",
    "    print(f\"ç†ç”±: {scenario['reason']}\")\n",
    "    print(f\"\\næ¨èå‚æ•°:\")\n",
    "    for key, value in scenario['params'].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\næç¤ºè¯: {scenario['prompt']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = call_llm(\n",
    "        scenario['prompt'],\n",
    "        **scenario['params'],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    print(f\"è¾“å‡º:\\n{result['content']}\")\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametersé€ŸæŸ¥è¡¨\n",
    "<!-- å‚æ•°é€ŸæŸ¥è¡¨ -->\n",
    "\n",
    "æ ¹æ®å®éªŒç»“æœ,è¿™é‡Œæ˜¯ä¸€ä¸ªå®ç”¨çš„å‚æ•°é…ç½®æŒ‡å—:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Createå‚æ•°é…ç½®è¡¨\n",
    "config_table = pd.DataFrame([\n",
    "    {\n",
    "        \"ä»»åŠ¡ç±»å‹\": \"æ•°æ®æå–/åˆ†ç±»\",\n",
    "        \"Temperature\": \"0\",\n",
    "        \"Top-p\": \"1.0\",\n",
    "        \"Freq Penalty\": \"0\",\n",
    "        \"Pres Penalty\": \"0\",\n",
    "        \"è¯´æ˜\": \"éœ€è¦ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§\"\n",
    "    },\n",
    "    {\n",
    "        \"ä»»åŠ¡ç±»å‹\": \"ä»£ç ç”Ÿæˆ\",\n",
    "        \"Temperature\": \"0-0.2\",\n",
    "        \"Top-p\": \"1.0\",\n",
    "        \"Freq Penalty\": \"0\",\n",
    "        \"Pres Penalty\": \"0\",\n",
    "        \"è¯´æ˜\": \"ç²¾ç¡®ä½†å…è®¸å‘½åå˜åŒ–\"\n",
    "    },\n",
    "    {\n",
    "        \"ä»»åŠ¡ç±»å‹\": \"æ–‡æœ¬æ‘˜è¦\",\n",
    "        \"Temperature\": \"0.3\",\n",
    "        \"Top-p\": \"1.0\",\n",
    "        \"Freq Penalty\": \"0\",\n",
    "        \"Pres Penalty\": \"0\",\n",
    "        \"è¯´æ˜\": \"å‡†ç¡®ä½†å…è®¸æ”¹å†™\"\n",
    "    },\n",
    "    {\n",
    "        \"ä»»åŠ¡ç±»å‹\": \"é—®ç­”/å¯¹è¯\",\n",
    "        \"Temperature\": \"0.7\",\n",
    "        \"Top-p\": \"0.9\",\n",
    "        \"Freq Penalty\": \"0.3\",\n",
    "        \"Pres Penalty\": \"0.3\",\n",
    "        \"è¯´æ˜\": \"å¹³è¡¡å‡†ç¡®æ€§å’Œè‡ªç„¶åº¦\"\n",
    "    },\n",
    "    {\n",
    "        \"ä»»åŠ¡ç±»å‹\": \"åˆ›æ„å†™ä½œ\",\n",
    "        \"Temperature\": \"0.8-1.0\",\n",
    "        \"Top-p\": \"1.0\",\n",
    "        \"Freq Penalty\": \"0.5-1.0\",\n",
    "        \"Pres Penalty\": \"0.5-1.0\",\n",
    "        \"è¯´æ˜\": \"é¼“åŠ±åˆ›é€ æ€§å’Œå¤šæ ·æ€§\"\n",
    "    },\n",
    "    {\n",
    "        \"ä»»åŠ¡ç±»å‹\": \"å¤´è„‘é£æš´\",\n",
    "        \"Temperature\": \"0.9\",\n",
    "        \"Top-p\": \"0.95\",\n",
    "        \"Freq Penalty\": \"1.0-1.5\",\n",
    "        \"Pres Penalty\": \"0.6-1.0\",\n",
    "        \"è¯´æ˜\": \"æœ€å¤§åŒ–å¤šæ ·æ€§å’Œæ–°é¢–æ€§\"\n",
    "    },\n",
    "    {\n",
    "        \"ä»»åŠ¡ç±»å‹\": \"é•¿æ–‡æœ¬ç”Ÿæˆ\",\n",
    "        \"Temperature\": \"0.7\",\n",
    "        \"Top-p\": \"0.9\",\n",
    "        \"Freq Penalty\": \"0.5\",\n",
    "        \"Pres Penalty\": \"0.3\",\n",
    "        \"è¯´æ˜\": \"é¿å…é‡å¤åŒæ—¶ä¿æŒè¿è´¯\"\n",
    "    },\n",
    "])\n",
    "\n",
    "print(\"\\næ¨ç†å‚æ•°é…ç½®é€ŸæŸ¥è¡¨\\n\")\n",
    "print(config_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nğŸ’¡ é‡è¦æç¤º:\")\n",
    "print(\"1. OpenAI å»ºè®®åªè°ƒæ•´ temperature æˆ– top_p,ä¸è¦åŒæ—¶è°ƒæ•´\")\n",
    "print(\"2. ä»æ¨èå€¼å¼€å§‹,æ ¹æ®å®é™…æ•ˆæœå¾®è°ƒ\")\n",
    "print(\"3. Temperature=0 å¯¹äºéœ€è¦ç¡®å®šæ€§çš„ä»»åŠ¡è‡³å…³é‡è¦\")\n",
    "print(\"4. Penalty å‚æ•°å¯¹é•¿æ–‡æœ¬å’Œå¤šè½®å¯¹è¯æ•ˆæœæ›´æ˜æ˜¾\")\n",
    "print(\"5. åˆ›æ„ä»»åŠ¡å¯ä»¥å¤§èƒ†å°è¯•æ›´é«˜çš„å‚æ•°å€¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹ : ä¸ºæŠ€æœ¯Documentæ‘˜è¦æ‰¾åˆ°æœ€ä½³ Temperature\n",
    "<!-- ç»ƒä¹ : ä¸ºæŠ€æœ¯æ–‡æ¡£æ‘˜è¦æ‰¾åˆ°æœ€ä½³ Temperature -->\n",
    "\n",
    "å°è¯•ä¸åŒçš„ temperature å€¼,æ‰¾å‡ºæœ€é€‚åˆæ‘˜è¦ä»»åŠ¡çš„è®¾ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŠ€æœ¯æ–‡æ¡£ç¤ºä¾‹\n",
    "tech_doc = \"\"\"\n",
    "React 18 å¼•å…¥äº†å¹¶å‘æ¸²æŸ“(Concurrent Rendering)ç‰¹æ€§,è¿™æ˜¯ React æ¶æ„çš„é‡å¤§å‡çº§ã€‚\n",
    "å¹¶å‘æ¸²æŸ“å…è®¸ React åŒæ—¶å‡†å¤‡å¤šä¸ªç‰ˆæœ¬çš„ UI,å¯ä»¥ä¸­æ–­æ¸²æŸ“è¿‡ç¨‹æ¥å¤„ç†æ›´é«˜ä¼˜å…ˆçº§çš„æ›´æ–°ã€‚\n",
    "æ–°å¢çš„ useTransition å’Œ useDeferredValue Hooks è®©å¼€å‘è€…èƒ½å¤Ÿæ ‡è®°éç´§æ€¥æ›´æ–°,\n",
    "ä»è€Œåœ¨ä¸é˜»å¡ç”¨æˆ·äº¤äº’çš„æƒ…å†µä¸‹æ›´æ–° UIã€‚æ­¤å¤–,Suspense ç»„ä»¶ç°åœ¨æ”¯æŒæœåŠ¡ç«¯æ¸²æŸ“,\n",
    "å¯ä»¥åœ¨æ•°æ®åŠ è½½æ—¶æ˜¾ç¤º fallback UI,æ”¹å–„ç”¨æˆ·ä½“éªŒã€‚è‡ªåŠ¨æ‰¹å¤„ç†(Automatic Batching)\n",
    "åŠŸèƒ½æ‰©å±•åˆ°äº† Promiseã€setTimeout å’ŒåŸç”Ÿäº‹ä»¶å¤„ç†å™¨,å‡å°‘äº†ä¸å¿…è¦çš„é‡æ¸²æŸ“ã€‚\n",
    "\"\"\"\n",
    "\n",
    "summarize_prompt = f\"ç”¨ 2-3 å¥è¯æ€»ç»“ä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£çš„æ ¸å¿ƒå†…å®¹:\\n\\n{tech_doc}\"\n",
    "\n",
    "# Testä¸åŒçš„ temperature\n",
    "test_temperatures = [0, 0.3, 0.5, 0.7]\n",
    "\n",
    "print(\"ç»ƒä¹ : ä¸ºæŠ€æœ¯æ–‡æ¡£æ‘˜è¦æ‰¾åˆ°æœ€ä½³ Temperature\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"åŸæ–‡ ({len(tech_doc)} å­—ç¬¦):\\n{tech_doc}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "summaries = {}\n",
    "for temp in test_temperatures:\n",
    "    print(f\"\\nğŸŒ¡ï¸  Temperature = {temp}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = call_llm(summarize_prompt, temperature=temp, max_tokens=200)\n",
    "    summaries[temp] = result['content']\n",
    "    \n",
    "    print(f\"{result['content']}\")\n",
    "    print(f\"\\nğŸ“Š å­—æ•°: {len(result['content'])} | Tokens: {result['usage']['completion_tokens']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nåˆ†æä¸è¯„ä¼°:\")\n",
    "print(\"\\nè¯·å›ç­”ä»¥ä¸‹é—®é¢˜:\")\n",
    "print(\"1. å“ªä¸ª temperature äº§ç”Ÿçš„æ‘˜è¦æœ€å‡†ç¡®?\")\n",
    "print(\"2. å“ªä¸ª temperature äº§ç”Ÿçš„æ‘˜è¦æœ€ç®€æ´?\")\n",
    "print(\"3. ä¸åŒ temperature çš„è¾“å‡ºå·®å¼‚å¤§å—?\")\n",
    "print(\"4. å¯¹äºæŠ€æœ¯æ–‡æ¡£æ‘˜è¦,ä½ ä¼šé€‰æ‹©å“ªä¸ª temperature?ä¸ºä»€ä¹ˆ?\")\n",
    "print(\"\\nğŸ’¡ æç¤º: å¯¹äºäº‹å®æ€§ä»»åŠ¡,è¾ƒä½çš„ temperature (0-0.3) é€šå¸¸æ›´å¥½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "é€šè¿‡æœ¬ç¬”è®°æœ¬çš„å®éªŒ,ä½ åº”è¯¥æŒæ¡äº†:\n",
    "\n",
    "### 1. æ ¸å¿ƒå‚æ•°\n",
    "\n",
    "**Temperature (0-2)**:\n",
    "- æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§å’Œåˆ›é€ æ€§\n",
    "- 0 = ç¡®å®šæ€§,é€‚åˆéœ€è¦ç²¾ç¡®ç­”æ¡ˆçš„ä»»åŠ¡\n",
    "- 0.7-1.0 = å¹³è¡¡,é€‚åˆå¤§å¤šæ•°ä»»åŠ¡\n",
    "- 1.0+ = é«˜åˆ›é€ æ€§,é€‚åˆåˆ›æ„å†™ä½œ\n",
    "\n",
    "**Top-p (0-1)**:\n",
    "- æ›¿ä»£ temperature çš„é‡‡æ ·æ–¹æ³•\n",
    "- å»ºè®®ä¸è¦ä¸ temperature åŒæ—¶è°ƒæ•´\n",
    "- 0.1 = ä¿å®ˆ,0.9 = å¹³è¡¡,1.0 = å…¨éƒ¨è€ƒè™‘\n",
    "\n",
    "**Frequency Penalty (-2 åˆ° 2)**:\n",
    "- æ ¹æ®å‡ºç°æ¬¡æ•°æƒ©ç½š token\n",
    "- æ­£å€¼å‡å°‘é‡å¤,é€‚åˆé•¿æ–‡æœ¬å’Œåˆ—è¡¨\n",
    "\n",
    "**Presence Penalty (-2 åˆ° 2)**:\n",
    "- æ ¹æ®æ˜¯å¦å‡ºç°è¿‡æƒ©ç½š token\n",
    "- æ­£å€¼é¼“åŠ±æ–°è¯é¢˜,é€‚åˆå¤´è„‘é£æš´å’Œå¯¹è¯\n",
    "\n",
    "### 2. ä»»åŠ¡åŒ¹é…\n",
    "\n",
    "- **ç²¾ç¡®ä»»åŠ¡** (æå–ã€åˆ†ç±»ã€ä»£ç ): Temperature = 0-0.2\n",
    "- **å¹³è¡¡ä»»åŠ¡** (æ‘˜è¦ã€é—®ç­”): Temperature = 0.3-0.7\n",
    "- **åˆ›æ„ä»»åŠ¡** (å†™ä½œã€å¤´è„‘é£æš´): Temperature = 0.8-1.0+\n",
    "\n",
    "### 3. æœ€ä½³å®è·µ\n",
    "\n",
    "1. ä»ä¿å®ˆå‚æ•°å¼€å§‹,é€æ­¥å¢åŠ éšæœºæ€§\n",
    "2. å¯¹å…³é”®ä»»åŠ¡ä½¿ç”¨ temperature=0 ç¡®ä¿ä¸€è‡´æ€§\n",
    "3. ä½¿ç”¨ penalty å‚æ•°å‡å°‘é•¿æ–‡æœ¬ä¸­çš„é‡å¤\n",
    "4. å¤šæ¬¡æµ‹è¯•æ‰¾åˆ°æœ€é€‚åˆä½ ä»»åŠ¡çš„å‚æ•°\n",
    "5. è®°å½•æˆåŠŸçš„å‚æ•°é…ç½®ä»¥ä¾¿å¤ç”¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}