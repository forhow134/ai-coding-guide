{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 12.3 高级 RAG 技术\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-org/ai-first-app/blob/main/demos/12-rag-memory/advanced_rag.ipynb)\n\n**预计 API 费用: ~$0.05**\n\n本 Notebook 演示高级 RAG 技术:分块策略、重排序、混合搜索。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -q langchain langchain-openai chromadb cohere rank-bm25"]},{"cell_type":"markdown","metadata":{},"source":["## 实验 1: 分块策略对比"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from langchain.text_splitter import (\n    CharacterTextSplitter,\n    RecursiveCharacterTextSplitter,\n    TokenTextSplitter\n)\n\ntext = \"\"\"\nPython 异常处理最佳实践\n\n1. 使用具体的异常类型\n不要捕获所有异常,应该捕获具体的异常类型。\n\n2. 避免空的 except 块\n空的 except 块会隐藏错误。\n\n3. 使用 finally 清理资源\nfinally 块总是会执行,适合清理资源。\n\"\"\"\n\n# 策略 1: 固定字符数\nprint(\"=== 策略 1: 固定字符数 ===\")\nsplitter1 = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\nchunks1 = splitter1.split_text(text)\nfor i, chunk in enumerate(chunks1):\n    print(f\"Chunk {i+1}: {chunk[:50]}...\\n\")\n\n# 策略 2: 递归分块(推荐)\nprint(\"\\n=== 策略 2: 递归分块 ===\")\nsplitter2 = RecursiveCharacterTextSplitter(\n    chunk_size=100,\n    chunk_overlap=20,\n    separators=[\"\\n\\n\", \"\\n\", \"。\", \" \", \"\"]\n)\nchunks2 = splitter2.split_text(text)\nfor i, chunk in enumerate(chunks2):\n    print(f\"Chunk {i+1}: {chunk[:50]}...\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## 实验 2: 重排序 (Reranking)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cohere\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\n# 准备文档\ndocs_content = [\n    \"Python 使用 try-except 处理异常\",\n    \"Java 使用 try-catch 处理异常\",\n    \"异常情况下系统会重启\",\n    \"Python 的异常处理非常灵活\",\n    \"异常天气可能导致航班延误\"\n]\n\nfrom langchain.schema import Document\ndocs = [Document(page_content=content) for content in docs_content]\n\n# 创建向量库\nvectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())\n\n# 查询\nquery = \"Python 如何处理异常?\"\n\n# 向量搜索 Top-5\nprint(\"=== 向量搜索结果 ===\")\nresults_vector = vectorstore.similarity_search_with_score(query, k=5)\nfor i, (doc, score) in enumerate(results_vector):\n    print(f\"{i+1}. [{score:.4f}] {doc.page_content}\")\n\n# Rerank\nprint(\"\\n=== Rerank 后结果 ===\")\nco = cohere.Client(\"your-cohere-api-key\")  # 需要 Cohere API key\nrerank_results = co.rerank(\n    query=query,\n    documents=docs_content,\n    top_n=3,\n    model=\"rerank-multilingual-v3.0\"\n)\n\nfor i, result in enumerate(rerank_results.results):\n    print(f\"{i+1}. [{result.relevance_score:.4f}] {docs_content[result.index]}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 实验 3: 混合搜索 (Hybrid Search)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from rank_bm25 import BM25Okapi\nimport jieba\n\n# BM25 关键词搜索\nclass BM25Searcher:\n    def __init__(self, documents):\n        self.documents = documents\n        # 分词\n        tokenized_docs = [list(jieba.cut(doc)) for doc in documents]\n        self.bm25 = BM25Okapi(tokenized_docs)\n    \n    def search(self, query, k=3):\n        tokenized_query = list(jieba.cut(query))\n        scores = self.bm25.get_scores(tokenized_query)\n        top_k_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n        return [(self.documents[i], scores[i]) for i in top_k_idx]\n\n# 混合搜索\ndef hybrid_search(query, vectorstore, bm25_searcher, k=3, alpha=0.5):\n    # 向量搜索\n    vector_results = vectorstore.similarity_search_with_score(query, k=k*2)\n    \n    # BM25 搜索\n    bm25_results = bm25_searcher.search(query, k=k*2)\n    \n    # 归一化并融合分数\n    combined = {}\n    \n    for doc, score in vector_results:\n        text = doc.page_content\n        combined[text] = combined.get(text, 0) + (1 - score) * alpha\n    \n    for text, score in bm25_results:\n        combined[text] = combined.get(text, 0) + score * (1 - alpha)\n    \n    # 排序\n    sorted_results = sorted(combined.items(), key=lambda x: x[1], reverse=True)[:k]\n    return sorted_results\n\n# 测试\nbm25_searcher = BM25Searcher(docs_content)\n\nprint(\"=== 混合搜索结果 ===\")\nhybrid_results = hybrid_search(query, vectorstore, bm25_searcher, k=3)\nfor i, (doc, score) in enumerate(hybrid_results):\n    print(f\"{i+1}. [{score:.4f}] {doc}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 动手练习\n\n1. **对比不同分块大小**: chunk_size 200 vs 500 vs 1000\n2. **调整混合搜索权重**: alpha 0.3 vs 0.5 vs 0.7\n3. **实现查询改写**: 用 LLM 改写模糊查询\n4. **添加元数据过滤**: 只搜索特定类别的文档\n\n---\n\n## 关键要点\n\n1. **递归分块最优**: 保持语义完整性\n2. **Reranking 提升精度**: 二次排序提升 Top-K 质量\n3. **混合搜索**: 向量 + BM25 兼顾语义和精确\n4. **权重调优**: alpha 参数影响搜索结果\n\n---\n\n**下一步**: [12.4 记忆管理](./memory_chatbot.ipynb)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":2}
