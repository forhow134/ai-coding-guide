---
prev:
  text: '12.5 GraphRAG'
  link: '/zh/12-rag-memory/graph-rag'
next:
  text: '12.7 AI Memory äº§å“å…¨æ™¯'
  link: '/zh/12-rag-memory/memory-products'
---

# 12.6 Agentic RAG

## å¼€ç¯‡:å¦‚æœ RAG ä¼šè‡ªå·±æ€è€ƒå‘¢?

æƒ³è±¡ä¸€ä¸‹,ä½ é—®åŠ©æ‰‹:"ä¸ºä»€ä¹ˆæˆ‘å®¶çŒ«æ€»æ˜¯åœ¨å‡Œæ™¨3ç‚¹å«?"

**ä¼ ç»Ÿ RAG**:  
â†’ æ£€ç´¢"çŒ« å‡Œæ™¨ å«"  
â†’ è¿”å›å‰5æ¡ç»“æœ  
â†’ ç”Ÿæˆç­”æ¡ˆ:"å¯èƒ½æ˜¯é¥¿äº†æˆ–è€…æ— èŠ"  
â†’ ç»“æŸ

**Agentic RAG**:  
â†’ "å—¯,è®©æˆ‘å…ˆæŸ¥ä¸€ä¸‹çŒ«çš„ç”Ÿç‰©é’Ÿ..."(ç¬¬1æ¬¡æ£€ç´¢)  
â†’ "ç­‰ç­‰,ç”¨æˆ·è¯´çš„æ˜¯å‡Œæ™¨3ç‚¹,è¿™æ˜¯å¦ä¸ç‹©çŒæœ¬èƒ½æœ‰å…³?"(ç¬¬2æ¬¡æ£€ç´¢)  
â†’ "æ£€ç´¢ç»“æœæåˆ°'é»„æ˜/é»æ˜æ´»è·ƒæœŸ',ä½†å‡Œæ™¨3ç‚¹ä¸åœ¨è¿™ä¸ªèŒƒå›´..."(è‡ªæˆ‘åæ€)  
â†’ "è®©æˆ‘é‡æ–°æ£€ç´¢'çŒ« å¤œé—´æ´»åŠ¨å¼‚å¸¸'..."(çº æ­£æ€§æ£€ç´¢)  
â†’ "æ‰¾åˆ°äº†!å¯èƒ½æ˜¯ç”²çŠ¶è…ºé—®é¢˜æˆ–ç„¦è™‘,å»ºè®®çœ‹å…½åŒ»"(æœ€ç»ˆç­”æ¡ˆ)

è¿™å°±æ˜¯ Agentic RAG â€”â€” **ä¼šæ€è€ƒã€ä¼šè´¨ç–‘ã€ä¼šè¿­ä»£çš„æ£€ç´¢ç³»ç»Ÿ**ã€‚

::: tip æ ¸å¿ƒåŒºåˆ«
- **Traditional RAG**: ğŸ¤– "æˆ‘åªæ˜¯ä¸ªæ¬è¿å·¥,ä½ è¦å•¥æˆ‘æ¬å•¥"
- **Agentic RAG**: ğŸ§  "æˆ‘æ˜¯ä¾¦æ¢,è®©æˆ‘è°ƒæŸ¥ä¸€ä¸‹çº¿ç´¢æ˜¯å¦é è°±"
:::

---

## ä¼ ç»Ÿ RAG çš„ä¸‰å¤§ç¡¬ä¼¤

### 1. å•æ¬¡æ£€ç´¢ç›²åŒº(One-Shot Blindness)

**åœºæ™¯**: "SpaceX çš„ Starship ç¬¬ä¸‰æ¬¡è¯•é£æˆåŠŸäº†å—?"

```python
# ä¼ ç»Ÿ RAG çš„æ£€ç´¢é€»è¾‘
query = "Starship third flight success"
docs = vector_db.search(query, top_k=5)  # ä¸€æ¬¡æ£€ç´¢
answer = llm.generate(docs)              # ç›´æ¥ç”Ÿæˆ

# é—®é¢˜: å¦‚æœæ£€ç´¢åˆ°çš„æ˜¯ç¬¬äºŒæ¬¡è¯•é£çš„æ—§æ–°é—»æ€ä¹ˆåŠ?
# ç­”æ¡ˆ: ä¸ç®¡,ç¡¬ç€å¤´çš®ç”Ÿæˆ
```

**ç»“æœ**: å¯èƒ½è¿”å›è¿‡æœŸä¿¡æ¯,å› ä¸ºå®ƒä¸ä¼šéªŒè¯æ—¶é—´çº¿ã€‚

### 2. æ— éªŒè¯æœºåˆ¶(No Fact-Checking)

**åœºæ™¯**: "Python 3.12 æœ‰å“ªäº›æ–°ç‰¹æ€§?"

```python
# ä¼ ç»Ÿ RAG
docs = retrieve("Python 3.12 features")  # å¯èƒ½æ£€ç´¢åˆ° 3.11 çš„æ–‡æ¡£
answer = generate(docs)                  # ç”Ÿæˆç­”æ¡ˆ,ä¸ç®¡å¯¹ä¸å¯¹

# Agentic RAG
docs = retrieve("Python 3.12 features")
if not verify_relevance(docs, "3.12"):   # éªŒè¯ç›¸å…³æ€§
    docs = retrieve("Python 3.12 release notes official")  # é‡æ–°æ£€ç´¢
answer = generate(docs)
```

### 3. æ— è·¯ç”±èƒ½åŠ›(No Routing)

**é—®é¢˜ç±»å‹**:
- äº‹å®æŸ¥è¯¢ â†’ åº”è¯¥ç”¨å‘é‡æ£€ç´¢
- ä»£ç é—®é¢˜ â†’ åº”è¯¥ç”¨ä»£ç æœç´¢å¼•æ“
- å®æ—¶ä¿¡æ¯ â†’ åº”è¯¥è°ƒç”¨æœç´¢ API

**ä¼ ç»Ÿ RAG**: å…¨éƒ¨ç”¨å‘é‡æ£€ç´¢,ä¸€æ‹›é²œåƒéå¤©(ç„¶åè¢«åƒäº†)

**Agentic RAG**: "è®©æˆ‘æƒ³æƒ³è¯¥ç”¨å“ªä¸ªå·¥å…·..."

---

## ä»€ä¹ˆæ˜¯ Agentic RAG?

### å®šä¹‰

> **Agentic RAG** = Agent(æ™ºèƒ½ä½“)+ RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)

æŠŠ RAG çš„æ£€ç´¢è¿‡ç¨‹äº¤ç»™ä¸€ä¸ª**å…·æœ‰å†³ç­–èƒ½åŠ›çš„ Agent**,è®©å®ƒå†³å®š:
- **WHEN**(ä½•æ—¶æ£€ç´¢): æ˜¯å¦éœ€è¦æ£€ç´¢?è¿˜æ˜¯ç›´æ¥ç”¨å·²çŸ¥çŸ¥è¯†?
- **WHERE**(ä»å“ªæ£€ç´¢): å‘é‡åº“?æœç´¢å¼•æ“?SQLæ•°æ®åº“?
- **HOW**(å¦‚ä½•æ£€ç´¢): å•æ¬¡æŸ¥è¯¢?è¿˜æ˜¯å¤šæ­¥æ¨ç†?

### æ ¸å¿ƒæ¶æ„

```
ç”¨æˆ·é—®é¢˜
   â†“
[Agent è§„åˆ’å™¨]
   â†“
   â”œâ”€â†’ éœ€è¦æ£€ç´¢? â†’ [æ£€ç´¢å·¥å…·é€‰æ‹©]
   â”‚                   â†“
   â”‚              [æ‰§è¡Œæ£€ç´¢]
   â”‚                   â†“
   â”‚              [éªŒè¯ç»“æœ]
   â”‚                   â†“
   â”‚              ç»“æœå¯é ? â”€â”€Noâ”€â”€â†’ [é‡æ–°æ£€ç´¢/è°ƒæ•´ç­–ç•¥]
   â”‚                   â†“ Yes
   â””â”€â†’ ä¸éœ€è¦æ£€ç´¢ â†’ [ç›´æ¥ç”Ÿæˆç­”æ¡ˆ]
```

---

## å››å¤§ Agentic RAG æ¨¡å¼

### 1. Self-RAG(è‡ªæˆ‘åæ€ RAG)

**æ ¸å¿ƒæ€æƒ³**: ç”Ÿæˆç­”æ¡ˆå,è‡ªå·±è¯„ä¼°ç­”æ¡ˆè´¨é‡,å¿…è¦æ—¶é‡æ–°æ£€ç´¢ã€‚

```mermaid
graph LR
    A[ç”¨æˆ·é—®é¢˜] --> B[æ£€ç´¢æ–‡æ¡£]
    B --> C[ç”Ÿæˆç­”æ¡ˆ]
    C --> D{è‡ªæˆ‘è¯„ä¼°}
    D -->|è´¨é‡å·®| B
    D -->|è´¨é‡å¥½| E[è¿”å›ç­”æ¡ˆ]
```

**ä»£ç ç¤ºä¾‹**(ä¼ªä»£ç ):

```python
def self_rag(query):
    max_iterations = 3
    
    for i in range(max_iterations):
        # æ£€ç´¢
        docs = retrieve(query)
        
        # ç”Ÿæˆç­”æ¡ˆ
        answer = llm.generate(f"åŸºäºæ–‡æ¡£: {docs}\nå›ç­”: {query}")
        
        # è‡ªæˆ‘è¯„ä¼°
        score = llm.evaluate(f"ç­”æ¡ˆ: {answer}\nè¯„åˆ†(1-10): ")
        
        if score >= 8:
            return answer  # æ»¡æ„,è¿”å›
        
        # ä¸æ»¡æ„,æ”¹è¿›æŸ¥è¯¢
        query = llm.refine_query(query, answer, docs)
    
    return answer  # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°,è¿”å›æœ€åç»“æœ
```

**é€‚ç”¨åœºæ™¯**:
- éœ€è¦é«˜è´¨é‡ç­”æ¡ˆçš„åœºæ™¯(åŒ»ç–—ã€æ³•å¾‹å’¨è¯¢)
- å…è®¸è¾ƒé•¿å“åº”æ—¶é—´

::: warning æ€§èƒ½è­¦å‘Š
Self-RAG ä¼šå¤šæ¬¡è°ƒç”¨ LLM,æˆæœ¬å’Œå»¶è¿Ÿéƒ½ä¼šå¢åŠ ã€‚ç”Ÿäº§ç¯å¢ƒå»ºè®®è®¾ç½® `max_iterations=2`ã€‚
:::

---

### 2. Corrective RAG(çº æ­£æ€§ RAG)

**æ ¸å¿ƒæ€æƒ³**: å…ˆåˆ¤æ–­æ£€ç´¢ç»“æœæ˜¯å¦ç›¸å…³,ä¸ç›¸å…³å°±æ¢ä¸ªç­–ç•¥é‡æ–°æ£€ç´¢ã€‚

```mermaid
graph TD
    A[æ£€ç´¢] --> B{æ–‡æ¡£ç›¸å…³?}
    B -->|ç›¸å…³| C[ç”Ÿæˆç­”æ¡ˆ]
    B -->|ä¸ç›¸å…³| D[Web æœç´¢]
    B -->|éƒ¨åˆ†ç›¸å…³| E[è¿‡æ»¤+è¡¥å……æ£€ç´¢]
    D --> C
    E --> C
```

**ä»£ç ç¤ºä¾‹**:

```python
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI

def corrective_rag(query):
    # ç¬¬ä¸€æ­¥: å‘é‡æ£€ç´¢
    docs = vector_store.search(query, top_k=5)
    
    # ç¬¬äºŒæ­¥: ç›¸å…³æ€§åˆ¤æ–­
    relevance_prompt = PromptTemplate.from_template(
        "æ–‡æ¡£: {docs}\né—®é¢˜: {query}\n"
        "è¿™äº›æ–‡æ¡£æ˜¯å¦ç›¸å…³? å›ç­” RELEVANT/IRRELEVANT/PARTIAL"
    )
    
    llm = ChatOpenAI(model="gpt-4")
    relevance = llm.predict(relevance_prompt.format(docs=docs, query=query))
    
    # ç¬¬ä¸‰æ­¥: æ ¹æ®ç›¸å…³æ€§å†³ç­–
    if relevance == "RELEVANT":
        return llm.generate(f"åŸºäºæ–‡æ¡£: {docs}\nå›ç­”: {query}")
    
    elif relevance == "IRRELEVANT":
        # å®Œå…¨ä¸ç›¸å…³,ä½¿ç”¨ Web æœç´¢
        web_results = tavily_search(query)  # Tavily API
        return llm.generate(f"åŸºäºæœç´¢: {web_results}\nå›ç­”: {query}")
    
    else:  # PARTIAL
        # éƒ¨åˆ†ç›¸å…³,è¿‡æ»¤åè¡¥å……æ£€ç´¢
        filtered_docs = [d for d in docs if is_relevant(d, query)]
        additional_docs = web_search(query, top_k=2)
        all_docs = filtered_docs + additional_docs
        return llm.generate(f"åŸºäºæ–‡æ¡£: {all_docs}\nå›ç­”: {query}")

def is_relevant(doc, query):
    # ç®€å•çš„å…³é”®è¯åŒ¹é…æˆ–è¯­ä¹‰ç›¸ä¼¼åº¦
    return semantic_similarity(doc, query) > 0.7
```

**å…³é”®ç»„ä»¶**:
- **ç›¸å…³æ€§åˆ†ç±»å™¨**: å¯ä»¥ç”¨å°æ¨¡å‹(å¦‚ BERT)æˆ– LLM åˆ¤æ–­
- **å¤‡ç”¨æ£€ç´¢ç­–ç•¥**: Web æœç´¢ã€SQL æŸ¥è¯¢ã€API è°ƒç”¨ç­‰

---

### 3. Adaptive RAG(è‡ªé€‚åº” RAG)

**æ ¸å¿ƒæ€æƒ³**: æ ¹æ®é—®é¢˜ç±»å‹,è‡ªåŠ¨é€‰æ‹©æ£€ç´¢ç­–ç•¥ã€‚

```python
from enum import Enum

class QueryType(Enum):
    SIMPLE = "simple"          # ç®€å•äº‹å®æŸ¥è¯¢
    COMPLEX = "complex"        # éœ€è¦æ¨ç†
    RECENT = "recent"          # å®æ—¶ä¿¡æ¯
    CODE = "code"              # ä»£ç é—®é¢˜

def adaptive_rag(query):
    # ç¬¬ä¸€æ­¥: åˆ†ç±»é—®é¢˜ç±»å‹
    query_type = classify_query(query)
    
    # ç¬¬äºŒæ­¥: è·¯ç”±åˆ°ä¸åŒç­–ç•¥
    if query_type == QueryType.SIMPLE:
        # ç®€å•æŸ¥è¯¢ â†’ å•æ¬¡å‘é‡æ£€ç´¢
        docs = vector_store.search(query, top_k=3)
        return llm.generate(f"åŸºäº: {docs}\nå›ç­”: {query}")
    
    elif query_type == QueryType.COMPLEX:
        # å¤æ‚æŸ¥è¯¢ â†’ å¤šæ­¥æ¨ç† RAG
        return multi_hop_rag(query)
    
    elif query_type == QueryType.RECENT:
        # å®æ—¶ä¿¡æ¯ â†’ ç›´æ¥ Web æœç´¢
        web_results = tavily_search(query)
        return llm.generate(f"åŸºäº: {web_results}\nå›ç­”: {query}")
    
    elif query_type == QueryType.CODE:
        # ä»£ç é—®é¢˜ â†’ ä»£ç æœç´¢å¼•æ“
        code_results = github_code_search(query)
        return llm.generate(f"åŸºäºä»£ç : {code_results}\nå›ç­”: {query}")

def classify_query(query):
    """ä½¿ç”¨ LLM åˆ†ç±»é—®é¢˜ç±»å‹"""
    prompt = f"""
    é—®é¢˜: {query}
    
    è¿™æ˜¯ä»€ä¹ˆç±»å‹çš„é—®é¢˜?
    - SIMPLE: ç®€å•äº‹å®æŸ¥è¯¢(å¦‚"ä»€ä¹ˆæ˜¯ RAG?")
    - COMPLEX: éœ€è¦å¤šæ­¥æ¨ç†(å¦‚"æ¯”è¾ƒ RAG å’Œå¾®è°ƒçš„ä¼˜ç¼ºç‚¹")
    - RECENT: å®æ—¶ä¿¡æ¯(å¦‚"ä»Šå¤©å¤©æ°”")
    - CODE: ä»£ç é—®é¢˜(å¦‚"å¦‚ä½•ç”¨ Python å®ç°å¿«æ’?")
    
    åªè¿”å›ç±»å‹åç§°:
    """
    
    response = llm.predict(prompt).strip()
    return QueryType(response.lower())
```

**è·¯ç”±å†³ç­–æ ‘**:

```
é—®é¢˜
 â”œâ”€ åŒ…å«æ—¶é—´è¯(ä»Šå¤©/æœ€æ–°/ç°åœ¨) â†’ Web æœç´¢
 â”œâ”€ åŒ…å«ä»£ç å…³é”®è¯(å®ç°/ä»£ç /å‡½æ•°) â†’ ä»£ç æœç´¢
 â”œâ”€ åŒ…å«æ¯”è¾ƒ/åˆ†æè¯ â†’ å¤šæ­¥ RAG
 â””â”€ å…¶ä»– â†’ å‘é‡æ£€ç´¢
```

---

### 4. Multi-hop RAG(å¤šè·³ RAG)

**æ ¸å¿ƒæ€æƒ³**: ä¸€ä¸ªé—®é¢˜éœ€è¦å¤šæ¬¡æ£€ç´¢,æ¯æ¬¡æ£€ç´¢çš„ç»“æœæ˜¯ä¸‹æ¬¡æ£€ç´¢çš„è¾“å…¥ã€‚

**åœºæ™¯**: "LangChain å’Œ LlamaIndex å“ªä¸ªæ›´é€‚åˆæ„å»º Agentic RAG?"

```python
def multi_hop_rag(query):
    # ç¬¬ä¸€æ­¥: åˆ†è§£é—®é¢˜
    sub_queries = decompose_query(query)
    # è¾“å‡º: ["LangChain çš„ä¼˜åŠ¿", "LlamaIndex çš„ä¼˜åŠ¿", "Agentic RAG çš„éœ€æ±‚"]
    
    # ç¬¬äºŒæ­¥: é€ä¸ªæ£€ç´¢
    all_contexts = []
    for sub_q in sub_queries:
        docs = vector_store.search(sub_q, top_k=3)
        summary = llm.summarize(docs)  # æ€»ç»“å­ç­”æ¡ˆ
        all_contexts.append(summary)
    
    # ç¬¬ä¸‰æ­¥: ç»¼åˆç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    final_context = "\n".join(all_contexts)
    return llm.generate(f"åŸºäºä»¥ä¸‹ä¿¡æ¯:\n{final_context}\n\nå›ç­”: {query}")

def decompose_query(query):
    """LLM åˆ†è§£é—®é¢˜"""
    prompt = f"""
    å°†ä»¥ä¸‹é—®é¢˜åˆ†è§£ä¸º3-5ä¸ªå­é—®é¢˜:
    {query}
    
    è¿”å›åˆ—è¡¨æ ¼å¼:
    1. ...
    2. ...
    """
    response = llm.predict(prompt)
    return parse_list(response)
```

**æµç¨‹å›¾**:

```mermaid
graph TD
    A[å¤æ‚é—®é¢˜] --> B[åˆ†è§£å­é—®é¢˜]
    B --> C1[æ£€ç´¢å­é—®é¢˜1]
    B --> C2[æ£€ç´¢å­é—®é¢˜2]
    B --> C3[æ£€ç´¢å­é—®é¢˜3]
    C1 --> D1[æ€»ç»“ç­”æ¡ˆ1]
    C2 --> D2[æ€»ç»“ç­”æ¡ˆ2]
    C3 --> D3[æ€»ç»“ç­”æ¡ˆ3]
    D1 --> E[ç»¼åˆç­”æ¡ˆ]
    D2 --> E
    D3 --> E
```

**é€‚ç”¨åœºæ™¯**:
- æ¯”è¾ƒç±»é—®é¢˜("A vs B")
- å› æœæ¨ç†("ä¸ºä»€ä¹ˆ X å¯¼è‡´ Y?")
- ç»¼åˆåˆ†æ("æ€»ç»“ X çš„ä¼˜ç¼ºç‚¹")

---

## å®Œæ•´ä»£ç ç¤ºä¾‹:LangGraph æ„å»º Agentic RAG

LangGraph æ˜¯ LangChain çš„çŠ¶æ€æœºæ¡†æ¶,æœ€é€‚åˆæ„å»º Agentic RAGã€‚

### å®‰è£…ä¾èµ–

```bash
pip install langgraph langchain langchain-openai faiss-cpu
```

### å®Œæ•´ä»£ç 

```python
from typing import TypedDict, Annotated, List
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document

# ============ 1. å®šä¹‰çŠ¶æ€ ============
class AgentState(TypedDict):
    query: str                    # ç”¨æˆ·é—®é¢˜
    documents: List[Document]     # æ£€ç´¢åˆ°çš„æ–‡æ¡£
    answer: str                   # ç”Ÿæˆçš„ç­”æ¡ˆ
    needs_web_search: bool        # æ˜¯å¦éœ€è¦ Web æœç´¢
    iteration: int                # è¿­ä»£æ¬¡æ•°

# ============ 2. åˆå§‹åŒ–å·¥å…· ============
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
embeddings = OpenAIEmbeddings()

# åˆ›å»ºç¤ºä¾‹å‘é‡åº“
docs = [
    Document(page_content="RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ,ç»“åˆæ£€ç´¢å’Œç”Ÿæˆä¸¤ä¸ªæ­¥éª¤"),
    Document(page_content="Agentic RAG è®© Agent æ§åˆ¶æ£€ç´¢æµç¨‹,å¯ä»¥å¤šæ¬¡æ£€ç´¢"),
    Document(page_content="LangGraph æ˜¯æ„å»º Agent å·¥ä½œæµçš„çŠ¶æ€æœºæ¡†æ¶"),
]
vector_store = FAISS.from_documents(docs, embeddings)

# ============ 3. å®šä¹‰èŠ‚ç‚¹å‡½æ•° ============
def retrieve(state: AgentState) -> AgentState:
    """æ£€ç´¢èŠ‚ç‚¹"""
    query = state["query"]
    docs = vector_store.similarity_search(query, k=3)
    state["documents"] = docs
    print(f"ğŸ“š æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
    return state

def evaluate_relevance(state: AgentState) -> AgentState:
    """è¯„ä¼°ç›¸å…³æ€§"""
    docs = state["documents"]
    query = state["query"]
    
    prompt = f"""
    é—®é¢˜: {query}
    æ–‡æ¡£: {[d.page_content for d in docs]}
    
    è¿™äº›æ–‡æ¡£æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜? åªå›ç­” YES æˆ– NO:
    """
    
    response = llm.predict(prompt).strip()
    state["needs_web_search"] = (response == "NO")
    
    if state["needs_web_search"]:
        print("âŒ æ–‡æ¡£ä¸ç›¸å…³,éœ€è¦ Web æœç´¢")
    else:
        print("âœ… æ–‡æ¡£ç›¸å…³")
    
    return state

def web_search(state: AgentState) -> AgentState:
    """Web æœç´¢èŠ‚ç‚¹(æ¨¡æ‹Ÿ)"""
    query = state["query"]
    # å®é™…åº”è¯¥è°ƒç”¨ Tavily API æˆ–å…¶ä»–æœç´¢å¼•æ“
    fake_results = [
        Document(page_content=f"æ¥è‡ª Web çš„ç»“æœ: {query} çš„æœ€æ–°ä¿¡æ¯...")
    ]
    state["documents"] = fake_results
    print("ğŸŒ æ‰§è¡Œ Web æœç´¢")
    return state

def generate_answer(state: AgentState) -> AgentState:
    """ç”Ÿæˆç­”æ¡ˆèŠ‚ç‚¹"""
    docs = state["documents"]
    query = state["query"]
    
    context = "\n".join([d.page_content for d in docs])
    prompt = f"åŸºäºä»¥ä¸‹æ–‡æ¡£:\n{context}\n\nå›ç­”é—®é¢˜: {query}"
    
    answer = llm.predict(prompt)
    state["answer"] = answer
    print(f"ğŸ’¡ ç”Ÿæˆç­”æ¡ˆ: {answer[:50]}...")
    return state

# ============ 4. æ„å»ºå›¾ ============
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("retrieve", retrieve)
workflow.add_node("evaluate", evaluate_relevance)
workflow.add_node("web_search", web_search)
workflow.add_node("generate", generate_answer)

# å®šä¹‰è¾¹
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "evaluate")

# æ¡ä»¶è¾¹: æ ¹æ®ç›¸å…³æ€§å†³å®šä¸‹ä¸€æ­¥
workflow.add_conditional_edges(
    "evaluate",
    lambda state: "web_search" if state["needs_web_search"] else "generate",
    {
        "web_search": "web_search",
        "generate": "generate"
    }
)

workflow.add_edge("web_search", "generate")
workflow.add_edge("generate", END)

# ç¼–è¯‘å›¾
app = workflow.compile()

# ============ 5. è¿è¡Œ ============
if __name__ == "__main__":
    initial_state = {
        "query": "ä»€ä¹ˆæ˜¯ Agentic RAG?",
        "documents": [],
        "answer": "",
        "needs_web_search": False,
        "iteration": 0
    }
    
    result = app.invoke(initial_state)
    print("\n" + "="*50)
    print("æœ€ç»ˆç­”æ¡ˆ:", result["answer"])
```

### è¿è¡Œç»“æœ

```bash
ğŸ“š æ£€ç´¢åˆ° 3 ä¸ªæ–‡æ¡£
âœ… æ–‡æ¡£ç›¸å…³
ğŸ’¡ ç”Ÿæˆç­”æ¡ˆ: Agentic RAG æ˜¯ä¸€ç§è®© Agent æ§åˆ¶æ£€ç´¢æµç¨‹çš„å¢å¼ºç‰ˆ RAG...

==================================================
æœ€ç»ˆç­”æ¡ˆ: Agentic RAG æ˜¯ä¸€ç§è®© Agent æ§åˆ¶æ£€ç´¢æµç¨‹çš„å¢å¼ºç‰ˆ RAG,
å¯ä»¥æ ¹æ®éœ€è¦å¤šæ¬¡æ£€ç´¢ã€éªŒè¯ç»“æœ,å¹¶è‡ªé€‚åº”é€‰æ‹©æ£€ç´¢ç­–ç•¥ã€‚
```

### æµç¨‹å¯è§†åŒ–

```mermaid
graph TD
    A[å¼€å§‹] --> B[æ£€ç´¢æ–‡æ¡£]
    B --> C{è¯„ä¼°ç›¸å…³æ€§}
    C -->|ç›¸å…³| D[ç”Ÿæˆç­”æ¡ˆ]
    C -->|ä¸ç›¸å…³| E[Web æœç´¢]
    E --> D
    D --> F[ç»“æŸ]
```

---

## Agentic RAG å·¥å…·ç”Ÿæ€

### 1. LangGraph(æ¨è)

**ä¼˜åŠ¿**:
- ğŸ¯ ä¸º Agent å·¥ä½œæµè®¾è®¡,çŠ¶æ€ç®¡ç†æ¸…æ™°
- ğŸ”„ æ”¯æŒå¾ªç¯å’Œæ¡ä»¶åˆ†æ”¯
- ğŸ› ï¸ ä¸ LangChain å·¥å…·ç”Ÿæ€æ— ç¼é›†æˆ

**é€‚ç”¨åœºæ™¯**: éœ€è¦å¤æ‚å†³ç­–é€»è¾‘çš„ Agentic RAG

**ä»£ç ç»“æ„**:
```python
StateGraph â†’ å®šä¹‰èŠ‚ç‚¹ â†’ æ·»åŠ è¾¹ â†’ ç¼–è¯‘ â†’ è¿è¡Œ
```

---

### 2. LlamaIndex Agents

**ä¼˜åŠ¿**:
- ğŸ“¦ å¼€ç®±å³ç”¨çš„ Agent æ¨¡æ¿
- ğŸ”— åŸç”Ÿæ”¯æŒå¤šç§å‘é‡åº“å’Œ LLM
- ğŸ“Š å†…ç½® Observability(å¯è§‚æµ‹æ€§)

**ä»£ç ç¤ºä¾‹**:

```python
from llama_index.core.agent import ReActAgent
from llama_index.tools import QueryEngineTool

# åˆ›å»ºæ£€ç´¢å·¥å…·
query_engine = index.as_query_engine()
query_tool = QueryEngineTool.from_defaults(
    query_engine=query_engine,
    name="vector_search",
    description="æœç´¢æ–‡æ¡£åº“ä¸­çš„ä¿¡æ¯"
)

# åˆ›å»º Agent
agent = ReActAgent.from_tools([query_tool], llm=llm, verbose=True)

# è¿è¡Œ
response = agent.chat("ä»€ä¹ˆæ˜¯ Agentic RAG?")
```

**ReAct æ¨¡å¼**:
```
Thought: æˆ‘éœ€è¦æœç´¢ Agentic RAG çš„å®šä¹‰
Action: vector_search("Agentic RAG")
Observation: [æ£€ç´¢ç»“æœ]
Thought: ç»“æœçœ‹èµ·æ¥ä¸å®Œæ•´,æˆ‘éœ€è¦æ›´å¤šä¿¡æ¯
Action: vector_search("Agentic RAG examples")
Observation: [æ£€ç´¢ç»“æœ]
Thought: ç°åœ¨æˆ‘æœ‰è¶³å¤Ÿä¿¡æ¯äº†
Answer: Agentic RAG æ˜¯...
```

---

### 3. DSPy(Stanford)

**ç‰¹ç‚¹**: ç”¨**ç¼–ç¨‹**çš„æ–¹å¼å®šä¹‰ Agent é€»è¾‘,è€Œä¸æ˜¯ Promptã€‚

```python
import dspy

class AgenticRAG(dspy.Module):
    def __init__(self):
        self.retrieve = dspy.Retrieve(k=3)
        self.generate = dspy.ChainOfThought("context, question -> answer")
    
    def forward(self, question):
        context = self.retrieve(question).passages
        return self.generate(context=context, question=question)

# ç¼–è¯‘(è‡ªåŠ¨ä¼˜åŒ– Prompt)
compiled_rag = dspy.teleprompt.BootstrapFewShot().compile(
    AgenticRAG(),
    trainset=my_trainset
)
```

**ä¼˜åŠ¿**: è‡ªåŠ¨ä¼˜åŒ– Prompt,é€‚åˆç ”ç©¶å’Œå®éªŒã€‚

---

## ä½•æ—¶ä½¿ç”¨ Agentic RAG?

### ä½¿ç”¨å†³ç­–æ ‘

```
é—®é¢˜ç‰¹ç‚¹
 â”œâ”€ éœ€è¦å¤šæ­¥æ¨ç†? â”€â”€Yesâ”€â”€â†’ Multi-hop RAG
 â”œâ”€ éœ€è¦éªŒè¯ç­”æ¡ˆ? â”€â”€Yesâ”€â”€â†’ Self-RAG
 â”œâ”€ éœ€è¦å®æ—¶ä¿¡æ¯? â”€â”€Yesâ”€â”€â†’ Adaptive RAG(è·¯ç”±åˆ° Web)
 â”œâ”€ æ£€ç´¢è´¨é‡ä¸ç¨³å®š? â”€â”€Yesâ”€â”€â†’ Corrective RAG
 â””â”€ ä»¥ä¸Šéƒ½ä¸éœ€è¦ â”€â”€â†’ ä¼ ç»Ÿ RAG(æ›´å¿«æ›´ä¾¿å®œ)
```

### æˆæœ¬å¯¹æ¯”

| æ–¹æ¡ˆ | LLM è°ƒç”¨æ¬¡æ•° | å»¶è¿Ÿ | æˆæœ¬ | å‡†ç¡®æ€§ |
|------|-------------|------|------|--------|
| ä¼ ç»Ÿ RAG | 1æ¬¡ | ä½ | $ | â­â­â­ |
| Corrective RAG | 2-3æ¬¡ | ä¸­ | $$ | â­â­â­â­ |
| Self-RAG | 3-5æ¬¡ | é«˜ | $$$ | â­â­â­â­â­ |
| Multi-hop RAG | 4-6æ¬¡ | é«˜ | $$$ | â­â­â­â­â­ |

### æ¨èç­–ç•¥

::: tip ç”Ÿäº§ç¯å¢ƒå»ºè®®
1. **é»˜è®¤ç”¨ä¼ ç»Ÿ RAG**: 80% çš„åœºæ™¯å¤Ÿç”¨
2. **å…³é”®é—®é¢˜ç”¨ Corrective RAG**: éªŒè¯+å¤‡ç”¨æ£€ç´¢
3. **å¤æ‚åˆ†æç”¨ Multi-hop RAG**: å¤šæ­¥æ¨ç†
4. **å®éªŒæ€§åœºæ™¯ç”¨ Self-RAG**: è¿½æ±‚æè‡´è´¨é‡

**æ··åˆç­–ç•¥**: ç”¨ Adaptive RAG æ ¹æ®é—®é¢˜ç±»å‹è‡ªåŠ¨è·¯ç”±!
:::

---

## å®æˆ˜æ¡ˆä¾‹:å®¢æœç³»ç»Ÿçš„ Agentic RAG

**éœ€æ±‚**: æ„å»ºä¸€ä¸ªèƒ½å¤„ç†å¤æ‚å®¢æˆ·é—®é¢˜çš„æ™ºèƒ½å®¢æœã€‚

### é—®é¢˜ç±»å‹

1. **ç®€å•é—®é¢˜**: "ä½ ä»¬çš„é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆ?"  
   â†’ ä¼ ç»Ÿ RAG,æŸ¥çŸ¥è¯†åº“

2. **éœ€è¦éªŒè¯**: "æˆ‘çš„è®¢å•å· 12345 ä»€ä¹ˆæ—¶å€™å‘è´§?"  
   â†’ Corrective RAG,æŸ¥æ•°æ®åº“éªŒè¯è®¢å•å·æ˜¯å¦å­˜åœ¨

3. **å¤šæ­¥æ¨ç†**: "ä¸ºä»€ä¹ˆæˆ‘çš„è®¢å•è¢«å–æ¶ˆäº†?æ€ä¹ˆç”³è¯‰?"  
   â†’ Multi-hop RAG:
   - ç¬¬1æ­¥: æŸ¥è®¢å•çŠ¶æ€
   - ç¬¬2æ­¥: æŸ¥å–æ¶ˆåŸå› 
   - ç¬¬3æ­¥: æŸ¥ç”³è¯‰æµç¨‹

### æ¶æ„

```python
def customer_service_rag(query, user_id):
    # åˆ†ç±»é—®é¢˜
    query_type = classify_query(query)
    
    if query_type == "simple":
        # æŸ¥çŸ¥è¯†åº“
        docs = kb_search(query)
        return generate(docs)
    
    elif query_type == "order_related":
        # æå–è®¢å•å·
        order_id = extract_order_id(query)
        
        # éªŒè¯è®¢å•å·
        order_exists = db.check_order(order_id, user_id)
        if not order_exists:
            return "è®¢å•å·ä¸å­˜åœ¨,è¯·æ£€æŸ¥"
        
        # æŸ¥è®¢å•ä¿¡æ¯
        order_info = db.get_order(order_id)
        
        # ç”Ÿæˆç­”æ¡ˆ
        return generate(f"è®¢å•ä¿¡æ¯: {order_info}\né—®é¢˜: {query}")
    
    elif query_type == "complex":
        # å¤šæ­¥ RAG
        return multi_hop_rag(query, user_id)
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Agentic RAG** = Agent æ§åˆ¶çš„æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ
2. **å››å¤§æ¨¡å¼**:
   - Self-RAG: è‡ªæˆ‘åæ€,è¿­ä»£æ”¹è¿›
   - Corrective RAG: éªŒè¯ç»“æœ,çº æ­£é”™è¯¯
   - Adaptive RAG: è·¯ç”±ç­–ç•¥,å› åœ°åˆ¶å®œ
   - Multi-hop RAG: å¤šæ­¥æ¨ç†,ç»¼åˆåˆ†æ
3. **å·¥å…·**: LangGraph(çµæ´»)ã€LlamaIndex(æ˜“ç”¨)ã€DSPy(ç§‘ç ”)
4. **æƒè¡¡**: å‡†ç¡®æ€§ â†‘,æˆæœ¬/å»¶è¿Ÿ â†‘

### One-liner Summary

> **Agentic RAG å°±æ˜¯æŠŠ RAG ä»"æ¬è¿å·¥"å‡çº§ä¸º"ä¾¦æ¢" â€”â€” ä¼šæ€è€ƒã€ä¼šè´¨ç–‘ã€ä¼šå¤šæ¬¡è°ƒæŸ¥,ç›´åˆ°æ‰¾åˆ°çœŸç›¸ã€‚**

---

## å»¶ä¼¸é˜…è¯»

- [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [Self-RAG è®ºæ–‡](https://arxiv.org/abs/2310.11511)
- [Corrective RAG è®ºæ–‡](https://arxiv.org/abs/2401.15884)
- [LlamaIndex Agents æ•™ç¨‹](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/)

---

::: info ä¸‹ä¸€èŠ‚é¢„å‘Š
**12.7 AI Memory äº§å“å…¨æ™¯** â€”â€” ä» Mem0 åˆ° Langbase,çœ‹çœ‹å¸‚é¢ä¸Šæœ‰å“ªäº›å¼€ç®±å³ç”¨çš„è®°å¿†ç³»ç»Ÿã€‚
:::
