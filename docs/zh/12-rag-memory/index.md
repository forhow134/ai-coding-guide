# ç¬¬ 12 ç« :RAG & è®°å¿†å­˜å‚¨

> å­¦å®Œæœ¬ç« ,ä½ èƒ½:æ„å»º RAG ç³»ç»Ÿ,ä½¿ç”¨å‘é‡æ•°æ®åº“,å®ç° AI è®°å¿†ç®¡ç†

```mermaid
graph TB
    A["LLM çŸ¥è¯†å›°å¢ƒ"] -->|ä¸Šä¸‹æ–‡æœ‰é™| B["RAG æŠ€æœ¯"]
    B --> C["Load<br/>åŠ è½½æ–‡æ¡£"]
    C --> D["Split<br/>åˆ†å—"]
    D --> E["Embed<br/>å‘é‡åŒ–"]
    E --> F["Store<br/>å­˜å‚¨"]
    F --> G["Retrieve<br/>æ£€ç´¢"]
    G --> H["Generate<br/>ç”Ÿæˆ"]
    
    H --> I["é«˜çº§ RAG"]
    I --> J["åˆ†å—ç­–ç•¥"]
    I --> K["é‡æ’åº"]
    I --> L["æ··åˆæœç´¢"]
    
    H --> M["è®°å¿†ç®¡ç†"]
    M --> N["çŸ­æœŸè®°å¿†"]
    M --> O["é•¿æœŸè®°å¿†"]
    
    style B fill:#e1f5ff
    style I fill:#fff4e1
    style M fill:#ffe1e1
```

**ç« èŠ‚å¯¼è§ˆ:**

- **12.1 RAG åŸºç¡€**:ç†è§£ RAG åŸç†,ä»é›¶æ„å»º RAG ç³»ç»Ÿ
- **12.2 Embedding & å‘é‡æ•°æ®åº“**:å‘é‡åŒ–ä¸å‘é‡æœç´¢
- **12.3 é«˜çº§ RAG**:åˆ†å—ç­–ç•¥ã€é‡æ’åºã€æ··åˆæœç´¢
- **12.4 Memory & Storage**:AI è®°å¿†ç®¡ç†,å¯¹è¯ä¸Šä¸‹æ–‡ä¿æŒ

---

::: tip è¿è¡Œç¯å¢ƒ
æœ¬ç« ä»£ç æ¶‰åŠå¤šä¸ªåº“ï¼Œå»ºè®®ä¸€æ¬¡æ€§å®‰è£…ï¼š
```bash
pip install openai langchain chromadb python-dotenv
```
éƒ¨åˆ†é«˜çº§ç¤ºä¾‹è¿˜éœ€è¦ï¼š
```bash
pip install rank_bm25 cohere  # æ··åˆæ£€ç´¢ã€é‡æ’åº
```
:::

## 12.1 RAG åŸºç¡€ <DifficultyBadge level="intermediate" /> <CostBadge cost="$0.02" />

> LLM å°±åƒä¸ªå­¦éœ¸ï¼Œä½†è€ƒè¯•ä¸è®©çœ‹ä¹¦ï¼ŸRAG è¯´ï¼šè®© AI **å¼€å·è€ƒè¯•**ï¼Œæˆç»©è¹­è¹­æ¶¨ï¼

> å‰ç½®çŸ¥è¯†:3.1 LLM åŸºç¡€

### ä¸ºä»€ä¹ˆéœ€è¦å®ƒ?(Problem)

::: warning LLM çš„ä¸‰å¤§å°´å°¬æ—¶åˆ»
**å°´å°¬ 1ï¼šæ—¶é—´æ—…è¡Œè€…çš„å›°æƒ‘**
- ç”¨æˆ·ï¼š"2026 å¹´è°å½“é€‰äº†ï¼Ÿ"
- GPT-4ï¼š"æˆ‘çš„çŸ¥è¯†æˆªæ­¢ 2024 å¹´ 12 æœˆ..."
- ç”¨æˆ·ï¼šğŸ’¢

**å°´å°¬ 2ï¼šå…¬å¸æœºå¯†è¯»ä¸åˆ°**
- ç”¨æˆ·ï¼š"æˆ‘ä»¬å…¬å¸çš„é”€å”®æ”¿ç­–æ˜¯ï¼Ÿ"
- Claudeï¼š"æˆ‘æ²¡è§è¿‡ä½ ä»¬çš„å†…éƒ¨æ–‡æ¡£..."
- ç”¨æˆ·ï¼šğŸ’¢ğŸ’¢

**å°´å°¬ 3ï¼šä¿¡æ¯è¿‡è½½å®•æœº**
- ä½ ï¼šå¡ 100 ä¸‡å­—è¿› Prompt
- LLMï¼š"å¤ªå¤šäº†ï¼Œæˆ‘å¤´æ™•..." *æŒ‚äº†*
- ä½ ï¼šğŸ’¢ğŸ’¢ğŸ’¢
:::

**é—®é¢˜:LLM çš„ä¸‰å¤§çŸ¥è¯†å›°å¢ƒ**

```
å›°å¢ƒ 1:çŸ¥è¯†è¿‡æ—¶
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPT-4.1 è®­ç»ƒæ•°æ®æˆªæ­¢ 2024 å¹´ 12 æœˆ
Claude Sonnet 4.6 è®­ç»ƒæ•°æ®æˆªæ­¢ 2024 å¹´ 10 æœˆ
ç”¨æˆ·: "2026 å¹´è°æ˜¯æ€»ç»Ÿ?"
LLM: "æˆ‘ä¸çŸ¥é“ 2026 å¹´çš„ä¿¡æ¯"

å›°å¢ƒ 2:ç§æœ‰æ•°æ®
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å…¬å¸å†…éƒ¨æ–‡æ¡£ã€ç”¨æˆ·æ•°æ®ã€ä»£ç åº“
LLM è®­ç»ƒæ—¶æ²¡è§è¿‡,æ— æ³•å›ç­”

å›°å¢ƒ 3:ä¸Šä¸‹æ–‡é•¿åº¦æœ‰é™
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GPT-4.1: 1M tokens
GPT-5: 400K tokens
ä½†å…¨å¡è¿› Prompt:
  âŒ æˆæœ¬é«˜($0.01/1K tokens,300 é¡µ=$30)
  âŒ æ•ˆæœå·®(ä¿¡æ¯è¿‡è½½,æ³¨æ„åŠ›åˆ†æ•£)
  âŒ å»¶è¿Ÿé«˜(å¤„ç†æ—¶é—´é•¿)
```

**ä¸‰ç§è§£å†³æ–¹æ¡ˆå¯¹æ¯”:**

| æ–¹æ¡ˆ | åŸç† | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----|------|-----|------|---------|
| **Long Context** | å¡è¿› Prompt | ç®€å• | è´µã€æ…¢ã€æ•ˆæœå·® | å°è§„æ¨¡ã€ä¸€æ¬¡æ€§ |
| **Fine-tuning** | é‡æ–°è®­ç»ƒæ¨¡å‹ | æ•ˆæœå¥½ | æˆæœ¬é«˜ã€æ•°æ®ä¼šè¿‡æ—¶ã€è®­ç»ƒæ…¢ | ä¸“ä¸šé¢†åŸŸã€ç¨³å®šçŸ¥è¯† |
| **RAG** | å¤–æŒ‚çŸ¥è¯†åº“ | æˆæœ¬ä½ã€å®æ—¶æ›´æ–°ã€çµæ´» | æ£€ç´¢è´¨é‡å½±å“æ•ˆæœ | é€šç”¨åœºæ™¯ã€åŠ¨æ€æ•°æ® |

**RAG æ˜¯ä»€ä¹ˆ?**

RAG = **R**etrieval **A**ugmented **G**eneration(æ£€ç´¢å¢å¼ºç”Ÿæˆ)

```
ä¼ ç»Ÿ LLM:
ç”¨æˆ·é—®é¢˜ â†’ LLM â†’ ç­”æ¡ˆ
(ä»…ä¾èµ–è®­ç»ƒæ—¶çš„çŸ¥è¯†)

RAG:
ç”¨æˆ·é—®é¢˜ â†’ [æ£€ç´¢ç›¸å…³çŸ¥è¯†] â†’ LLM + çŸ¥è¯† â†’ ç­”æ¡ˆ
(åŠ¨æ€æ³¨å…¥å¤–éƒ¨çŸ¥è¯†)
```

**çœŸå®æ¡ˆä¾‹:å®¢æœæœºå™¨äºº**

```python
# ä¼ ç»Ÿæ–¹å¼:å…¨å¡ Prompt
prompt = f"""
ä½ æ˜¯å®¢æœæœºå™¨äºº,è¯·å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

å…¬å¸ä¿¡æ¯:
{å…¨éƒ¨äº§å“æ‰‹å†Œ}  # å‡è®¾ 50 ä¸‡å­—
{å…¨éƒ¨ FAQ}       # å‡è®¾ 10 ä¸‡å­—
{å…¨éƒ¨æ”¿ç­–æ–‡æ¡£}   # å‡è®¾ 5 ä¸‡å­—

ç”¨æˆ·é—®é¢˜:{question}
"""

é—®é¢˜:
âŒ 65 ä¸‡å­— â‰ˆ 160 ä¸‡ tokens
âŒ æˆæœ¬:$16/æ¬¡æŸ¥è¯¢(GPT-4)
âŒ å»¶è¿Ÿ:10+ ç§’
âŒ æ•ˆæœ:LLM è¢«æ— å…³ä¿¡æ¯æ·¹æ²¡

# RAG æ–¹å¼:åªæ£€ç´¢ç›¸å…³å†…å®¹
prompt = f"""
ä½ æ˜¯å®¢æœæœºå™¨äºº,è¯·å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç›¸å…³ä¿¡æ¯:
{æ£€ç´¢å‡ºçš„ 3 æ®µç›¸å…³å†…å®¹}  # å‡è®¾ 2000 å­—

ç”¨æˆ·é—®é¢˜:{question}
"""

ä¼˜åŠ¿:
âœ… 2000 å­— â‰ˆ 500 tokens
âœ… æˆæœ¬:$0.005/æ¬¡æŸ¥è¯¢
âœ… å»¶è¿Ÿ:1 ç§’
âœ… æ•ˆæœ:LLM ä¸“æ³¨äºç›¸å…³ä¿¡æ¯
```

### å®ƒæ˜¯ä»€ä¹ˆ?(Concept)

::: tip ç±»æ¯”æ—¶é—´ï¼šå¼€å·è€ƒè¯•
**é—­å·è€ƒè¯•ï¼ˆä¼ ç»Ÿ LLMï¼‰ï¼š**
- åªèƒ½é è„‘å­é‡Œçš„çŸ¥è¯†
- çŸ¥è¯†è¿‡æ—¶ï¼Ÿæ²¡åŠæ³•
- æ²¡å­¦è¿‡çš„ï¼Ÿä¸ä¼š

**å¼€å·è€ƒè¯•ï¼ˆRAGï¼‰ï¼š**
- å¯ä»¥ç¿»ä¹¦ã€æŸ¥èµ„æ–™
- æœ€æ–°çŸ¥è¯†ï¼ŸæŸ¥ä¹¦å°±è¡Œ
- ä¸“ä¸šé—®é¢˜ï¼Ÿæ‰¾åˆ°ç›¸å…³ç« èŠ‚å°±èƒ½ç­”

**RAG = ç»™ AI å¼€å·è€ƒè¯•çš„èµ„æ ¼** ğŸ“š
:::

**RAG å·¥ä½œæµç¨‹:**

```mermaid
graph TB
    subgraph "ç¦»çº¿ç´¢å¼•(ä¸€æ¬¡æ€§)"
        A["æ–‡æ¡£é›†åˆ"] --> B["Load<br/>åŠ è½½æ–‡æ¡£"]
        B --> C["Split<br/>åˆ‡åˆ†å—"]
        C --> D["Embed<br/>å‘é‡åŒ–"]
        D --> E["Store<br/>å­˜å…¥å‘é‡åº“"]
    end
    
    subgraph "åœ¨çº¿æŸ¥è¯¢(æ¯æ¬¡)"
        F["ç”¨æˆ·é—®é¢˜"] --> G["Embed<br/>å‘é‡åŒ–"]
        G --> H["Retrieve<br/>å‘é‡æœç´¢"]
        E --> H
        H --> I["Top-K ç›¸å…³æ–‡æ¡£"]
        I --> J["Generate<br/>LLM ç”Ÿæˆç­”æ¡ˆ"]
        F --> J
        J --> K["æœ€ç»ˆç­”æ¡ˆ"]
    end
    
    style A fill:#f0f0f0
    style E fill:#e1f5ff
    style H fill:#fff4e1
    style J fill:#e1ffe1
```

**å…­ä¸ªæ ¸å¿ƒæ­¥éª¤:**

### 1. Load(åŠ è½½æ–‡æ¡£)

```python
# æ”¯æŒå¤šç§æ ¼å¼
from langchain.document_loaders import (
    TextLoader,       # .txt
    PyPDFLoader,      # .pdf
    UnstructuredMarkdownLoader,  # .md
    CSVLoader,        # .csv
    JSONLoader,       # .json
    WebBaseLoader     # ç½‘é¡µ
)

# ç¤ºä¾‹:åŠ è½½ PDF
loader = PyPDFLoader("document.pdf")
documents = loader.load()
```

### 2. Split(åˆ‡åˆ†å—)

ä¸ºä»€ä¹ˆè¦åˆ‡åˆ†?
- Embedding æ¨¡å‹æœ‰é•¿åº¦é™åˆ¶(é€šå¸¸ 512 tokens)
- å°å—æ›´ç²¾å‡†(æ£€ç´¢æ—¶)
- æ§åˆ¶æˆæœ¬(æ¯æ¬¡æŸ¥è¯¢åªè¿”å›å‡ ä¸ªå°å—)

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # æ¯å— 1000 å­—ç¬¦
    chunk_overlap=200,      # å—ä¹‹é—´é‡å  200 å­—ç¬¦
    length_function=len,
    is_separator_regex=False,
)

chunks = text_splitter.split_documents(documents)
```

### 3. Embed(å‘é‡åŒ–)

å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡(æ•°å­—æ•°ç»„):

```python
from openai import OpenAI

client = OpenAI()

# æ–‡æœ¬ â†’ å‘é‡
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="è¿™æ˜¯ä¸€æ®µæ–‡æœ¬"
)

vector = response.data[0].embedding
# [0.123, -0.456, 0.789, ...] (1536 ç»´)
```

**ä¸ºä»€ä¹ˆéœ€è¦å‘é‡åŒ–?**

```
æ–‡æœ¬ A:"ç‹—æ˜¯äººç±»çš„æœ‹å‹"
æ–‡æœ¬ B:"çŠ¬ç±»æ˜¯äººç±»çš„å¥½ä¼™ä¼´"

å¦‚æœç”¨å…³é”®è¯åŒ¹é…:
  â†’ å®Œå…¨ä¸åŒ¹é…(æ²¡æœ‰ç›¸åŒè¯)

å¦‚æœç”¨å‘é‡:
  â†’ å‘é‡ A å’Œå‘é‡ B å¾ˆæ¥è¿‘(è¯­ä¹‰ç›¸ä¼¼)
  â†’ cosine_similarity(A, B) = 0.92
```

### 4. Store(å­˜å‚¨)

å°†å‘é‡å­˜å…¥å‘é‡æ•°æ®åº“:

```python
from langchain.vectorstores import Chroma

vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=OpenAIEmbeddings(),
    persist_directory="./chroma_db"
)
```

### 5. Retrieve(æ£€ç´¢)

æ ¹æ®é—®é¢˜å‘é‡,æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£å—:

```python
# ç”¨æˆ·é—®é¢˜
question = "ç‹—çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆ?"

# æ£€ç´¢ Top-3 ç›¸å…³æ–‡æ¡£
docs = vectorstore.similarity_search(question, k=3)

for doc in docs:
    print(doc.page_content)
```

### 6. Generate(ç”Ÿæˆç­”æ¡ˆ)

å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ + é—®é¢˜ â†’ LLM ç”Ÿæˆç­”æ¡ˆ:

```python
from openai import OpenAI

client = OpenAI()

# æ„é€  Prompt
context = "\n\n".join([doc.page_content for doc in docs])
prompt = f"""
è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:

{context}

é—®é¢˜:{question}
"""

# ç”Ÿæˆç­”æ¡ˆ
response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": prompt}]
)

print(response.choices[0].message.content)
```

**å®Œæ•´ RAG æµç¨‹ä»£ç :**

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from openai import OpenAI

# 1. Load
loader = TextLoader("knowledge.txt")
documents = loader.load()

# 2. Split
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

# 3. Embed + 4. Store
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=OpenAIEmbeddings()
)

# 5. Retrieve
question = "ç”¨æˆ·çš„é—®é¢˜"
docs = vectorstore.similarity_search(question, k=3)

# 6. Generate
client = OpenAI()
context = "\n\n".join([doc.page_content for doc in docs])
prompt = f"åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜:{question}"

response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": prompt}]
)

print(response.choices[0].message.content)
```

**RAG vs Fine-tuning vs Long Context:**

| ç»´åº¦ | RAG | Fine-tuning | Long Context |
|-----|-----|-------------|-------------|
| **æˆæœ¬** | ä½($0.01/æŸ¥è¯¢) | é«˜($100-$10,000) | ä¸­($0.30/æŸ¥è¯¢) |
| **å®æ—¶æ€§** | å®æ—¶æ›´æ–° | é‡æ–°è®­ç»ƒ | å®æ—¶ |
| **æ•°æ®é‡** | æ— é™(å¤–éƒ¨åº“) | æœ‰é™(è®­ç»ƒé›†) | æœ‰é™(128K tokens) |
| **å‡†ç¡®æ€§** | ä¸­(å–å†³äºæ£€ç´¢) | é«˜ | ä¸­(æ³¨æ„åŠ›åˆ†æ•£) |
| **å»¶è¿Ÿ** | ä¸­(æ£€ç´¢+ç”Ÿæˆ) | ä½(ç›´æ¥ç”Ÿæˆ) | é«˜(é•¿æ–‡æœ¬å¤„ç†) |
| **é€‚ç”¨åœºæ™¯** | åŠ¨æ€çŸ¥è¯†ã€é€šç”¨ | ä¸“ä¸šé¢†åŸŸã€å›ºå®šçŸ¥è¯† | å•æ–‡æ¡£åˆ†æ |

### åŠ¨æ‰‹è¯•è¯•(Practice)

å®Œæ•´çš„ RAG ç³»ç»Ÿå®ç°,ä»æ–‡æœ¬æ–‡ä»¶æ„å»ºçŸ¥è¯†åº“å¹¶å›ç­”é—®é¢˜ã€‚

<ColabBadge path="demos/12-rag-memory/basic_rag.ipynb" />

### å°ç»“(Reflection)

- **è§£å†³äº†ä»€ä¹ˆ**:ç†è§£äº† RAG å¦‚ä½•è§£å†³ LLM çš„çŸ¥è¯†å›°å¢ƒ,å¹¶æŒæ¡äº†åŸºç¡€ RAG æµç¨‹
- **æ²¡è§£å†³ä»€ä¹ˆ**:RAG çš„æ ¸å¿ƒæ˜¯å‘é‡æœç´¢,ä½†å‘é‡æ˜¯ä»€ä¹ˆ?å‘é‡æ•°æ®åº“å¦‚ä½•å·¥ä½œ?â€”â€”ä¸‹ä¸€èŠ‚è¯¦è§£
- **å…³é”®è¦ç‚¹**:
  1. **RAG è§£å†³ä¸‰å¤§å›°å¢ƒ**:çŸ¥è¯†è¿‡æ—¶ã€ç§æœ‰æ•°æ®ã€ä¸Šä¸‹æ–‡é•¿åº¦
  2. **å…­æ­¥æµç¨‹**:Load â†’ Split â†’ Embed â†’ Store â†’ Retrieve â†’ Generate
  3. **æ ¸å¿ƒåŸç†**:æ£€ç´¢ç›¸å…³çŸ¥è¯†,åŠ¨æ€æ³¨å…¥ Prompt
  4. **æˆæœ¬ä¼˜åŠ¿**:åªæ£€ç´¢ç›¸å…³å†…å®¹,ä¸éœ€è¦å…¨é‡è®­ç»ƒæˆ–å…¨é‡è¾“å…¥
  5. **å®æ—¶æ›´æ–°**:çŸ¥è¯†åº“æ›´æ–°,ç«‹å³ç”Ÿæ•ˆ
  6. **RAG vs å…¶ä»–æ–¹æ¡ˆ**:å„æœ‰ä¼˜åŠ£,æ ¹æ®åœºæ™¯é€‰æ‹©

::: tip ä¸€å¥è¯æ€»ç»“
**RAG = å¼€å·è€ƒè¯•ï¼Œè®© AI è¾¹æŸ¥èµ„æ–™è¾¹ç­”é¢˜ï¼Œæˆæœ¬ä½ã€æ•ˆæœå¥½ã€çŸ¥è¯†è¿˜èƒ½å®æ—¶æ›´æ–°ã€‚**
:::

**å…³é”®æ´å¯Ÿ:**
- RAG ä¸æ˜¯æ›¿ä»£ Fine-tuning,è€Œæ˜¯**äº’è¡¥**:Fine-tuning æå‡æ¨¡å‹èƒ½åŠ›,RAG æä¾›æœ€æ–°çŸ¥è¯†
- RAG çš„æ•ˆæœ**é«˜åº¦ä¾èµ–æ£€ç´¢è´¨é‡**:æ£€ç´¢ä¸å‡†,å†å¼ºçš„ LLM ä¹Ÿæ²¡ç”¨

---

*æœ€åæ›´æ–°:2026-02-20*
