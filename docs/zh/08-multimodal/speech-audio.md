# 8.3 Speech & Audio <DifficultyBadge level="intermediate" /> <CostBadge cost="$0.01" />

> å‰ç½®çŸ¥è¯†:8.1 Vision(å›¾åƒç†è§£)

::: tip å†·çŸ¥è¯†
Whisper è¿™ä¸ªåå­—æ¥è‡ª"çªƒçªƒç§è¯­",æš—ç¤ºå®ƒèƒ½å¬æ‡‚æœ€ç»†å¾®çš„å£°éŸ³ã€‚ä½†å®é™…ä¸Š,ä½ å¯¹ç€å®ƒå¤§å¼"HELP!!!"å®ƒä¹Ÿèƒ½æ·¡å®šåœ°è½¬å½•å‡ºæ¥,ä¸ä¼šå› ä¸ºä½ éŸ³é‡å¤§å°±å¤šæ”¶é’±ã€‚
:::

### ä¸ºä»€ä¹ˆéœ€è¦å®ƒ?(Problem)

**é—®é¢˜:è¯­éŸ³å†…å®¹éš¾ä»¥å¤„ç†å’Œç”Ÿæˆ**

æƒ³è±¡ä¸€ä¸‹:ä½ å¼€äº† 2 å°æ—¶çš„ä¼š,æ•£ä¼šåè€æ¿è¯´"æŠŠä¼šè®®çºªè¦æ•´ç†ä¸€ä¸‹å‘æˆ‘"ã€‚ä½ å†…å¿ƒ:???

- æ‰‹åŠ¨å¬å½•éŸ³é€å­—è®°å½•?å¤ªç´¯äº†,è€Œä¸”å®¹æ˜“æ‰“çŒç¡
- åªè®°å…³é”®ç‚¹?è€æ¿è¯´:"ä½ æ€ä¹ˆæŠŠæˆ‘è¯´çš„é‡ç‚¹éƒ½æ¼äº†?"

æˆ–è€…,ä½ æƒ³åšä¸ªè¯­éŸ³å®¢æœ,ä½†æ˜¯:
- å½•å›ºå®šè¯æœ¯?ç”¨æˆ·é—®çš„åƒå¥‡ç™¾æ€ª,è¯æœ¯æ ¹æœ¬ä¸å¤Ÿç”¨
- æ‰¾çœŸäººå®¢æœ?æˆæœ¬é«˜,è€Œä¸”åŠå¤œæ²¡äººå€¼ç­

**AI çš„è€³æœµå’Œå˜´å·´(Speech & Audio)æ¥æ•‘åœºäº†!** å®ƒèƒ½å¬æ‡‚äººè¯(STT),ä¹Ÿèƒ½è¯´äººè¯(TTS)ã€‚

çœŸå®åœºæ™¯:

**åœºæ™¯ 1:ä¼šè®®è®°å½•**

> "åˆšå¼€äº† 2 å°æ—¶çš„ä¼š,éœ€è¦æ•´ç†ä¼šè®®çºªè¦"

**ä¼ ç»Ÿæ–¹å¼:**
1. äººå·¥å¬å½•éŸ³é€å­—è®°å½•(è€—æ—¶ã€æ˜“å‡ºé”™)
2. åªèƒ½è®°å…³é”®ç‚¹(é—æ¼ç»†èŠ‚)

**åœºæ™¯ 2:å®¢æœè¯­éŸ³å›å¤**

> "ç”¨æˆ·æ‰“ç”µè¯å’¨è¯¢,éœ€è¦è‡ªåŠ¨è¯­éŸ³å›å¤"

**ä¼ ç»Ÿæ–¹å¼:**
1. å½•åˆ¶å›ºå®šè¯æœ¯(ä¸çµæ´»)
2. çœŸäººå®¢æœ(æˆæœ¬é«˜)

**åœºæ™¯ 3:æ’­å®¢/è§†é¢‘å­—å¹•**

> "è§†é¢‘éœ€è¦æ·»åŠ å­—å¹•,æ”¯æŒå¤šè¯­è¨€"

**ä¼ ç»Ÿæ–¹å¼:**
1. ä¸“ä¸šå­—å¹•ç»„åˆ¶ä½œ(å‘¨æœŸé•¿ã€è´µ)
2. è‡ªåŠ¨å­—å¹•å·¥å…·(å‡†ç¡®ç‡ä½)

**åœºæ™¯ 4:æ— éšœç¢è®¿é—®**

> "è§†éšœç”¨æˆ·éœ€è¦å¬ç½‘é¡µå†…å®¹"

**AI è¯­éŸ³æŠ€æœ¯è§£å†³äº†è¿™äº›é—®é¢˜:**
1. **STT(Speech-to-Text)**:è¯­éŸ³è½¬æ–‡å­—,è‡ªåŠ¨ç”Ÿæˆå­—å¹•ã€ä¼šè®®è®°å½•
2. **TTS(Text-to-Speech)**:æ–‡å­—è½¬è¯­éŸ³,ç”Ÿæˆè‡ªç„¶çš„è¯­éŸ³å›å¤

**ä¸€å¥è¯æ€»ç»“:ç»™ AI è£…ä¸Šè€³æœµå’Œå˜´å·´,è®©å®ƒèƒ½å¬èƒ½è¯´,å†ä¹Ÿä¸ç”¨æ‹…å¿ƒè€æ¿çš„"çµé­‚æ‹·é—®"äº†ã€‚** ğŸ§

::: warning ç¿»è½¦ç°åœº
æˆ‘:ç”¨ Whisper è½¬å½•è€æ¿çš„è¯­éŸ³æ¶ˆæ¯  
Whisper:"å—¯...é‚£ä¸ª...å°±æ˜¯...ç„¶å...æ‰€ä»¥..."  
æˆ‘:"åŸæ¥è€æ¿è¯´è¯è¿™ä¹ˆå¤šåºŸè¯!"  
è€æ¿(çœ‹åˆ°è½¬å½•æ–‡æœ¬):"..."  
æ•™è®­:**Whisper ä¼šå¿ å®è®°å½•æ¯ä¸€ä¸ª"å—¯""å•Š",åŒ…æ‹¬ä½ ä¸æƒ³è®©åˆ«äººçœ‹åˆ°çš„å£å¤´ç¦…ã€‚**
:::

### å®ƒæ˜¯ä»€ä¹ˆ?(Concept)

**Speech & Audio å¤„ç†åŒ…æ‹¬ä¸¤å¤§æ–¹å‘:**

```mermaid
graph LR
    A["è¯­éŸ³ AI"] --> B["STT<br/>Speech-to-Text"]
    A --> C["TTS<br/>Text-to-Speech"]
    
    B --> B1["Whisper<br/>OpenAI"]
    B --> B2["Speech API<br/>Google/Azure"]
    
    C --> C1["OpenAI TTS"]
    C --> C2["ElevenLabs"]
    C --> C3["Azure TTS"]
    
    style B fill:#e1ffe1
    style C fill:#fff3e0
```

---

## 1. Speech-to-Text(è¯­éŸ³è½¬æ–‡å­—)

**ä¸»æµ STT æ¨¡å‹:**

| æ¨¡å‹ | å…¬å¸ | ä»·æ ¼ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----|------|------|------|---------|
| **Whisper** | OpenAI | $0.006/åˆ†é’Ÿ | å¤šè¯­è¨€ã€å‡†ç¡®ç‡é«˜ | ä¼šè®®è®°å½•ã€å­—å¹•ç”Ÿæˆ |
| **Azure Speech** | Microsoft | $1/å°æ—¶ | å®æ—¶æµå¼ã€æ”¯æŒæ–¹è¨€ | å®¢æœã€å®æ—¶ç¿»è¯‘ |
| **Google Speech** | Google | $0.006/15 ç§’ | è‡ªåŠ¨æ ‡ç‚¹ã€è¯´è¯äººè¯†åˆ« | æ’­å®¢ã€é‡‡è®¿ |
| **AssemblyAI** | AssemblyAI | $0.00025/ç§’ | è‡ªåŠ¨æ‘˜è¦ã€å®ä½“è¯†åˆ« | å†…å®¹åˆ†æ |

::: tip Whisper çš„ä¼˜åŠ¿
- **å¤šè¯­è¨€**:æ”¯æŒ 99 ç§è¯­è¨€(ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡...)â€”â€”æ¯”ä½ çš„å¤–è¯­è€å¸ˆè¿˜å‰å®³
- **æ— éœ€è®­ç»ƒ**:å¼€ç®±å³ç”¨,ä¸ç”¨åƒä»¥å‰çš„è¯­éŸ³è¯†åˆ«é‚£æ ·"è®­ç»ƒåŠå¤©æ‰è®¤è¯†ä½ "
- **æ ¼å¼ä¸°å¯Œ**:æ”¯æŒ JSONã€SRTã€VTT ç­‰å¤šç§è¾“å‡ºæ ¼å¼â€”â€”æƒ³è¦å•¥æ ¼å¼å°±ç»™å•¥æ ¼å¼
- **å¼€æºç‰ˆæœ¬**:å¯ä»¥æœ¬åœ°éƒ¨ç½²(éœ€è¦ GPU)â€”â€”ä¸æƒ³èŠ±é’±?è‡ªå·±æ­!

**ä¸€å¥è¯:Whisper å°±æ˜¯è¯­éŸ³è½¬æ–‡å­—ç•Œçš„"å…­è¾¹å½¢æˆ˜å£«",å•¥éƒ½èƒ½å¹²ã€‚**
:::

**Whisper å·¥ä½œåŸç†:**

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant App as ä½ çš„åº”ç”¨
    participant API as Whisper API
    
    User->>App: ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶<br/>(mp3/wav/m4a...)
    App->>API: å‘é€è½¬å½•è¯·æ±‚<br/>file + language + format
    API->>API: è¯­éŸ³è¯†åˆ«<br/>+æ ‡ç‚¹+æ—¶é—´æˆ³
    API->>App: è¿”å›æ–‡å­—<br/>(JSON/SRT/VTT)
    App->>User: æ˜¾ç¤ºè½¬å½•ç»“æœ
    
    style API fill:#e1ffe1
```

**åŸºç¡€ä»£ç ç¤ºä¾‹:**

```python
from openai import OpenAI

client = OpenAI()

# æ‰“å¼€éŸ³é¢‘æ–‡ä»¶
audio_file = open("meeting.mp3", "rb")

# è½¬å½•
transcript = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file,
    response_format="text"  # text | json | srt | vtt | verbose_json
)

print(transcript)
```

**å‚æ•°è¯¦è§£:**

| å‚æ•° | è¯´æ˜ | å¯é€‰å€¼ |
|-----|------|--------|
| `model` | æ¨¡å‹ç‰ˆæœ¬ | `whisper-1` |
| `file` | éŸ³é¢‘æ–‡ä»¶ | æ”¯æŒ mp3, mp4, mpeg, mpga, m4a, wav, webm<br/>æœ€å¤§ 25MB |
| `language` | æºè¯­è¨€(å¯é€‰,æé«˜å‡†ç¡®ç‡) | `zh`(ä¸­æ–‡), `en`(è‹±æ–‡), `ja`(æ—¥æ–‡)... |
| `response_format` | è¾“å‡ºæ ¼å¼ | `text` | `json` | `srt` | `vtt` | `verbose_json` |
| `temperature` | é‡‡æ ·æ¸©åº¦ | 0-1,é»˜è®¤ 0(æ›´ç¡®å®š) |
| `prompt` | æç¤ºè¯(å¼•å¯¼é£æ ¼å’Œæœ¯è¯­) | å­—ç¬¦ä¸² |
| `timestamp_granularities` | æ—¶é—´æˆ³ç²’åº¦ | `segment` | `word` |

**æ”¯æŒçš„è¾“å‡ºæ ¼å¼:**

```python
# çº¯æ–‡æœ¬
response_text = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file,
    response_format="text"
)
print(response_text)  # "ä½ å¥½,ä»Šå¤©æˆ‘ä»¬è®¨è®º..."

# JSON(åŒ…å«è¯­è¨€ä¿¡æ¯)
response_json = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file,
    response_format="json"
)
print(response_json)
# {"text": "ä½ å¥½,ä»Šå¤©æˆ‘ä»¬è®¨è®º..."}

# SRT å­—å¹•æ ¼å¼
response_srt = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file,
    response_format="srt"
)
print(response_srt)
# 1
# 00:00:00,000 --> 00:00:03,000
# ä½ å¥½,ä»Šå¤©æˆ‘ä»¬è®¨è®º...

# Verbose JSON(è¯¦ç»†ä¿¡æ¯:æ—¶é—´æˆ³ã€ç½®ä¿¡åº¦)
response_verbose = client.audio.transcriptions.create(
    model="whisper-1",
    file=audio_file,
    response_format="verbose_json",
    timestamp_granularities=["word", "segment"]
)
print(response_verbose)
```

**ç¿»è¯‘åŠŸèƒ½(Translations):**

Whisper è¿˜æ”¯æŒå°†ä»»æ„è¯­è¨€ç¿»è¯‘æˆè‹±æ–‡:

```python
# å°†ä¸­æ–‡éŸ³é¢‘ç¿»è¯‘æˆè‹±æ–‡
translation = client.audio.translations.create(
    model="whisper-1",
    file=open("chinese_audio.mp3", "rb")
)

print(translation.text)  # è¾“å‡ºè‹±æ–‡ç¿»è¯‘
```

**ä¸€å¥è¯æ€»ç»“:Whisper ä¸ä»…èƒ½å¬æ‡‚ 99 ç§è¯­è¨€,è¿˜èƒ½å¸®ä½ "ç¿»è¯‘"æˆè‹±æ–‡ã€‚å¤–è¯­å¬åŠ›è€ƒè¯•?äº¤ç»™å®ƒ!** ğŸŒ

---

## 2. Text-to-Speech(æ–‡å­—è½¬è¯­éŸ³)

**ä¸»æµ TTS æ¨¡å‹:**

| æ¨¡å‹ | å…¬å¸ | ä»·æ ¼ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----|------|------|------|---------|
| **OpenAI TTS** | OpenAI | $15/1M å­—ç¬¦(tts-1)<br/>$30/1M å­—ç¬¦(tts-1-hd) | è‡ªç„¶ã€å¤šéŸ³è‰² | å†…å®¹æ’­æŠ¥ã€æœ‰å£°ä¹¦ |
| **ElevenLabs** | ElevenLabs | $5/æœˆ(30k å­—ç¬¦) | æƒ…æ„Ÿä¸°å¯Œã€å…‹éš†å£°éŸ³ | å¹¿å‘Šé…éŸ³ã€æ¸¸æˆ |
| **Azure TTS** | Microsoft | $16/1M å­—ç¬¦ | æ”¯æŒ SSMLã€ç¥ç»è¯­éŸ³ | ä¼ä¸šåº”ç”¨ |
| **Google TTS** | Google | $16/1M å­—ç¬¦ | å¤šè¯­è¨€ã€WaveNet | å¤šè¯­è¨€åº”ç”¨ |

::: tip å†·çŸ¥è¯†
OpenAI TTS çš„éŸ³è‰²åå­—éƒ½å¾ˆæœ‰æ„æ€:
- **alloy**(åˆé‡‘):ä¸­æ€§,åƒæ˜¯"é’¢é“ä¾ çš„ AI åŠ©æ‰‹ Jarvis"
- **echo**(å›å£°):ç”·æ€§,æ²‰ç¨³,é€‚åˆè®²"ææ€–æ•…äº‹"
- **nova**(æ–°æ˜Ÿ):å¥³æ€§,æ´»æ³¼,åƒæ˜¯"é‚»å®¶å¥³å­©"
- **shimmer**(å¾®å…‰):å¥³æ€§,æ¸©æŸ”,é€‚åˆè®²"ç¡å‰æ•…äº‹"

**ä¸€å¥è¯:é€‰éŸ³è‰²å°±åƒé€‰æ¼”å‘˜,æ‰¾å¯¹äº†èƒ½åŠ åˆ†,æ‰¾é”™äº†å°±ç¿»è½¦ã€‚**
:::

**OpenAI TTS éŸ³è‰²:**

| éŸ³è‰² | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|-----|------|---------|
| `alloy` | ä¸­æ€§ã€æ¸…æ™° | æ–°é—»æ’­æŠ¥ã€æ•™ç¨‹ |
| `echo` | ç”·æ€§ã€æ²‰ç¨³ | æœ‰å£°ä¹¦ã€è®²è§£ |
| `fable` | è‹±å¼ã€ä¼˜é›… | æ–‡å­¦ä½œå“ã€å¹¿å‘Š |
| `onyx` | æ·±æ²‰ã€ç£æ€§ | å“ç‰Œä»‹ç»ã€çºªå½•ç‰‡ |
| `nova` | å¥³æ€§ã€æ´»æ³¼ | å®¢æœã€è½»æ¾å†…å®¹ |
| `shimmer` | å¥³æ€§ã€æ¸©æŸ” | æ•…äº‹ã€å¼•å¯¼ |

**TTS å·¥ä½œåŸç†:**

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant App as ä½ çš„åº”ç”¨
    participant API as TTS API
    
    User->>App: è¾“å…¥æ–‡å­—
    App->>API: å‘é€ TTS è¯·æ±‚<br/>text + voice + model
    API->>API: æ–‡å­—åˆ†æ<br/>è¯­éŸ³åˆæˆ
    API->>App: è¿”å›éŸ³é¢‘æµ<br/>(mp3/opus/aac/flac)
    App->>User: æ’­æ”¾è¯­éŸ³
    
    style API fill:#fff3e0
```

**åŸºç¡€ä»£ç ç¤ºä¾‹:**

```python
from openai import OpenAI
from pathlib import Path

client = OpenAI()

# ç”Ÿæˆè¯­éŸ³
response = client.audio.speech.create(
    model="tts-1",  # tts-1 | tts-1-hd
    voice="alloy",  # alloy | echo | fable | onyx | nova | shimmer
    input="æ¬¢è¿ä½¿ç”¨ AI è¯­éŸ³åŠ©æ‰‹ã€‚ä»Šå¤©æˆ‘å°†ä¸ºæ‚¨ä»‹ç»å¦‚ä½•ä½¿ç”¨ OpenAI çš„ TTS åŠŸèƒ½ã€‚"
)

# ä¿å­˜åˆ°æ–‡ä»¶
speech_file_path = Path("output.mp3")
response.stream_to_file(speech_file_path)

print(f"è¯­éŸ³å·²ç”Ÿæˆ: {speech_file_path}")
```

**å‚æ•°è¯¦è§£:**

| å‚æ•° | è¯´æ˜ | å¯é€‰å€¼ |
|-----|------|--------|
| `model` | æ¨¡å‹ç‰ˆæœ¬ | `tts-1`(å¿«é€Ÿ)<br/>`tts-1-hd`(é«˜è´¨é‡,2 å€ä»·æ ¼) |
| `voice` | éŸ³è‰² | `alloy` | `echo` | `fable` | `onyx` | `nova` | `shimmer` |
| `input` | è¾“å…¥æ–‡å­— | æœ€å¤š 4096 å­—ç¬¦ |
| `response_format` | éŸ³é¢‘æ ¼å¼ | `mp3`(é»˜è®¤) | `opus` | `aac` | `flac` | `wav` | `pcm` |
| `speed` | è¯­é€Ÿ | 0.25-4.0,é»˜è®¤ 1.0 |

**æµå¼è¾“å‡º(å®æ—¶ç”Ÿæˆ):**

```python
from openai import OpenAI

client = OpenAI()

response = client.audio.speech.create(
    model="tts-1",
    voice="alloy",
    input="è¿™æ˜¯ä¸€æ®µéœ€è¦å®æ—¶æ’­æ”¾çš„æ–‡å­—ã€‚",
    response_format="opus"  # opus é€‚åˆæµå¼ä¼ è¾“
)

# æµå¼å†™å…¥æ–‡ä»¶
with open("speech_stream.opus", "wb") as f:
    for chunk in response.iter_bytes(chunk_size=1024):
        f.write(chunk)
```

**è°ƒèŠ‚è¯­é€Ÿ:**

```python
from openai import OpenAI

client = OpenAI()

text = "è¿™æ˜¯ä¸€æ®µæµ‹è¯•è¯­éŸ³,ç”¨äºå¯¹æ¯”ä¸åŒè¯­é€Ÿçš„æ•ˆæœã€‚"

# æ…¢é€Ÿ(0.5x)
response_slow = client.audio.speech.create(
    model="tts-1",
    voice="alloy",
    input=text,
    speed=0.5
)
response_slow.stream_to_file("slow.mp3")

# æ­£å¸¸é€Ÿåº¦(1.0x)
response_normal = client.audio.speech.create(
    model="tts-1",
    voice="alloy",
    input=text,
    speed=1.0
)
response_normal.stream_to_file("normal.mp3")

# å¿«é€Ÿ(1.5x)
response_fast = client.audio.speech.create(
    model="tts-1",
    voice="alloy",
    input=text,
    speed=1.5
)
response_fast.stream_to_file("fast.mp3")
```

---

## 3. å®Œæ•´åº”ç”¨åœºæ™¯

**åœºæ™¯ 1:ä¼šè®®è®°å½•ç³»ç»Ÿ**

```python
from openai import OpenAI

client = OpenAI()

def transcribe_meeting(audio_path: str) -> dict:
    """
    è½¬å½•ä¼šè®®éŸ³é¢‘å¹¶ç”Ÿæˆæ‘˜è¦
    """
    # 1. è¯­éŸ³è½¬æ–‡å­—
    with open(audio_path, "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="verbose_json",
            timestamp_granularities=["segment"]
        )
    
    # 2. ç”¨ LLM ç”Ÿæˆæ‘˜è¦å’Œå¾…åŠäº‹é¡¹
    summary_response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {
                "role": "user",
                "content": f"""
è¯·åˆ†æè¿™æ®µä¼šè®®è®°å½•,è¾“å‡º:
1. ä¼šè®®ä¸»é¢˜
2. æ ¸å¿ƒè®¨è®ºç‚¹(3-5 æ¡)
3. å¾…åŠäº‹é¡¹(TODO)
4. å…³é”®å†³ç­–

ä¼šè®®è®°å½•:
{transcript.text}
"""
            }
        ]
    )
    
    return {
        "transcript": transcript.text,
        "segments": transcript.segments,
        "summary": summary_response.choices[0].message.content
    }

# ä½¿ç”¨
result = transcribe_meeting("meeting.mp3")
print("è½¬å½•:", result["transcript"])
print("\næ‘˜è¦:", result["summary"])
```

**åœºæ™¯ 2:å¤šè¯­è¨€å®¢æœ**

```python
from openai import OpenAI

client = OpenAI()

def voice_customer_service(audio_path: str, language: str = "zh"):
    """
    è¯­éŸ³å®¢æœ:è¯†åˆ«å®¢æˆ·é—®é¢˜ â†’ LLM å›ç­” â†’ è¯­éŸ³å›å¤
    """
    # 1. è¯­éŸ³è½¬æ–‡å­—
    with open(audio_path, "rb") as audio_file:
        question = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language=language
        ).text
    
    print(f"å®¢æˆ·é—®é¢˜: {question}")
    
    # 2. LLM ç”Ÿæˆå›ç­”
    answer = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœ,å›ç­”è¦ç®€æ´å‹å¥½ã€‚"},
            {"role": "user", "content": question}
        ]
    ).choices[0].message.content
    
    print(f"å›ç­”: {answer}")
    
    # 3. æ–‡å­—è½¬è¯­éŸ³
    response = client.audio.speech.create(
        model="tts-1",
        voice="nova",  # å¥³æ€§ã€æ´»æ³¼
        input=answer
    )
    
    response.stream_to_file("customer_reply.mp3")
    
    return {
        "question": question,
        "answer": answer,
        "audio_path": "customer_reply.mp3"
    }

# ä½¿ç”¨
result = voice_customer_service("customer_question.mp3")
```

**åœºæ™¯ 3:è§†é¢‘å­—å¹•ç”Ÿæˆ**

```python
from openai import OpenAI

client = OpenAI()

def generate_subtitles(video_audio_path: str, output_srt: str = "subtitles.srt"):
    """
    ä»è§†é¢‘éŸ³é¢‘æå–å­—å¹•(SRT æ ¼å¼)
    """
    with open(video_audio_path, "rb") as audio_file:
        transcript = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="srt"  # ç›´æ¥è¾“å‡º SRT æ ¼å¼
        )
    
    # ä¿å­˜ SRT æ–‡ä»¶
    with open(output_srt, "w", encoding="utf-8") as f:
        f.write(transcript)
    
    print(f"å­—å¹•å·²ç”Ÿæˆ: {output_srt}")
    return transcript

# ä½¿ç”¨
srt_content = generate_subtitles("video.mp3", "output.srt")
print(srt_content)
```

**åœºæ™¯ 4:æœ‰å£°ä¹¦ç”Ÿæˆ**

```python
from openai import OpenAI
from pathlib import Path

client = OpenAI()

def text_to_audiobook(text_file: str, output_dir: str = "audiobook"):
    """
    å°†é•¿æ–‡æœ¬è½¬æ¢ä¸ºæœ‰å£°ä¹¦(åˆ†ç« èŠ‚)
    """
    # è¯»å–æ–‡æœ¬
    with open(text_file, "r", encoding="utf-8") as f:
        content = f.read()
    
    # æŒ‰ç« èŠ‚åˆ†å‰²(å‡è®¾ç”¨ "## ç¬¬ X ç« " åˆ†å‰²)
    chapters = content.split("##")
    chapters = [ch.strip() for ch in chapters if ch.strip()]
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # ä¸ºæ¯ç« ç”Ÿæˆè¯­éŸ³
    for i, chapter_text in enumerate(chapters, 1):
        print(f"ç”Ÿæˆç¬¬ {i} ç« ...")
        
        # é™åˆ¶é•¿åº¦(TTS æœ€å¤š 4096 å­—ç¬¦)
        if len(chapter_text) > 4000:
            chapter_text = chapter_text[:4000] + "..."
        
        response = client.audio.speech.create(
            model="tts-1-hd",  # é«˜è´¨é‡
            voice="echo",  # ç”·æ€§ã€æ²‰ç¨³
            input=chapter_text,
            speed=0.9  # ç•¥æ…¢,æ›´é€‚åˆå¬ä¹¦
        )
        
        response.stream_to_file(output_path / f"chapter_{i:02d}.mp3")
    
    print(f"æœ‰å£°ä¹¦å·²ç”Ÿæˆåˆ°: {output_path}")

# ä½¿ç”¨
text_to_audiobook("book.txt", "my_audiobook")
```

### åŠ¨æ‰‹è¯•è¯•(Practice)

å®Œæ•´ç¤ºä¾‹:ä¼šè®®è½¬å½•ã€å®¢æœè¯­éŸ³ã€å­—å¹•ç”Ÿæˆã€æœ‰å£°ä¹¦ã€‚

<ColabBadge path="demos/08-multimodal/speech_audio.ipynb" />

### å°ç»“(Reflection)

**ä¸€å¥è¯æ€»ç»“:æˆ‘ä»¬ç»™ AI è£…äº†è€³æœµå’Œå˜´å·´,ç°åœ¨å®ƒèƒ½"å¬è¯´è¯»å†™"äº†!(å°±å·®é—»å’Œæ‘¸äº†)**

- **è§£å†³äº†ä»€ä¹ˆ**:è¯­éŸ³è½¬æ–‡å­—(Whisper)ã€æ–‡å­—è½¬è¯­éŸ³(TTS),æ”¯æŒå¤šè¯­è¨€å’Œå¤šç§è¾“å‡ºæ ¼å¼
- **æ²¡è§£å†³ä»€ä¹ˆ**:éƒ½æ˜¯å•å‘çš„"å½•éŸ³ â†’ å¤„ç† â†’ è¾“å‡º",ä¸èƒ½å®æ—¶è¯­éŸ³å¯¹è¯â€”â€”ä¸‹ä¸€èŠ‚ä»‹ç»
- **å…³é”®è¦ç‚¹**:
  1. **Whisper è½¬å½•å‡†ç¡®ç‡é«˜**:æ”¯æŒ 99 ç§è¯­è¨€,ä»·æ ¼ $0.006/åˆ†é’Ÿ
  2. **å¤šç§è¾“å‡ºæ ¼å¼**:text | json | srt | vtt | verbose_json
  3. **TTS éŸ³è‰²ä¸°å¯Œ**:6 ç§éŸ³è‰²,è¦†ç›–ç”·å¥³ã€ä¸åŒé£æ ¼
  4. **è¯­é€Ÿå¯è°ƒèŠ‚**:0.25-4.0 å€é€Ÿ
  5. **æµå¼è¾“å‡º**:é€‚åˆå®æ—¶æ’­æ”¾
  6. **æ–‡ä»¶å¤§å°é™åˆ¶**:Whisper æœ€å¤§ 25MB
  7. **ç»„åˆ STT + LLM + TTS**:å®ç°æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹

::: warning ç¿»è½¦ç°åœº
æˆ‘:ç”¨ TTS ç”Ÿæˆæœ‰å£°ä¹¦,é€‰äº†æœ€å¿«çš„ 4.0 å€é€Ÿ  
ç»“æœ:å¬èµ·æ¥åƒ"æ‰“äº†å…´å¥‹å‰‚çš„æœºå…³æª"  
æœ‹å‹:"è¿™æ˜¯è¯´å”±å—?"  
æ•™è®­:**è¯­é€Ÿå¤ªå¿«ä¼šè®©äººå¬ä¸æ¸…,1.0-1.2 å€é€Ÿæœ€è‡ªç„¶ã€‚**
:::

::: tip å†·çŸ¥è¯†
ä¸ºä»€ä¹ˆ TTS ç”Ÿæˆçš„ä¸­æ–‡æœ‰æ—¶å€™"æ€ªæ€ªçš„"?å› ä¸ºä¸­æ–‡æœ‰å››ä¸ªå£°è°ƒ,TTS æ¨¡å‹å®¹æ˜“æŠŠ"å¦ˆå¦ˆéª‘é©¬,é©¬æ…¢,å¦ˆå¦ˆéª‚é©¬"å¿µæˆ"å¹³å¹³å¹³å¹³å¹³å¹³å¹³å¹³"ã€‚æƒ³è¦è‡ªç„¶çš„ä¸­æ–‡è¯­éŸ³?è¯•è¯• Azure TTS çš„ä¸­æ–‡ç¥ç»è¯­éŸ³,ä¸“é—¨è®­ç»ƒè¿‡ã€‚
:::

---

*æœ€åæ›´æ–°:2026-02-20*
