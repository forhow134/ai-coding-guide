## 13.3 Observabilityï¼ˆå¯è§‚æµ‹ï¼‰ï¼šç»™ AI è£…ä¸Šé£æœºé»‘åŒ£å­ <DifficultyBadge level="advanced" /> <CostBadge cost="$0.01" />

> å‰ç½®çŸ¥è¯†ï¼š7.1 Function Calling åŸºç¡€

::: warning ä¸€ä¸ªè‡´å‘½é—®é¢˜
ä¸Šçº¿ 3 å¤©åï¼Œç”¨æˆ·åé¦ˆ AI å›ç­”è´¨é‡å˜å·®äº†ã€‚ä½ æ‰“å¼€ä»£ç ï¼Œçœ‹äº†çœ‹é…ç½®ï¼Œä¸€åˆ‡æ­£å¸¸ã€‚ç„¶åâ€¦â€¦ä½ ä¸çŸ¥é“è¯¥æ€ä¹ˆåŠäº†ã€‚
:::

### ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿï¼ˆProblemï¼‰

**"ä¸Šçº¿ 3 å¤©åï¼Œç”¨æˆ·åé¦ˆ AI å›ç­”è´¨é‡å˜å·®äº†ï¼Œä½†ä½ ä¸çŸ¥é“ä¸ºä»€ä¹ˆã€‚"**

æ²¡æœ‰å¯è§‚æµ‹æ€§çš„ AI åº”ç”¨å°±åƒ**é£æœºæ²¡æœ‰é»‘åŒ£å­**â€”â€”å‡ºäº‹äº†æ‰çŸ¥é“å‡ºäº‹äº†ï¼Œä½†ä¸çŸ¥é“æ€ä¹ˆå‡ºçš„äº‹ã€‚

**ğŸ­ æƒ³è±¡è¿™ä¸ªåœºæ™¯ï¼š**
- è€æ¿ï¼šä¸ºä»€ä¹ˆå“åº”æ—¶é—´ä» 2 ç§’å˜æˆ 10 ç§’äº†ï¼Ÿ
- ä½ ï¼šâ€¦â€¦æˆ‘çœ‹çœ‹ä»£ç ï¼Ÿï¼ˆ10 åˆ†é’Ÿåï¼‰ä»£ç æ²¡é—®é¢˜å•Š
- è€æ¿ï¼šé‚£ä¸ºä»€ä¹ˆæ…¢äº†ï¼Ÿ
- ä½ ï¼šâ€¦â€¦å¯èƒ½æ˜¯æ¨¡å‹å˜æ…¢äº†ï¼Ÿæˆ–è€… API é™æµï¼Ÿæˆ–è€…â€¦â€¦
- è€æ¿ï¼šä½ åˆ°åº•çŸ¥ä¸çŸ¥é“ï¼Ÿ

**è¿™å°±æ˜¯æ²¡æœ‰ç›‘æ§çš„ç—›è‹¦ã€‚**

| é—®é¢˜ | ç—‡çŠ¶ | åŸå› ä¸æ˜ |
|------|------|---------|
| **å“åº”å˜æ…¢** | ç”¨æˆ·ç­‰å¾…æ—¶é—´ä» 2 ç§’å¢åŠ åˆ° 10 ç§’ | æ˜¯æ¨¡å‹å˜æ…¢è¿˜æ˜¯ API é™æµï¼Ÿ |
| **è´¨é‡ä¸‹é™** | ç”¨æˆ·æŠ•è¯‰ç­”æ¡ˆä¸å‡†ç¡® | å“ªäº›é—®é¢˜å‡ºé”™äº†ï¼Ÿ |
| **æˆæœ¬æš´æ¶¨** | æœ¬æœˆè´¦å•æ˜¯ä¸Šæœˆ 3 å€ | Token æ¶ˆè€—åœ¨å“ªé‡Œï¼Ÿ |
| **é”™è¯¯ç‡ä¸Šå‡** | API è°ƒç”¨å¤±è´¥ | æ˜¯è¶…æ—¶ã€é™æµè¿˜æ˜¯æ¨¡å‹é”™è¯¯ï¼Ÿ |

**çœŸå®æ¡ˆä¾‹ï¼š**

::: tip ç”µå•†å®¢æœæœºå™¨äººçš„è¡€æ³ªå²
æŸç”µå•†å®¢æœæœºå™¨äººä¸Šçº¿åï¼š
- **ç¬¬ 1 å¤©**ï¼šå“åº”æ—¶é—´ 2sï¼Œç”¨æˆ·æ»¡æ„åº¦ 85%ï¼Œå›¢é˜Ÿåº†ç¥
- **ç¬¬ 7 å¤©**ï¼šå“åº”æ—¶é—´ 8sï¼Œç”¨æˆ·æ»¡æ„åº¦ 60%ï¼Œå¼€å§‹ç´§å¼ 
- **ç¬¬ 14 å¤©**ï¼šå‘ç°æŸäº›ç”¨æˆ·çš„æé—®è§¦å‘äº†è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œå•æ¬¡æˆæœ¬ $0.50

**é—®é¢˜**ï¼šæ²¡æœ‰ç›‘æ§ï¼Œé—®é¢˜å‘ç°å¤ªæ™šï¼Œå·²ç»çƒ§äº† $500ã€‚
**æ•™è®­**ï¼šObservability ä¸æ˜¯å¯é€‰é¡¹ï¼Œæ˜¯æ•‘å‘½ç¨»è‰ã€‚
:::

**ä¸ºä»€ä¹ˆ AI åº”ç”¨ç‰¹åˆ«éœ€è¦å¯è§‚æµ‹æ€§ï¼Ÿ**

ä¼ ç»Ÿåº”ç”¨ï¼šè¯·æ±‚ â†’ å“åº”ï¼ˆç›‘æ§ HTTP çŠ¶æ€ç ã€å»¶è¿Ÿå°±å¤Ÿäº†ï¼‰  
AI åº”ç”¨ï¼šè¯·æ±‚ â†’ LLMï¼ˆ**Token æ¶ˆè€—ã€ä¸Šä¸‹æ–‡é•¿åº¦ã€è´¨é‡è¯„åˆ†ã€æ¨¡å‹æ¸©åº¦ã€ç¼“å­˜å‘½ä¸­â€¦â€¦**ï¼‰â†’ å“åº”

**éœ€è¦ç›‘æ§çš„ç»´åº¦æ›´å¤šã€æ›´å¤æ‚ã€æ›´çƒ§é’±ã€‚**

### å®ƒæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆConceptï¼‰

**Observabilityï¼ˆå¯è§‚æµ‹æ€§ï¼‰** æ˜¯é€šè¿‡æ—¥å¿—ã€æŒ‡æ ‡ã€è¿½è¸ªç†è§£ AI åº”ç”¨è¿è¡ŒçŠ¶æ€çš„èƒ½åŠ›ï¼š

**ğŸ›©ï¸ æ‰“ä¸ªæ¯”æ–¹ï¼š**
- **Logsï¼ˆæ—¥å¿—ï¼‰**ï¼šé£è¡Œæ—¥å¿—ï¼Œè®°å½•æ¯ä¸€æ¬¡æ“ä½œ
- **Metricsï¼ˆæŒ‡æ ‡ï¼‰**ï¼šä»ªè¡¨ç›˜ï¼Œæ˜¾ç¤ºé€Ÿåº¦ã€é«˜åº¦ã€æ²¹è€—
- **Tracesï¼ˆè¿½è¸ªï¼‰**ï¼šé£è¡Œè½¨è¿¹ï¼ŒçŸ¥é“ä»å“ªé£åˆ°å“ª

```mermaid
graph LR
    A["ç”¨æˆ·è¯·æ±‚"] --> B["AI åº”ç”¨"]
    B --> C["LLM API"]
    C --> B
    B --> D["å“åº”"]
    
    B --> E["æ—¥å¿—ç³»ç»Ÿ"]
    C --> E
    E --> F["Logs<br/>è°ƒç”¨è®°å½•"]
    E --> G["Metrics<br/>æ€§èƒ½æŒ‡æ ‡"]
    E --> H["Traces<br/>è°ƒç”¨é“¾"]
    
    F --> I["åˆ†æä»ªè¡¨ç›˜"]
    G --> I
    H --> I
    
    style E fill:#fff3e0
    style I fill:#c8e6c9
```

**ä¸‰å¤§æ”¯æŸ±ï¼š**

### 1. Logsï¼ˆæ—¥å¿—ï¼‰

è®°å½•æ¯æ¬¡ LLM è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯ï¼š

```json
{
  "timestamp": "2026-02-20T10:30:15Z",
  "session_id": "sess_abc123",
  "user_id": "user_456",
  "model": "gpt-4.1-mini",
  "prompt_tokens": 150,
  "completion_tokens": 80,
  "total_tokens": 230,
  "latency_ms": 1250,
  "cost_usd": 0.0012,
  "input": "å¦‚ä½•ä¼˜åŒ– SQL æŸ¥è¯¢ï¼Ÿ",
  "output": "ä¼˜åŒ– SQL æŸ¥è¯¢çš„å…³é”®æ–¹æ³•...",
  "quality_score": 8.5,
  "error": null
}
```

### 2. Metricsï¼ˆæŒ‡æ ‡ï¼‰

èšåˆç»Ÿè®¡ä¿¡æ¯ï¼š

| æŒ‡æ ‡ç±»å‹ | æŒ‡æ ‡åç§° | ç”¨é€” |
|---------|---------|------|
| **æ€§èƒ½æŒ‡æ ‡** | å¹³å‡å“åº”æ—¶é—´ã€P95/P99 å»¶è¿Ÿ | è¯†åˆ«æ€§èƒ½é—®é¢˜ |
| **æˆæœ¬æŒ‡æ ‡** | Token æ¶ˆè€—æ€»é‡ã€æ¯æ—¥æˆæœ¬ | æ§åˆ¶é¢„ç®— |
| **è´¨é‡æŒ‡æ ‡** | å¹³å‡è´¨é‡è¯„åˆ†ã€é”™è¯¯ç‡ | ç›‘æ§è¾“å‡ºè´¨é‡ |
| **ç”¨é‡æŒ‡æ ‡** | è¯·æ±‚æ•°ã€æ´»è·ƒç”¨æˆ·æ•° | äº†è§£ä½¿ç”¨æƒ…å†µ |

### 3. Tracesï¼ˆè¿½è¸ªï¼‰

è¿½è¸ªå®Œæ•´çš„è°ƒç”¨é“¾ï¼š

```mermaid
graph TD
    A["ç”¨æˆ·è¯·æ±‚"] --> B["æ£€ç´¢ç›¸å…³æ–‡æ¡£<br/>150ms"]
    B --> C["æ„å»º Prompt<br/>20ms"]
    C --> D["è°ƒç”¨ LLM<br/>1200ms"]
    D --> E["åå¤„ç†<br/>50ms"]
    E --> F["è¿”å›å“åº”<br/>æ€»è®¡ 1420ms"]
    
    style D fill:#ffcdd2
```

**ä¸»æµå¯è§‚æµ‹å·¥å…·ï¼š**

**1. LangSmithï¼ˆæ¨èï¼‰**

```python
from langsmith import Client
from langsmith.run_helpers import traceable

client = Client()

@traceable(run_type="llm", project_name="my-app")
def my_llm_call(question: str) -> str:
    response = openai_client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": question}]
    )
    return response.choices[0].message.content

# è‡ªåŠ¨è®°å½•åˆ° LangSmith
result = my_llm_call("ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ")
```

**2. OpenTelemetry for LLM**

```python
from opentelemetry import trace
from opentelemetry.instrumentation.openai import OpenAIInstrumentor

# è‡ªåŠ¨è¿½è¸ª OpenAI è°ƒç”¨
OpenAIInstrumentor().instrument()

tracer = trace.get_tracer(__name__)

with tracer.start_as_current_span("llm_call"):
    response = client.chat.completions.create(...)
```

**3. è‡ªå»ºæ—¥å¿—ç³»ç»Ÿ**

```python
import logging
import json
from datetime import datetime

class LLMLogger:
    def __init__(self, log_file: str = "llm_calls.jsonl"):
        self.log_file = log_file
    
    def log_call(self, **kwargs):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            **kwargs
        }
        with open(self.log_file, 'a') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
```

**ç›‘æ§ä»ªè¡¨ç›˜ç¤ºä¾‹ï¼š**

```mermaid
graph TD
    A["LLM å¯è§‚æµ‹ä»ªè¡¨ç›˜"] --> B["å®æ—¶æŒ‡æ ‡"]
    A --> C["å†å²è¶‹åŠ¿"]
    A --> D["å‘Šè­¦è§„åˆ™"]
    
    B --> B1["å½“å‰ QPS<br/>å¹³å‡å»¶è¿Ÿ<br/>é”™è¯¯ç‡"]
    C --> C1["Token æ¶ˆè€—è¶‹åŠ¿<br/>æˆæœ¬è¶‹åŠ¿<br/>è´¨é‡è¶‹åŠ¿"]
    D --> D1["å»¶è¿Ÿ>5s<br/>æˆæœ¬>$10/å¤©<br/>é”™è¯¯ç‡>1%"]
    
    style A fill:#c8e6c9
```

**å…³é”®ç›‘æ§æŒ‡æ ‡ï¼š**

```python
# æ ¸å¿ƒ KPI
core_metrics = {
    "performance": {
        "avg_latency_ms": 1200,
        "p95_latency_ms": 2500,
        "p99_latency_ms": 4000,
        "requests_per_second": 10,
    },
    "cost": {
        "total_tokens_today": 1500000,
        "cost_usd_today": 15.00,
        "avg_cost_per_request": 0.015,
    },
    "quality": {
        "avg_quality_score": 8.2,
        "error_rate": 0.005,  # 0.5%
        "user_satisfaction": 0.85,
    },
    "usage": {
        "active_users_today": 250,
        "total_requests_today": 1000,
        "avg_requests_per_user": 4,
    }
}
```

### åŠ¨æ‰‹è¯•è¯•ï¼ˆPracticeï¼‰

**å®éªŒ 1ï¼šæ„å»ºç®€å•çš„ LLM æ—¥å¿—ç³»ç»Ÿ**

```python
import json
import time
from datetime import datetime
from typing import Optional
from openai import OpenAI

client = OpenAI()

class LLMLogger:
    """LLM è°ƒç”¨æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_file: str = "llm_calls.jsonl"):
        self.log_file = log_file
    
    def log_call(
        self,
        input_text: str,
        output_text: str,
        model: str,
        prompt_tokens: int,
        completion_tokens: int,
        latency_ms: float,
        error: Optional[str] = None,
        **metadata
    ):
        """è®°å½•ä¸€æ¬¡ LLM è°ƒç”¨"""
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "model": model,
            "input": input_text,
            "output": output_text,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
            "latency_ms": round(latency_ms, 2),
            "error": error,
            **metadata
        }
        
        # è®¡ç®—æˆæœ¬ï¼ˆç¤ºä¾‹ä»·æ ¼ï¼‰
        if model == "gpt-4.1-mini":
            input_cost = prompt_tokens * 0.15 / 1_000_000
            output_cost = completion_tokens * 0.6 / 1_000_000
        else:
            input_cost = 0
            output_cost = 0
        
        log_entry["cost_usd"] = round(input_cost + output_cost, 6)
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    def get_metrics(self) -> dict:
        """åˆ†ææ—¥å¿—ï¼Œç”ŸæˆæŒ‡æ ‡"""
        with open(self.log_file, 'r', encoding='utf-8') as f:
            logs = [json.loads(line) for line in f]
        
        if not logs:
            return {}
        
        total_calls = len(logs)
        total_tokens = sum(log['total_tokens'] for log in logs)
        total_cost = sum(log['cost_usd'] for log in logs)
        latencies = [log['latency_ms'] for log in logs]
        errors = [log for log in logs if log['error']]
        
        return {
            "total_calls": total_calls,
            "total_tokens": total_tokens,
            "total_cost_usd": round(total_cost, 4),
            "avg_latency_ms": round(sum(latencies) / len(latencies), 2),
            "max_latency_ms": max(latencies),
            "error_count": len(errors),
            "error_rate": round(len(errors) / total_calls, 4),
        }

# ä½¿ç”¨æ—¥å¿—è®°å½•å™¨
logger = LLMLogger()

def tracked_llm_call(question: str) -> str:
    """å¸¦æ—¥å¿—çš„ LLM è°ƒç”¨"""
    start_time = time.time()
    error = None
    output_text = ""
    usage = None
    
    try:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": question}],
        )
        output_text = response.choices[0].message.content
        usage = response.usage
    except Exception as e:
        error = str(e)
    
    latency_ms = (time.time() - start_time) * 1000
    
    # è®°å½•æ—¥å¿—
    logger.log_call(
        input_text=question,
        output_text=output_text,
        model="gpt-4.1-mini",
        prompt_tokens=usage.prompt_tokens if usage else 0,
        completion_tokens=usage.completion_tokens if usage else 0,
        latency_ms=latency_ms,
        error=error,
    )
    
    return output_text

# æµ‹è¯•ï¼šæ¨¡æ‹Ÿå¤šæ¬¡è°ƒç”¨
questions = [
    "ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ",
    "è§£é‡Šä»€ä¹ˆæ˜¯é—­åŒ…",
    "Docker å’Œè™šæ‹Ÿæœºçš„åŒºåˆ«",
    "å¦‚ä½•ä¼˜åŒ– SQL æŸ¥è¯¢",
    "ä»€ä¹ˆæ˜¯ RESTful API",
]

print("=== æ‰§è¡Œ LLM è°ƒç”¨ ===\n")
for i, q in enumerate(questions, 1):
    print(f"{i}. {q}")
    answer = tracked_llm_call(q)
    print(f"   å›ç­”: {answer[:100]}...\n")

# æŸ¥çœ‹æŒ‡æ ‡
print("\n=== è¿è¡ŒæŒ‡æ ‡ ===")
metrics = logger.get_metrics()
for key, value in metrics.items():
    print(f"{key}: {value}")
```

**å®éªŒ 2ï¼šè¿½è¸ª RAG è°ƒç”¨é“¾**

```python
import time
from typing import List, Dict

class TraceLogger:
    """è°ƒç”¨é“¾è¿½è¸ªå™¨"""
    
    def __init__(self):
        self.traces: List[Dict] = []
        self.current_trace: Dict = {}
    
    def start_trace(self, name: str):
        """å¼€å§‹ä¸€ä¸ªè¿½è¸ª"""
        self.current_trace = {
            "name": name,
            "start_time": time.time(),
            "spans": []
        }
    
    def add_span(self, name: str, duration_ms: float, **metadata):
        """æ·»åŠ ä¸€ä¸ªæ­¥éª¤"""
        self.current_trace["spans"].append({
            "name": name,
            "duration_ms": round(duration_ms, 2),
            **metadata
        })
    
    def end_trace(self):
        """ç»“æŸè¿½è¸ª"""
        total_duration = (time.time() - self.current_trace["start_time"]) * 1000
        self.current_trace["total_duration_ms"] = round(total_duration, 2)
        self.traces.append(self.current_trace)
        self.current_trace = {}
    
    def print_trace(self):
        """æ‰“å°æœ€åä¸€æ¬¡è¿½è¸ª"""
        if not self.traces:
            return
        
        trace = self.traces[-1]
        print(f"\n{'='*60}")
        print(f"Trace: {trace['name']}")
        print(f"æ€»è€—æ—¶: {trace['total_duration_ms']}ms")
        print(f"{'='*60}")
        
        for span in trace["spans"]:
            print(f"  â””â”€ {span['name']}: {span['duration_ms']}ms")
            if span.get('tokens'):
                print(f"     Tokens: {span['tokens']}")

# æ¨¡æ‹Ÿ RAG ç³»ç»Ÿ
tracer = TraceLogger()

def simulated_rag_query(question: str) -> str:
    """æ¨¡æ‹Ÿ RAG æŸ¥è¯¢ï¼ˆå¸¦è¿½è¸ªï¼‰"""
    tracer.start_trace(f"RAG Query: {question[:30]}...")
    
    # Step 1: æ£€ç´¢æ–‡æ¡£
    start = time.time()
    time.sleep(0.15)  # æ¨¡æ‹Ÿæ£€ç´¢è€—æ—¶
    retrieved_docs = ["æ–‡æ¡£1", "æ–‡æ¡£2", "æ–‡æ¡£3"]
    tracer.add_span("æ£€ç´¢ç›¸å…³æ–‡æ¡£", (time.time() - start) * 1000, doc_count=3)
    
    # Step 2: æ„å»º Prompt
    start = time.time()
    time.sleep(0.02)  # æ¨¡æ‹Ÿæ„å»º Prompt
    prompt = f"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š{retrieved_docs}\n\né—®é¢˜ï¼š{question}"
    tracer.add_span("æ„å»º Prompt", (time.time() - start) * 1000, prompt_length=len(prompt))
    
    # Step 3: è°ƒç”¨ LLM
    start = time.time()
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": prompt}],
    )
    llm_duration = (time.time() - start) * 1000
    answer = response.choices[0].message.content
    tracer.add_span(
        "è°ƒç”¨ LLM",
        llm_duration,
        tokens=response.usage.total_tokens,
        model="gpt-4.1-mini"
    )
    
    # Step 4: åå¤„ç†
    start = time.time()
    time.sleep(0.05)  # æ¨¡æ‹Ÿåå¤„ç†
    tracer.add_span("åå¤„ç†", (time.time() - start) * 1000)
    
    tracer.end_trace()
    return answer

# æµ‹è¯•è¿½è¸ª
result = simulated_rag_query("ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ")
tracer.print_trace()

print(f"\nå›ç­”: {result}")
```

**å®éªŒ 3ï¼šå®æ—¶æ€§èƒ½ç›‘æ§ä»ªè¡¨ç›˜ï¼ˆç®€åŒ–ç‰ˆï¼‰**

```python
import time
from collections import deque
from datetime import datetime, timedelta

class MetricsCollector:
    """å®æ—¶æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, window_minutes: int = 5):
        self.window = timedelta(minutes=window_minutes)
        self.data = deque()  # (timestamp, latency, tokens, cost)
    
    def record(self, latency_ms: float, tokens: int, cost_usd: float):
        """è®°å½•ä¸€æ¬¡è°ƒç”¨"""
        self.data.append((datetime.now(), latency_ms, tokens, cost_usd))
        self._cleanup_old_data()
    
    def _cleanup_old_data(self):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        cutoff = datetime.now() - self.window
        while self.data and self.data[0][0] < cutoff:
            self.data.popleft()
    
    def get_metrics(self) -> dict:
        """è·å–å½“å‰çª—å£çš„æŒ‡æ ‡"""
        if not self.data:
            return {}
        
        latencies = [d[1] for d in self.data]
        tokens = [d[2] for d in self.data]
        costs = [d[3] for d in self.data]
        
        # è®¡ç®— QPS
        duration_seconds = (self.data[-1][0] - self.data[0][0]).total_seconds()
        qps = len(self.data) / duration_seconds if duration_seconds > 0 else 0
        
        return {
            "qps": round(qps, 2),
            "total_requests": len(self.data),
            "avg_latency_ms": round(sum(latencies) / len(latencies), 2),
            "p95_latency_ms": round(sorted(latencies)[int(len(latencies) * 0.95)], 2),
            "total_tokens": sum(tokens),
            "total_cost_usd": round(sum(costs), 4),
        }
    
    def print_dashboard(self):
        """æ‰“å°ä»ªè¡¨ç›˜"""
        metrics = self.get_metrics()
        
        print("\n" + "="*60)
        print(f"ğŸ“Š å®æ—¶ç›‘æ§ä»ªè¡¨ç›˜ (æœ€è¿‘ {self.window.seconds // 60} åˆ†é’Ÿ)")
        print("="*60)
        print(f"è¯·æ±‚æ•°:       {metrics.get('total_requests', 0)}")
        print(f"QPS:          {metrics.get('qps', 0)}")
        print(f"å¹³å‡å»¶è¿Ÿ:     {metrics.get('avg_latency_ms', 0)} ms")
        print(f"P95 å»¶è¿Ÿ:     {metrics.get('p95_latency_ms', 0)} ms")
        print(f"Token æ¶ˆè€—:   {metrics.get('total_tokens', 0):,}")
        print(f"æˆæœ¬:         ${metrics.get('total_cost_usd', 0)}")
        print("="*60 + "\n")

# ä½¿ç”¨ç›‘æ§å™¨
monitor = MetricsCollector(window_minutes=5)

# æ¨¡æ‹Ÿæµé‡
print("æ¨¡æ‹Ÿ LLM è°ƒç”¨æµé‡...\n")
for i in range(20):
    # æ¨¡æ‹Ÿè°ƒç”¨
    latency = 800 + (i % 5) * 200  # 800-1600ms
    tokens = 100 + (i % 3) * 50    # 100-200 tokens
    cost = tokens * 0.15 / 1_000_000
    
    monitor.record(latency, tokens, cost)
    
    if (i + 1) % 5 == 0:
        monitor.print_dashboard()
    
    time.sleep(0.1)  # æ¨¡æ‹Ÿè¯·æ±‚é—´éš”
```

<ColabBadge path="demos/13-production/observability.ipynb" />

### å°ç»“ï¼ˆReflectionï¼‰

**ğŸ¯ ä¸€å¥è¯æ€»ç»“ï¼šObservability æ˜¯ AI åº”ç”¨çš„é£æœºé»‘åŒ£å­ï¼Œè®°å½•ä¸€åˆ‡ã€åˆ†æä¸€åˆ‡ã€é¢„è­¦ä¸€åˆ‡ã€‚**

- **è§£å†³äº†ä»€ä¹ˆ**ï¼šä¸º AI åº”ç”¨æ·»åŠ æ—¥å¿—ã€æŒ‡æ ‡ã€è¿½è¸ªï¼Œå®æ—¶ç›‘æ§æ€§èƒ½ã€æˆæœ¬ã€è´¨é‡
- **æ²¡è§£å†³ä»€ä¹ˆ**ï¼šæœ‰äº†ç›‘æ§ï¼Œä½†æˆæœ¬è¿˜æ˜¯å¤ªé«˜ï¼Ÿâ€”â€”ä¸‹ä¸€èŠ‚ä»‹ç»æˆæœ¬ä¼˜åŒ–
- **å…³é”®è¦ç‚¹**ï¼š
  1. **ä¸‰å¤§æ”¯æŸ±**ï¼šLogsï¼ˆè¯¦ç»†è®°å½•ï¼‰ã€Metricsï¼ˆèšåˆæŒ‡æ ‡ï¼‰ã€Tracesï¼ˆè°ƒç”¨é“¾ï¼‰
  2. **å…³é”®æŒ‡æ ‡**ï¼šå»¶è¿Ÿã€Token æ¶ˆè€—ã€æˆæœ¬ã€è´¨é‡è¯„åˆ†ã€é”™è¯¯ç‡
  3. **å·¥å…·é€‰æ‹©**ï¼šLangSmithï¼ˆæ‰˜ç®¡ï¼Œçœå¿ƒï¼‰ã€OpenTelemetryï¼ˆå¼€æºï¼Œè‡ªç”±ï¼‰ã€è‡ªå»ºæ—¥å¿—ç³»ç»Ÿï¼ˆä¾¿å®œï¼‰
  4. **å®æ—¶ç›‘æ§**ï¼šè®¾ç½®ä»ªè¡¨ç›˜å’Œå‘Šè­¦è§„åˆ™ï¼ˆåˆ«ç­‰å‡ºäº‹æ‰çŸ¥é“ï¼‰
  5. **æŒç»­ä¼˜åŒ–**ï¼šæ ¹æ®ç›‘æ§æ•°æ®å‘ç°æ€§èƒ½ç“¶é¢ˆå’Œæˆæœ¬å¼‚å¸¸

::: tip è®°ä½è¿™ä¸ªæ¯”å–»
Observability = é£æœºé»‘åŒ£å­ï¼šå¹³æ—¶ä¸èµ·çœ¼ï¼Œå‡ºäº‹åæ‰çŸ¥é“å®ƒå¤šé‡è¦ã€‚
:::

---

*æœ€åæ›´æ–°ï¼š2026-02-20*
