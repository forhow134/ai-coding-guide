## 13.4 æˆæœ¬ä¼˜åŒ–ä¸å®‰å…¨ï¼šåˆ«è®© AI æˆä¸ºç”µè´¹æ€æ‰‹ <DifficultyBadge level="advanced" /> <CostBadge cost="$0" />

> å‰ç½®çŸ¥è¯†ï¼š3.1 Token ä¸è®¡è´¹

::: danger ææ€–æ•…äº‹
**ä¸Šä¸ªæœˆè´¦å•ï¼š$5000**  
**è¿™ä¸ªæœˆè´¦å•ï¼š$15000**  
**è€æ¿ï¼šä¸‹ä¸ªæœˆé¢„ç®—å¤šå°‘ï¼Ÿ**  
**ä½ ï¼šâ€¦â€¦è¦ä¸æˆ‘ä»¬åˆ«ç”¨ AI äº†ï¼Ÿ**
:::

### ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿï¼ˆProblemï¼‰

**"ä¸Šä¸ªæœˆçš„ AI API è´¦å•ï¼š$5000ã€‚è¿™ä¸ªæœˆï¼š$15000ã€‚ä¸‹ä¸ªæœˆé¢„ç®—å¤šå°‘ï¼Ÿ"**

æ²¡æœ‰æˆæœ¬æ§åˆ¶çš„ AI åº”ç”¨å°±åƒ**æ¼æ°´çš„æ°´é¾™å¤´**â€”â€”å¼€å§‹åªæ˜¯æ»´ç­”æ»´ç­”ï¼Œæœ€åå˜æˆç€‘å¸ƒã€‚

**ğŸ­ æƒ³è±¡è¿™ä¸ªåœºæ™¯ï¼š**
- ç¬¬ 1 å‘¨ï¼šè´¦å• $100ï¼Œæ„Ÿè§‰è¿˜è¡Œ
- ç¬¬ 2 å‘¨ï¼šè´¦å• $500ï¼Œæœ‰ç‚¹å¤šä½†èƒ½æ¥å—
- ç¬¬ 3 å‘¨ï¼šè´¦å• $2000ï¼Œå¼€å§‹å¿ƒç–¼
- ç¬¬ 4 å‘¨ï¼šè´¦å• $8000ï¼Œè€æ¿æŠŠä½ å«è¿›åŠå…¬å®¤

**ç„¶åä½ å‘ç°ï¼šåŸæ¥æ˜¯æ¯æ¬¡éƒ½æŠŠ 3000 Token çš„ç³»ç»Ÿæç¤ºè¯é‡å¤å‘é€äº†å‡ åƒæ¬¡ã€‚**

| åœºæ™¯ | æˆæœ¬é™·é˜± | æŸå¤± |
|------|---------|------|
| **é•¿ä¸Šä¸‹æ–‡å¯¹è¯** | æ¯æ¬¡éƒ½å‘é€å®Œæ•´å†å²è®°å½• | 10 è½®å¯¹è¯ Token æ¶ˆè€— 10 å€ |
| **é‡å¤é—®é¢˜** | åŒæ ·çš„é—®é¢˜é‡å¤è°ƒç”¨ LLM | æµªè´¹ 70% Token |
| **æ‰¹é‡å¤„ç†** | é€æ¡å®æ—¶è°ƒç”¨ | æˆæœ¬æ˜¯æ‰¹é‡çš„ 2 å€ |
| **æœªä¼˜åŒ– Prompt** | å†—é•¿çš„ç³»ç»Ÿæç¤ºè¯ | æ¯æ¬¡å¤šæ¶ˆè€— 500 Token |

**çœŸå®æ¡ˆä¾‹ï¼š**

::: tip ä¼ä¸šæ–‡æ¡£ Q&A ç³»ç»Ÿçš„æˆæœ¬ä¼˜åŒ–ä¼ å¥‡
**ä¼˜åŒ–å‰ï¼š**
- ç³»ç»Ÿæç¤ºè¯ï¼š3000 Tokenï¼ˆåŒ…å«è¯¦ç»†çš„å…¬å¸è§„åˆ™ï¼‰
- æ¯æ¬¡æŸ¥è¯¢é‡å¤å‘é€ç³»ç»Ÿæç¤ºè¯
- æ¯æ—¥ 10000 æ¬¡æŸ¥è¯¢ = 3000ä¸‡ è¾“å…¥ Token
- **æœˆæˆæœ¬ï¼š$4500**ï¼ˆä»…ç³»ç»Ÿæç¤ºè¯ï¼ï¼‰

**ä½¿ç”¨ Prompt Caching åï¼š**
- ç³»ç»Ÿæç¤ºè¯ç¼“å­˜ï¼Œåªè®¡è´¹ä¸€æ¬¡
- **æœˆæˆæœ¬ï¼š$450**ï¼ˆ**é™ä½ 90%**ï¼‰

**èŠ‚çœçš„é’±å¹²ä»€ä¹ˆäº†ï¼Ÿç»™å›¢é˜Ÿå›¢å»ºåƒäº†é¡¿ç«é”…ã€‚**
:::

**ä¸ºä»€ä¹ˆ AI åº”ç”¨æˆæœ¬éš¾æ§åˆ¶ï¼Ÿ**

- **Token è®¡è´¹å¤æ‚**ï¼šè¾“å…¥ Tokenã€è¾“å‡º Tokenã€ç¼“å­˜ Token ä»·æ ¼ä¸åŒï¼ˆåƒæ‰‹æœºå¥—é¤ä¸€æ ·å¤æ‚ï¼‰
- **æˆæœ¬ä¸é€æ˜**ï¼šä¸çŸ¥é“å“ªä¸ªåŠŸèƒ½åœ¨å·å·çƒ§é’±
- **ç”¨é‡éš¾é¢„æµ‹**ï¼šç”¨æˆ·è¡Œä¸ºå¤šæ ·ï¼ŒToken æ¶ˆè€—åƒè¿‡å±±è½¦
- **ä¼˜åŒ–ä¸ç›´è§‚**ï¼šä¸çŸ¥é“ä»å“ªé‡Œä¸‹æ‰‹ï¼ˆå°±åƒå‡è‚¥ä¸çŸ¥é“å…ˆå‡å“ªï¼‰

**æˆæœ¬ä¼˜åŒ–ä¸æ˜¯å¯é€‰é¡¹ï¼Œæ˜¯ AI åº”ç”¨çš„"æ°´ç”µè´¹è´¦å•"ã€‚**

### å®ƒæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆConceptï¼‰

**æˆæœ¬ä¼˜åŒ–**æ˜¯é€šè¿‡æŠ€æœ¯æ‰‹æ®µé™ä½ AI åº”ç”¨çš„ Token æ¶ˆè€—å’Œ API è´¹ç”¨ï¼š

**ğŸ’° æ‰“ä¸ªæ¯”æ–¹ï¼š**
- **Prompt Caching**ï¼šé‡å¤ç”¨çš„ä¸œè¥¿ï¼ˆç³»ç»Ÿæç¤ºè¯ï¼‰ä¹°ä¸€æ¬¡å°±å¤Ÿäº†ï¼ŒåƒåŠå¥èº«å¡
- **Batch API**ï¼šæ‰¹é‡é‡‡è´­æ‰“ 5 æŠ˜ï¼Œåƒå»æ‰¹å‘å¸‚åœº
- **Semantic Caching**ï¼šè®°ä½ç­”æ¡ˆï¼Œç›¸åŒé—®é¢˜ä¸é‡å¤é—®ï¼Œåƒåšä½œä¸šæŠ„ç­”æ¡ˆ
- **æ¨¡å‹é€‰æ‹©**ï¼šç®€å•ä»»åŠ¡ç”¨ä¾¿å®œè´§ï¼Œå¤æ‚ä»»åŠ¡æ‰ç”¨è´µçš„ï¼Œåƒæ‰“è½¦é€‰å¿«è½¦è¿˜æ˜¯ä¸“è½¦

```mermaid
graph TD
    A["æˆæœ¬ä¼˜åŒ–ç­–ç•¥"] --> B["1. Prompt Caching<br/>-90% è¾“å…¥æˆæœ¬"]
    A --> C["2. Batch API<br/>-50% æˆæœ¬"]
    A --> D["3. Semantic Caching<br/>é¿å…é‡å¤è°ƒç”¨"]
    A --> E["4. æ¨¡å‹é€‰æ‹©<br/>é€‚é…ä»»åŠ¡å¤æ‚åº¦"]
    A --> F["5. è¾“å‡ºæ§åˆ¶<br/>é™åˆ¶ max_tokens"]
    
    style A fill:#c8e6c9
    style B fill:#fff3e0
    style C fill:#fff3e0
    style D fill:#fff3e0
```

### 1. Prompt Cachingï¼ˆæœ€å¼ºä¼˜åŒ–ï¼‰

**åŸç†ï¼šå°†ä¸å˜çš„å‰ç¼€å†…å®¹ï¼ˆå¦‚ç³»ç»Ÿæç¤ºè¯ã€æ–‡æ¡£ï¼‰ç¼“å­˜ 5 åˆ†é’Ÿï¼Œåç»­è¯·æ±‚å…è´¹æˆ–å¤§å¹…é™ä»·ã€‚**

| ä¾›åº”å•† | ç¼“å­˜ä»·æ ¼ | å†™å…¥ä»·æ ¼ | èŠ‚çœ |
|--------|---------|---------|------|
| **OpenAI** | è¾“å…¥ä»·æ ¼çš„ 50% | è¾“å…¥ä»·æ ¼çš„ 125% | é™ä½ 50% |
| **Anthropic** | è¾“å…¥ä»·æ ¼çš„ 10% | è¾“å…¥ä»·æ ¼çš„ 125% | **é™ä½ 90%** |

**é€‚ç”¨åœºæ™¯ï¼š**

- å›ºå®šçš„ç³»ç»Ÿæç¤ºè¯ï¼ˆæ¯ä¸ªè¯·æ±‚éƒ½ç›¸åŒï¼‰
- RAG æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆ5 åˆ†é’Ÿå†…å¯èƒ½é‡å¤æŸ¥è¯¢ï¼‰
- é•¿ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ†æï¼ˆæ–‡æ¡£å†…å®¹ä¸å˜ï¼‰

**OpenAI Prompt Caching ç¤ºä¾‹ï¼š**

```python
from openai import OpenAI

client = OpenAI()

# é•¿ç³»ç»Ÿæç¤ºè¯ï¼ˆ3000 Tokenï¼‰
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹...ï¼ˆæ­¤å¤„çœç•¥ 3000 å­—è§„åˆ™è¯´æ˜ï¼‰
"""

# ä½¿ç”¨ Prompt Caching
response = client.chat.completions.create(
    model="gpt-4.1",  # éœ€è¦æ”¯æŒ caching çš„æ¨¡å‹
    messages=[
        {
            "role": "system",
            "content": [
                {
                    "type": "text",
                    "text": SYSTEM_PROMPT,
                    "cache_control": {"type": "ephemeral"}  # æ ‡è®°ä¸ºå¯ç¼“å­˜
                }
            ]
        },
        {"role": "user", "content": "ä½ ä»¬çš„é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ"}
    ]
)

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šå†™å…¥ç¼“å­˜ï¼ˆ125% ä»·æ ¼ï¼‰
# 5 åˆ†é’Ÿå†…çš„åç»­è°ƒç”¨ï¼šä½¿ç”¨ç¼“å­˜ï¼ˆ50% ä»·æ ¼ï¼‰
```

**Anthropic Prompt Cachingï¼ˆæ›´æ¿€è¿›ï¼‰ï¼š**

```python
import anthropic

client = anthropic.Anthropic()

# é•¿æ–‡æ¡£ï¼ˆ10000 Tokenï¼‰
LONG_DOCUMENT = """ï¼ˆæ­¤å¤„çœç•¥ 1 ä¸‡å­—æ–‡æ¡£å†…å®¹ï¼‰"""

response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚"
        },
        {
            "type": "text",
            "text": LONG_DOCUMENT,
            "cache_control": {"type": "ephemeral"}  # ç¼“å­˜æ–‡æ¡£
        }
    ],
    messages=[
        {"role": "user", "content": "æ€»ç»“è¿™ä¸ªæ–‡æ¡£çš„æ ¸å¿ƒè§‚ç‚¹"}
    ]
)

# ç¼“å­˜å‘½ä¸­æ—¶ï¼Œæ–‡æ¡£éƒ¨åˆ†åªæ”¶å– 10% ä»·æ ¼ï¼ˆé™ä½ 90%ï¼‰
print(f"ç¼“å­˜è¯»å– Token: {response.usage.cache_read_input_tokens}")
print(f"ç¼“å­˜å†™å…¥ Token: {response.usage.cache_creation_input_tokens}")
```

**Prompt Caching æœ€ä½³å®è·µï¼š**

```mermaid
graph TD
    A["Prompt ç»“æ„è®¾è®¡"] --> B["å›ºå®šå†…å®¹åœ¨å‰<br/>å¯ç¼“å­˜éƒ¨åˆ†"]
    A --> C["åŠ¨æ€å†…å®¹åœ¨å<br/>ç”¨æˆ·è¾“å…¥"]
    
    B --> D["ç³»ç»Ÿæç¤ºè¯<br/>æ–‡æ¡£å†…å®¹<br/>Few-shot ç¤ºä¾‹"]
    C --> E["ç”¨æˆ·é—®é¢˜<br/>ä¼šè¯å†å²<br/>åŠ¨æ€å‚æ•°"]
    
    style B fill:#c8e6c9
    style C fill:#e1f5fe
```

### 2. Batch APIï¼ˆ-50% æˆæœ¬ï¼‰

**åŸç†ï¼šç¦»çº¿æ‰¹é‡å¤„ç†ä»»åŠ¡ï¼Œ24 å°æ—¶å†…å®Œæˆï¼Œæˆæœ¬å‡åŠã€‚**

| API ç±»å‹ | æˆæœ¬ | å»¶è¿Ÿ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|---------|
| **å®æ—¶ API** | æ ‡å‡†ä»·æ ¼ | <5 ç§’ | ç”¨æˆ·å®æ—¶äº¤äº’ |
| **Batch API** | **-50%** | <24 å°æ—¶ | æ•°æ®åˆ†æã€å†…å®¹ç”Ÿæˆã€è¯„ä¼° |

**OpenAI Batch API ç¤ºä¾‹ï¼š**

```python
from openai import OpenAI
import json

client = OpenAI()

# 1. å‡†å¤‡æ‰¹é‡ä»»åŠ¡æ–‡ä»¶ï¼ˆJSONL æ ¼å¼ï¼‰
tasks = [
    {
        "custom_id": "task-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "gpt-4.1-mini",
            "messages": [{"role": "user", "content": "ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ"}]
        }
    },
    {
        "custom_id": "task-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "gpt-4.1-mini",
            "messages": [{"role": "user", "content": "ä»€ä¹ˆæ˜¯ JavaScriptï¼Ÿ"}]
        }
    },
    # ... æœ€å¤š 50000 ä¸ªä»»åŠ¡
]

# å†™å…¥æ–‡ä»¶
with open("batch_tasks.jsonl", "w") as f:
    for task in tasks:
        f.write(json.dumps(task) + "\n")

# 2. ä¸Šä¼ æ–‡ä»¶
batch_file = client.files.create(
    file=open("batch_tasks.jsonl", "rb"),
    purpose="batch"
)

# 3. åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
batch = client.batches.create(
    input_file_id=batch_file.id,
    endpoint="/v1/chat/completions",
    completion_window="24h"
)

print(f"æ‰¹å¤„ç†ä»»åŠ¡å·²åˆ›å»ºï¼š{batch.id}")
print(f"çŠ¶æ€ï¼š{batch.status}")

# 4. æŸ¥è¯¢çŠ¶æ€
batch_status = client.batches.retrieve(batch.id)
print(f"è¿›åº¦ï¼š{batch_status.request_counts.completed}/{batch_status.request_counts.total}")

# 5. ä¸‹è½½ç»“æœï¼ˆä»»åŠ¡å®Œæˆåï¼‰
if batch_status.status == "completed":
    result_file = client.files.content(batch_status.output_file_id)
    results = [json.loads(line) for line in result_file.text.split('\n') if line]
    
    for result in results:
        print(f"Task {result['custom_id']}: {result['response']['body']['choices'][0]['message']['content']}")
```

**é€‚ç”¨åœºæ™¯ï¼š**

- è¯„ä¼°æ•°æ®é›†ï¼ˆ1000 ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼‰
- å†…å®¹æ‰¹é‡ç”Ÿæˆï¼ˆç¿»è¯‘ã€æ‘˜è¦ã€åˆ†ç±»ï¼‰
- æ•°æ®æ¸…æ´—å’Œæ ‡æ³¨
- å®šæœŸæŠ¥å‘Šç”Ÿæˆ

### 3. Semantic Cachingï¼ˆè¯­ä¹‰ç¼“å­˜ï¼‰

**åŸç†ï¼šç›¸ä¼¼é—®é¢˜ä¸é‡å¤è°ƒç”¨ LLMï¼Œç›´æ¥è¿”å›ç¼“å­˜ç­”æ¡ˆã€‚**

```python
from openai import OpenAI
import hashlib
import json

client = OpenAI()

class SemanticCache:
    def __init__(self, cache_file: str = "cache.json"):
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self) -> dict:
        try:
            with open(self.cache_file, 'r') as f:
                return json.load(f)
        except:
            return {}
    
    def _save_cache(self):
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)
    
    def _hash_key(self, text: str) -> str:
        return hashlib.md5(text.encode()).hexdigest()
    
    def get(self, question: str) -> str | None:
        key = self._hash_key(question.lower().strip())
        return self.cache.get(key)
    
    def set(self, question: str, answer: str):
        key = self._hash_key(question.lower().strip())
        self.cache[key] = answer
        self._save_cache()

# ä½¿ç”¨è¯­ä¹‰ç¼“å­˜
cache = SemanticCache()

def cached_llm_call(question: str) -> tuple[str, bool]:
    """å¸¦ç¼“å­˜çš„ LLM è°ƒç”¨"""
    # 1. æ£€æŸ¥ç¼“å­˜
    cached_answer = cache.get(question)
    if cached_answer:
        return cached_answer, True  # ç¼“å­˜å‘½ä¸­
    
    # 2. è°ƒç”¨ LLM
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": question}]
    )
    answer = response.choices[0].message.content
    
    # 3. å†™å…¥ç¼“å­˜
    cache.set(question, answer)
    
    return answer, False  # ç¼“å­˜æœªå‘½ä¸­

# æµ‹è¯•
questions = [
    "ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ",
    "ä»€ä¹ˆæ˜¯pythonï¼Ÿ",  # å¤§å°å†™ä¸åŒï¼Œä½†å†…å®¹ç›¸åŒ
    "ä»€ä¹ˆæ˜¯ JavaScriptï¼Ÿ",
]

for q in questions:
    answer, from_cache = cached_llm_call(q)
    print(f"é—®é¢˜ï¼š{q}")
    print(f"å›ç­”ï¼š{answer}")
    print(f"æ¥æºï¼š{'ç¼“å­˜' if from_cache else 'LLM'}")
    print()
```

### 4. å…¶ä»–æˆæœ¬ä¼˜åŒ–ç­–ç•¥

**ç­–ç•¥å¯¹æ¯”ï¼š**

| ç­–ç•¥ | èŠ‚çœæˆæœ¬ | å®æ–½éš¾åº¦ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|
| **Prompt Caching** | 50-90% | ä½ | å›ºå®šç³»ç»Ÿæç¤ºè¯ |
| **Batch API** | 50% | ä¸­ | éå®æ—¶ä»»åŠ¡ |
| **Semantic Caching** | 70%+ | ä¸­ | é‡å¤é—®é¢˜å¤š |
| **æ¨¡å‹é™çº§** | 50-90% | ä½ | ç®€å•ä»»åŠ¡ç”¨å°æ¨¡å‹ |
| **é™åˆ¶ max_tokens** | 20-50% | ä½ | æ§åˆ¶è¾“å‡ºé•¿åº¦ |
| **å‹ç¼©ä¸Šä¸‹æ–‡** | 30-50% | é«˜ | é•¿å¯¹è¯å†å² |

**æ¨¡å‹é€‰æ‹©ç­–ç•¥ï¼š**

```python
def choose_model(task_complexity: str, max_budget: float) -> str:
    """æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""
    
    model_costs = {
        "gpt-4o": {"input": 2.5, "output": 10.0, "quality": 10},
        "gpt-4.1-mini": {"input": 0.15, "output": 0.6, "quality": 8},
        "gpt-3.5-turbo": {"input": 0.5, "output": 1.5, "quality": 7},
    }
    
    if task_complexity == "simple":
        return "gpt-4.1-mini"  # æ•°æ®æå–ã€åˆ†ç±»
    elif task_complexity == "medium":
        return "gpt-4.1-mini"  # æ‘˜è¦ã€é—®ç­”
    elif task_complexity == "complex":
        return "gpt-4o"      # æ¨ç†ã€ä»£ç ç”Ÿæˆ
    
    return "gpt-4.1-mini"  # é»˜è®¤
```

**è¾“å‡ºæ§åˆ¶ï¼š**

```python
# é™åˆ¶è¾“å‡ºé•¿åº¦
response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": "è§£é‡Šé‡å­è®¡ç®—"}],
    max_tokens=100  # é™åˆ¶è¾“å‡ºï¼Œé¿å…å†—é•¿å›ç­”
)
```

### ä¼ä¸šå®‰å…¨ä¸åˆè§„

**æ•°æ®å®‰å…¨æªæ–½ï¼š**

| æªæ–½ | è¯´æ˜ | å·¥å…· |
|------|------|------|
| **æ•°æ®è„±æ•** | è¯·æ±‚å‰ç§»é™¤ PII | Presidio, æ­£åˆ™è¡¨è¾¾å¼ |
| **è®¿é—®æ§åˆ¶** | API Key æƒé™ç®¡ç† | IAM, RBAC |
| **å®¡è®¡æ—¥å¿—** | è®°å½•æ‰€æœ‰è°ƒç”¨ | LangSmith, ELK |
| **ç§æœ‰éƒ¨ç½²** | æœ¬åœ°è¿è¡Œæ¨¡å‹ | OpenAI Azure, AWS Bedrock |

**ä¼ä¸šéƒ¨ç½²é€‰é¡¹ï¼š**

```mermaid
graph TD
    A["ä¼ä¸šéƒ¨ç½²æ–¹æ¡ˆ"] --> B["1. API æœåŠ¡<br/>OpenAI/Anthropic"]
    A --> C["2. äº‘æ‰˜ç®¡<br/>Azure OpenAI/AWS Bedrock"]
    A --> D["3. ç§æœ‰éƒ¨ç½²<br/>è‡ªå»º LLM"]
    
    B --> B1["âœ“ å¿«é€Ÿä¸Šçº¿<br/>âœ— æ•°æ®ä¸Šäº‘"]
    C --> C1["âœ“ ä¼ä¸šçº§ SLA<br/>âœ“ æ•°æ®éš”ç¦»<br/>âœ— æˆæœ¬é«˜"]
    D --> D1["âœ“ å®Œå…¨æ§åˆ¶<br/>âœ— è¿ç»´æˆæœ¬é«˜<br/>âœ— æ€§èƒ½å—é™"]
    
    style A fill:#c8e6c9
```

**Rate Limitingï¼ˆé€Ÿç‡é™åˆ¶ï¼‰ï¼š**

```python
from time import sleep, time
from collections import deque

class RateLimiter:
    """ç®€å•çš„é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self, max_requests: int, time_window: int):
        self.max_requests = max_requests
        self.time_window = time_window  # ç§’
        self.requests = deque()
    
    def allow_request(self) -> bool:
        now = time()
        
        # æ¸…ç†è¿‡æœŸè¯·æ±‚
        while self.requests and self.requests[0] < now - self.time_window:
            self.requests.popleft()
        
        # æ£€æŸ¥æ˜¯å¦è¶…é™
        if len(self.requests) >= self.max_requests:
            return False
        
        # è®°å½•è¯·æ±‚
        self.requests.append(now)
        return True
    
    def wait_if_needed(self):
        """é˜»å¡ç­‰å¾…ç›´åˆ°å¯ä»¥å‘é€è¯·æ±‚"""
        while not self.allow_request():
            sleep(0.1)

# ä½¿ç”¨ï¼šé™åˆ¶æ¯åˆ†é’Ÿæœ€å¤š 10 æ¬¡è¯·æ±‚
limiter = RateLimiter(max_requests=10, time_window=60)

def rate_limited_llm_call(question: str) -> str:
    limiter.wait_if_needed()
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": question}]
    )
    return response.choices[0].message.content
```

### åŠ¨æ‰‹è¯•è¯•ï¼ˆPracticeï¼‰

**å®éªŒï¼šPrompt Caching æˆæœ¬å¯¹æ¯”**

```python
from openai import OpenAI
import time

client = OpenAI()

# é•¿ç³»ç»Ÿæç¤ºè¯ï¼ˆæ¨¡æ‹Ÿ 3000 Tokenï¼‰
LONG_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ï¼Œéœ€è¦éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

1. é€€è´§æ”¿ç­–ï¼š
   - 7 å¤©æ— ç†ç”±é€€è´§
   - å•†å“éœ€ä¿æŒåŸåŒ…è£…
   - è¿è´¹ç”±ä¹°å®¶æ‰¿æ‹…
   ï¼ˆæ­¤å¤„çœç•¥ 2500 å­—è¯¦ç»†è§„åˆ™ï¼‰

2. å‘è´§æ”¿ç­–ï¼š...
3. æ”¯ä»˜æ”¿ç­–ï¼š...
...ï¼ˆæ€»è®¡çº¦ 3000 Tokenï¼‰
""" * 10  # é‡å¤ä»¥æ¨¡æ‹Ÿé•¿å†…å®¹

# ä¸ä½¿ç”¨ Caching
def without_caching():
    """ä¸ä½¿ç”¨ç¼“å­˜"""
    total_input_tokens = 0
    start = time.time()
    
    questions = [
        "ä½ ä»¬çš„é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å‘è´§éœ€è¦å¤šä¹…ï¼Ÿ",
        "æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ",
    ]
    
    for q in questions:
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": LONG_SYSTEM_PROMPT},
                {"role": "user", "content": q}
            ]
        )
        total_input_tokens += response.usage.prompt_tokens
    
    duration = time.time() - start
    
    # ä¼°ç®—æˆæœ¬ï¼ˆGPT-4o-mini è¾“å…¥ä»·æ ¼ï¼š$0.15/1M tokensï¼‰
    cost = total_input_tokens * 0.15 / 1_000_000
    
    print("=== ä¸ä½¿ç”¨ Prompt Caching ===")
    print(f"æ€»è¾“å…¥ Token: {total_input_tokens:,}")
    print(f"ä¼°ç®—æˆæœ¬: ${cost:.4f}")
    print(f"è€—æ—¶: {duration:.2f}s")
    
    return cost

# ä½¿ç”¨ Cachingï¼ˆæ¨¡æ‹Ÿæ•ˆæœï¼‰
def with_caching():
    """ä½¿ç”¨ç¼“å­˜ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    system_prompt_tokens = 3000  # ç³»ç»Ÿæç¤ºè¯ Token æ•°
    user_tokens = 50  # å¹³å‡ç”¨æˆ·è¾“å…¥ Token æ•°
    questions_count = 3
    
    # ç¬¬ä¸€æ¬¡ï¼šå†™å…¥ç¼“å­˜ï¼ˆ125% ä»·æ ¼ï¼‰
    first_call_tokens = int(system_prompt_tokens * 1.25) + user_tokens
    
    # åç»­è°ƒç”¨ï¼šç¼“å­˜å‘½ä¸­ï¼ˆ50% ä»·æ ¼ï¼‰
    cached_call_tokens = int(system_prompt_tokens * 0.5) + user_tokens
    
    total_input_tokens = first_call_tokens + cached_call_tokens * (questions_count - 1)
    
    cost = total_input_tokens * 0.15 / 1_000_000
    
    print("\n=== ä½¿ç”¨ Prompt Caching ===")
    print(f"ç¬¬ä¸€æ¬¡è°ƒç”¨: {first_call_tokens:,} tokens (å†™å…¥ç¼“å­˜)")
    print(f"åç»­è°ƒç”¨: {cached_call_tokens:,} tokens/æ¬¡ (ç¼“å­˜å‘½ä¸­)")
    print(f"æ€»è¾“å…¥ Token: {total_input_tokens:,}")
    print(f"ä¼°ç®—æˆæœ¬: ${cost:.4f}")
    
    return cost

# å¯¹æ¯”
cost_without = without_caching()
cost_with = with_caching()

print("\n=== å¯¹æ¯”ç»“æœ ===")
print(f"èŠ‚çœæˆæœ¬: ${cost_without - cost_with:.4f}")
print(f"æˆæœ¬é™ä½: {(1 - cost_with / cost_without) * 100:.1f}%")
```

<ColabBadge path="demos/13-production/cost_optimization.ipynb" />

### å°ç»“ï¼ˆReflectionï¼‰

**ğŸ¯ ä¸€å¥è¯æ€»ç»“ï¼šæˆæœ¬ä¼˜åŒ–æ˜¯ AI åº”ç”¨çš„"èŠ‚èƒ½æ¨¡å¼"ï¼ŒPrompt Caching é™ 90%ï¼ŒBatch API æ‰“ 5 æŠ˜ï¼Œåˆ«è®©è´¦å•å“è·‘è€æ¿ã€‚**

- **è§£å†³äº†ä»€ä¹ˆ**ï¼šé€šè¿‡ Prompt Cachingã€Batch APIã€è¯­ä¹‰ç¼“å­˜ç­‰æ‰‹æ®µé™ä½ 90% æˆæœ¬
- **æ²¡è§£å†³ä»€ä¹ˆ**ï¼šç”Ÿäº§åŒ–æŠ€æœ¯éƒ½å­¦ä¼šäº†ï¼Œä½†æ€ä¹ˆåº”ç”¨åˆ°çœŸå®é¡¹ç›®ï¼Ÿâ€”â€”ä¸‹ä¸€ç« ä»‹ç» IT éƒ¨é—¨å®æˆ˜åœºæ™¯
- **å…³é”®è¦ç‚¹**ï¼š
  1. **Prompt Caching æ˜¯ç‹ç‚¸**ï¼šé™ä½ 50-90% è¾“å…¥æˆæœ¬ï¼ˆä¸ç”¨å°±æ˜¯äºï¼‰
  2. **Batch API é€‚åˆç¦»çº¿ä»»åŠ¡**ï¼šæˆæœ¬å‡åŠï¼Œä½†è¦ç­‰ 24 å°æ—¶ï¼ˆèƒ½å¿å°±ç”¨ï¼‰
  3. **è¯­ä¹‰ç¼“å­˜é¿å…é‡å¤è°ƒç”¨**ï¼šç›¸ä¼¼é—®é¢˜ç›´æ¥è¿”å›ç¼“å­˜ï¼ˆæ‡’æ˜¯ç¬¬ä¸€ç”Ÿäº§åŠ›ï¼‰
  4. **æ¨¡å‹é€‰æ‹©è¦èªæ˜**ï¼šç®€å•ä»»åŠ¡ç”¨å°æ¨¡å‹ï¼Œå¤æ‚ä»»åŠ¡ç”¨å¤§æ¨¡å‹ï¼ˆåˆ«æ‹¿å¤§ç‚®æ‰“èšŠå­ï¼‰
  5. **ä¼ä¸šéƒ¨ç½²**ï¼šAzure OpenAIã€AWS Bedrock æä¾›æ•°æ®éš”ç¦»ï¼ˆè´µä½†å®‰å…¨ï¼‰

::: tip è®°ä½è¿™ä¸ªæ¯”å–»
æˆæœ¬ä¼˜åŒ– = AI çš„æ°´ç”µè´¹è´¦å•ï¼šä¸ä¼˜åŒ–å°±ç­‰ç€è€æ¿æ‰¾ä½ è°ˆè¯ã€‚
:::

---

## ğŸ¯ ç”Ÿäº§ç¯‡æ£€æŸ¥ç‚¹

æ­å–œï¼ä½ å·²ç»æŒæ¡äº† AI åº”ç”¨ç”Ÿäº§åŒ–çš„æ ¸å¿ƒæŠ€èƒ½ï¼š

### ä½ å­¦ä¼šäº†ä»€ä¹ˆï¼Ÿ

- âœ… **Guardrails**ï¼šé˜²å¾¡ Prompt æ³¨å…¥ã€æœ‰å®³å†…å®¹ã€PII æ³„éœ²
- âœ… **Evaluation**ï¼šæ„å»º LLM-as-Judge è¯„ä¼°ä½“ç³»ï¼Œé‡åŒ–è´¨é‡
- âœ… **Observability**ï¼šå®ç°æ—¥å¿—ã€æŒ‡æ ‡ã€è¿½è¸ªï¼Œç›‘æ§è¿è¡ŒçŠ¶æ€
- âœ… **Cost Optimization**ï¼šé€šè¿‡ Prompt Caching é™ä½ 90% æˆæœ¬

### èƒ½åšä»€ä¹ˆé¡¹ç›®ï¼Ÿ

1. **ä¼ä¸šçº§ AI åº”ç”¨**ï¼šå…·å¤‡å®‰å…¨ã€ç›‘æ§ã€æˆæœ¬æ§åˆ¶èƒ½åŠ›
2. **è´¨é‡ä¿éšœä½“ç³»**ï¼šè‡ªåŠ¨åŒ–è¯„ä¼° + æŒç»­ç›‘æ§
3. **æˆæœ¬å¯æ§çš„æœåŠ¡**ï¼šä¸ä¼šå‡ºç°è´¦å•å¤±æ§çš„æƒ…å†µ

### ä¸‹ä¸€æ­¥ï¼Ÿ

ğŸ‘‰ **ç¬¬ 14 ç« ï¼šIT éƒ¨é—¨å®æˆ˜åœºæ™¯** - å°†æ‰€å­¦æŠ€æœ¯åº”ç”¨åˆ°çœŸå®é¡¹ç›®ï¼šå†…éƒ¨çŸ¥è¯†åº“ã€ä»£ç å®¡æŸ¥ã€è¿ç»´åŠ©æ‰‹

---

*æœ€åæ›´æ–°ï¼š2026-02-20*
