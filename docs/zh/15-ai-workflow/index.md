---
next:
  text: '15.2 主流平台详解'
  link: '/zh/15-ai-workflow/platforms'
---

# 15.1 AI 编排平台全景

::: info 拓展篇
本章为拓展内容，侧重于 No-code/Low-code 方案，不依赖前面的编程实战章节（Ch13-14）。即使跳过了实战部分，也能直接阅读本章。
:::

## 如果搭建 AI 应用就像搭积木一样简单……

想象一下这个场景:产品经理拿着需求文档冲进会议室,"我们需要一个智能客服机器人,能理解用户意图、查询数据库、发送邮件,还要能学习历史对话!"开发团队集体叹气,心里默默计算着要写多少代码、调多少 API、踩多少坑……

但等等,2026 年的现实是:产品经理可能自己就把这个机器人搭出来了,用的还是可视化拖拽界面,连一行代码都不用写。

欢迎来到 **AI 编排平台**的时代——一个"人人都能搭建 AI 应用"的平行宇宙。

## 为什么 2025-2026 突然爆发了?

### 完美风暴的形成

如果把 AI 编排平台的兴起比作一场完美风暴,那么它由几股力量共同推动:

**1. 大模型能力的标准化**

还记得 2022 年吗?每个大模型都有自己的 API 格式、调用方式、限流规则。开发者就像在学多国语言,GPT-4 说英语,Claude 说法语,Gemini 说日语……

到了 2025 年,OpenAI 的 Chat Completion API 事实上成为了行业标准。所有主流模型都支持类似的接口格式,就像当年 REST API 统一了 Web 服务一样。这让"模型抽象层"成为可能——编排平台可以轻松切换不同的大模型,就像换汽车的引擎一样简单。

```python
# 2022 年:每个模型都是特殊的雪花
openai_response = openai.ChatCompletion.create(...)
claude_response = anthropic.messages.create(...)  # 完全不同的 API
gemini_response = google.generative_ai.generate(...)  # 又是另一套

# 2026 年:统一的抽象接口
response = llm_client.chat(
    model="gpt-4",  # 或 "claude-3", "gemini-2.0"
    messages=[...]  # 统一的消息格式
)
```

**2. RAG 和 Agent 成为刚需**

单纯调用大模型已经不够了。用户发现:

- 没有 **RAG**(检索增强生成)的 AI 就像失忆症患者,啥都不记得
- 没有 **Agent** 能力的 AI 就像只会说不会做的嘴炮王,无法执行实际操作
- 没有 **Workflow**(工作流)的 AI 就像无头苍蝇,逻辑混乱

但手写这些能力?对于大多数团队来说,门槛太高了:

```python
# 手写 RAG 系统的噩梦
def rag_pipeline(query):
    # 1. 文档加载(支持 PDF/Word/Markdown/网页...)
    docs = load_documents()  # 要处理各种格式
    
    # 2. 文本切块(chunk size、overlap、分割策略...)
    chunks = split_documents(docs)  # 参数调优深似海
    
    # 3. 向量化(选哪个 embedding 模型?)
    embeddings = embed_chunks(chunks)  # OpenAI? Local?
    
    # 4. 向量存储(Pinecone? Weaviate? Chroma?)
    vector_store.add(embeddings)  # 又要学一套数据库
    
    # 5. 相似度检索(top-k、阈值、重排序...)
    relevant_docs = vector_store.search(query)  # 调参炼丹
    
    # 6. 上下文构建(token 限制、优先级排序...)
    context = build_context(relevant_docs)  # 别超 token 限制
    
    # 7. Prompt 工程(模板、变量、错误处理...)
    prompt = f"Context: {context}\nQuestion: {query}"
    
    # 8. 调用大模型(重试、限流、错误处理...)
    return llm.chat(prompt)  # 终于到这一步了...
```

这 8 个步骤,每一步都能写一篇论文。编排平台的价值就是:**把这些复杂度封装成拖拽式的"积木块"**。

**3. 开源社区的狂欢**

LangChain 和 LlamaIndex 在 2023 年的爆火证明了一件事:开发者迫切需要 AI 应用的"搭建框架"。但代码框架还不够,我们需要**可视化**的框架。

于是:

- **Dify**(2023):开源的 LLMOps 平台,GitHub 50k+ stars
- **FlowiseAI**(2023):可视化 LangChain,GitHub 30k+ stars
- **LangFlow**(2024):LangChain 官方的可视化工具
- **n8n + AI 节点**(2024):老牌自动化工具拥抱 AI

开源社区用实际行动投票:未来是**可视化编排**的。

**4. 企业的成本焦虑**

雇一个 AI 工程师,年薪 50 万起步。组建一个 AI 团队,每年烧掉几百万。但 80% 的需求其实都是标准化的:

- 智能客服机器人
- 文档问答系统
- 内容生成工具
- 数据分析助手

用编排平台?可能一个初级开发者甚至产品经理就能搞定,成本直降 90%。

**CFO** 看着预算表,眼睛放光:"你是说不用招那么多 AI 工程师了?"  
**CEO** 看着交付速度,激动拍桌:"你是说原型开发从 3 个月缩短到 3 天?"

市场需求就这样被点燃了。

## 三大类别:从代码到无代码的光谱

AI 编排平台不是铁板一块,它们形成了一个**从重代码到零代码**的连续光谱:

### 类型一:可视化 AI 构建器 (Visual AI Builders)

**代表:** Dify、Coze、Botpress

**核心理念:** 专为 AI 应用设计的可视化开发平台

**典型场景:**
- 构建聊天机器人(客服、销售助手、内部知识库)
- 搭建 RAG 系统(文档问答、知识管理)
- 创建 AI Agent(能调用工具、执行任务的智能体)

**特点:**

```
[用户输入] 
    ↓
[意图识别节点] ← 可以配置不同的 LLM
    ↓
[知识库检索节点] ← 拖拽上传文档,自动向量化
    ↓
[LLM 生成节点] ← 可视化编辑 Prompt
    ↓
[工具调用节点] ← 连接外部 API
    ↓
[响应输出]
```

每个节点都是一个拖拽的"积木块",配置好参数就能运行。就像搭乐高一样,只不过搭出来的是一个智能 AI。

**适用人群:**
- 产品经理:"我需要快速验证 AI 产品 idea"
- 创业团队:"我们要用最少的资源搭建 MVP"
- 企业内部工具开发者:"给每个部门都搭建定制化的 AI 助手"

### 类型二:自动化平台 + AI (Automation + AI)

**代表:** n8n + AI 节点、Zapier AI、Make(原 Integromat)

**核心理念:** 在传统自动化工作流中插入 AI 能力

**典型场景:**
- 邮件自动分类和回复(Gmail → AI 分析 → 自动归档/回复)
- 社交媒体内容生成(RSS 订阅 → AI 改写 → 定时发布)
- 数据清洗和分析(Google Sheets → AI 处理 → Slack 通知)

**特点:**

```
[触发器: 收到新邮件]
    ↓
[AI 节点: 分析邮件情绪和意图]
    ↓
[条件分支: 如果是投诉]
    ↓
[AI 节点: 生成道歉回复草稿]
    ↓
[动作: 发送到 Slack 审核频道]
    ↓
[动作: 自动标记邮件优先级]
```

这类平台的强项是**连接各种 SaaS 工具**。它们有成百上千的预制集成(integrations),可以让 AI 和你的整个软件生态系统对话。

**适用人群:**
- 运营人员:"我想自动化日常重复工作"
- 增长黑客:"我要搭建自动化的营销漏斗"
- 个人效率狂人:"自动化一切可以自动化的!"

### 类型三:AI 应用构建器 (AI App Builders)

**代表:** FlowiseAI、Langflow、Flowise

**核心理念:** 将 LangChain/LlamaIndex 等代码框架可视化

**典型场景:**
- 构建复杂的 RAG 系统(多数据源、混合检索、重排序)
- 搭建多 Agent 协作系统(不同 Agent 负责不同任务)
- 开发定制化的 AI 工具(需要精细控制底层逻辑)

**特点:**

```
[Document Loader: PDF]
    ↓
[Text Splitter: Recursive Character]
    ↓
[Embeddings: OpenAI ada-002]
    ↓
[Vector Store: Pinecone]
    ↓
[Retriever: Similarity Search (k=4)]
    ↓
[LLM Chain: GPT-4]
    ↓
[Output Parser: Structured JSON]
```

每个节点对应 LangChain 的一个组件,参数可以精细调整。适合需要**专业级控制**又不想写太多代码的开发者。

**适用人群:**
- AI 工程师:"我需要快速原型开发,但保留底层控制"
- 技术架构师:"我要在可视化环境中设计 AI 系统架构"
- 学习者:"我想理解 LangChain/LlamaIndex 的工作原理"

## 架构对比:代码优先 vs 低代码 vs 无代码

让我们用一个实际案例来对比三种架构:构建一个"文档问答系统"。

### 代码优先 (Code-First)

**工具:** LangChain + Python

**代码量:** ~200-500 行

**优势:**
- 完全控制,可以实现任何复杂逻辑
- 容易集成到现有代码库
- 性能优化空间大
- 版本控制友好(Git)

**劣势:**
- 开发速度慢,需要专业开发者
- 调试困难,尤其是 prompt 调优
- 非技术人员完全无法参与

```python
# 典型的 LangChain 代码
from langchain.vectorstores import Pinecone
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# 初始化
embeddings = OpenAIEmbeddings()
vectorstore = Pinecone.from_existing_index("my-index", embeddings)
llm = ChatOpenAI(model="gpt-4", temperature=0)

# 构建问答链
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 4})
)

# 使用
answer = qa_chain.run("什么是 RAG?")
```

### 低代码 (Low-Code)

**工具:** FlowiseAI、Langflow

**代码量:** ~0-50 行(主要是配置和扩展)

**优势:**
- 可视化调试,看得见数据流动
- 开发速度快 10 倍
- 技术人员和非技术人员可以协作
- 支持自定义节点扩展

**劣势:**
- 复杂逻辑可能受限于平台能力
- 性能可能不如纯代码优化
- 平台绑定风险

```
可视化界面:
┌─────────────────┐
│  PDF Loader     │
│  (拖拽上传文件)  │
└────────┬────────┘
         ↓
┌─────────────────┐
│  Text Splitter  │
│  chunk_size:1000│
└────────┬────────┘
         ↓
┌─────────────────┐
│  OpenAI Embed   │
│  model: ada-002 │
└────────┬────────┘
         ↓
┌─────────────────┐
│  Pinecone Store │
│  index: my-docs │
└────────┬────────┘
         ↓
┌─────────────────┐
│  Retrieval QA   │
│  LLM: GPT-4     │
└─────────────────┘
```

### 无代码 (No-Code)

**工具:** Dify、Coze

**代码量:** 0 行

**优势:**
- 任何人都能用,产品经理、运营都可以
- 原型开发速度极快(几小时到一天)
- 内置最佳实践,不容易踩坑
- 部署简单,一键发布

**劣势:**
- 定制化能力最弱
- 完全依赖平台功能
- 复杂场景可能无法实现

```
Dify 界面操作:
1. 创建应用 → 选择"知识库问答"模板
2. 上传文档 → 拖拽 PDF 文件,自动处理
3. 配置模型 → 下拉选择 GPT-4
4. 编辑 Prompt → 在文本框中输入提示词
5. 测试 → 右侧对话窗口直接测试
6. 发布 → 一键生成 API 和嵌入代码
```

### 选择决策树

```
你需要实现的功能是否超出平台能力?
├─ 是 → 使用代码优先
└─ 否 → 你的团队有专职 AI 工程师吗?
    ├─ 否 → 使用无代码平台
    └─ 是 → 你需要频繁迭代和调试吗?
        ├─ 是 → 使用低代码平台
        └─ 否 → 根据团队偏好选择
```

## 何时用哪种方法?

### 场景一:MVP 快速验证

**任务:** 创业公司想在 2 周内验证"AI 法律咨询助手"的市场需求

**最佳选择:** **无代码平台(Dify/Coze)**

**原因:**
- 速度第一,功能够用就行
- 团队可能还没有 AI 工程师
- 需要频繁调整,拖拽比改代码快
- 失败了也没有太多沉没成本

**预期时间线:**
- Day 1-2: 收集法律文档,上传到知识库
- Day 3-5: 调试 Prompt,优化回答质量
- Day 6-7: 搭建简单的 Web 界面
- Day 8-14: 小范围用户测试,快速迭代

### 场景二:企业级生产系统

**任务:** 银行要部署 AI 客服,要求高可用、安全合规、性能优化

**最佳选择:** **代码优先 + 部分低代码**

**原因:**
- 需要精细的性能优化(延迟、吞吐量)
- 安全要求高,需要自定义鉴权和审计
- 要集成复杂的内部系统
- 需要完整的 CI/CD 和监控

**架构:**
```
核心引擎: Python + LangChain(代码优先)
    ↓
管理后台: FlowiseAI(低代码,供运营配置)
    ↓
监控系统: 自研(代码优先)
```

### 场景三:个人效率工具

**任务:** 个人开发者想自动化"每天总结技术文章并发推特"

**最佳选择:** **自动化平台(n8n/Zapier)**

**原因:**
- 需要连接多个 SaaS(RSS、OpenAI、Twitter)
- 逻辑简单,不需要复杂编程
- 个人项目,性价比最重要
- 希望能快速搭建,不想维护代码

**工作流:**
```
[定时触发: 每天早上 9 点]
    ↓
[RSS 节点: 抓取 Hacker News 热门文章]
    ↓
[AI 节点: GPT-4 总结文章要点]
    ↓
[AI 节点: 改写成推特风格(280 字以内)]
    ↓
[Twitter API 节点: 发布推文]
    ↓
[Notion 节点: 保存到个人数据库]
```

## 评估平台的关键标准

选择 AI 编排平台就像选择约会对象,不能只看外表,要看内在(和是否适配)。这里是 7 个关键评估维度:

### 1. 部署灵活性

**问题:** 数据能离开平台吗?能私有化部署吗?

- **云端 SaaS:** Coze、Zapier AI(数据在他们服务器)
- **支持自部署:** Dify、n8n、FlowiseAI(你掌控数据)
- **混合部署:** 既有云服务,也能本地部署

**重要性:**  
如果你处理敏感数据(医疗、金融、企业内部),**自部署是刚需**。如果只是个人项目或非敏感应用,云端 SaaS 更省心。

### 2. 模型灵活性

**问题:** 能用哪些大模型?能切换吗?

- **单一模型:** 只支持平台自己的模型(绑定风险高)
- **多模型支持:** OpenAI、Anthropic、Google、开源模型都支持
- **本地模型:** 支持 Ollama、llama.cpp 等本地部署

**重要性:**  
2026 年的教训是:**永远不要把鸡蛋放在一个模型的篮子里**。GPT-4 可能今天最好,明天就被超越了。

### 3. 可扩展性

**问题:** 当平台能力不够时,能扩展吗?

- **封闭系统:** 只能用平台提供的功能(受限但简单)
- **插件系统:** 社区可以开发扩展(生态很重要)
- **代码扩展:** 支持自定义节点、函数(最灵活)

**重要性:**  
如果你只想快速搭建标准应用,封闭系统够用。如果有独特需求,可扩展性是救命稻草。

### 4. 社区和生态

**问题:** 遇到问题有人帮吗?有现成模板吗?

- **GitHub Stars:** 衡量受欢迎程度
- **Discord/论坛活跃度:** 衡量社区支持
- **模板市场:** 有没有现成的最佳实践可以复用

**Dify** 的 50k+ GitHub stars 不是白来的,意味着遇到问题,Google 一下大概率有答案。

### 5. 成本结构

**问题:** 这玩意儿烧钱吗?

- **开源免费:** Dify、n8n、FlowiseAI(只需服务器成本)
- **Freemium:** 有免费额度,超出付费(Coze、Zapier)
- **企业定价:** 按座位数或调用量收费

**隐藏成本:**
- 大模型 API 费用(这通常是大头)
- 向量数据库费用(如果用云服务)
- 服务器/云计算费用(如果自部署)

### 6. 开发者体验

**问题:** 用起来爽吗?

- **UI/UX:** 界面直观吗?学习曲线陡吗?
- **调试能力:** 能看到每一步的数据流吗?
- **文档质量:** 官方文档详细吗?有教程吗?
- **版本控制:** 能导出配置吗?能 Git 管理吗?

这个维度很主观,建议**实际试用 1-2 天**再决定。

### 7. 生产就绪度

**问题:** 能直接用于生产环境吗?

- **稳定性:** 会不会经常宕机或 bug?
- **性能:** 能处理多大并发?
- **监控和日志:** 能追踪问题吗?
- **安全性:** 有鉴权、审计功能吗?

**红旗标志:**
- 项目 GitHub 最近 3 个月没更新
- Issue 里充满未解决的 bug
- 没有企业级客户案例

## 一句话总结

**AI 编排平台的本质是:**

> 把 AI 开发从"手工艺"变成"工业化生产",让构建 AI 应用的门槛从"资深工程师 + 3 个月"降低到"产品经理 + 3 天"——当然,复杂度的降低总是以灵活性为代价,选对工具比学会工具更重要。

下一章,我们将深入剖析主流平台的细节:Dify 的开源生态、Coze 的字节跳动基因、n8n 的自动化魔法……以及如何用 Dify 在 30 分钟内搭建一个客服机器人。

**记住:** 工具是死的,人是活的。平台只是工具,真正重要的是**你要解决什么问题**。选择平台的第一步,永远是明确需求。

---

::: tip 💡 实战建议
如果你是第一次接触 AI 编排平台,建议:
1. **先玩 Dify**(开源、功能全、中文友好)
2. **再试 FlowiseAI**(理解底层原理)
3. **最后探索 n8n**(自动化工作流的终极形态)

每个平台都有免费版或开源版,不要光看文档,**动手试试才是王道**。
:::

::: warning ⚠️  避坑指南
- 不要一上来就追求完美的平台,**没有银弹**
- 不要过度依赖平台能力,核心逻辑要自己掌控
- 不要忽视成本,AI API 调用费可能比平台费用还高
- 不要跳过原型阶段,先验证需求再投入重度开发
:::
